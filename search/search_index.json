{"config":{"lang":["es"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Introducci\u00f3n","text":"<p>La capacidad de reproducir contenido multimedia es una caracter\u00edstica presente en la pr\u00e1ctica totalidad de los terminales telef\u00f3nicos existentes en el mercado hoy en d\u00eda, y es de especial importancia en el caso de los smartphones y tablets. En la asignatura Gr\u00e1ficos y Multimedia del M\u00e1ster Universitario en Desarrollo de Software para Dispositivos M\u00f3viles de la Universidad de Alicante veremos c\u00f3mo trabajar con contenido multimedia, y como mostrar gr\u00e1ficos 3D de alto rendimiento en dispositivos Android e iOS.</p>"},{"location":"index.html#copyright","title":"Copyright","text":"<p>Copyright 2015-2016 Universidad de Alicante. Todos los derechos reservados.</p> <p>Este documento est\u00e1 protegido por copyright y se distribuye bajo licencias que restringen su uso, copia y distribuci\u00f3n. Se restringe al uso estrictamente personal y como material did\u00e1ctico del M\u00e1ster Universitario en Desarrollo de Software para Dispositivos M\u00f3viles, curso 2015-2016, de la Universidad de Alicante.</p> <p>La distribuci\u00f3n y copia de este documento no est\u00e1n permitidas, salvo autorizaci\u00f3n previa de la Universidad de Alicante.</p> <p>La documentaci\u00f3n se suministra \"tal cual\", sin ning\u00fan tipo de condiciones, declaraciones ni garant\u00edas, expresas o impl\u00edcitas, incluidas las relativas a la comercializaci\u00f3n, la adecuaci\u00f3n a fines concretos o la no infracci\u00f3n de las leyes, salvo en los casos en que dichas renuncias no fueran legalmente v\u00e1lidas.</p>"},{"location":"adobe-air.html","title":"Aplicaciones Adobe Air","text":"<p>Adobe Air es una tecnolog\u00eda multiplataforma que nos permite llevar las aplicaciones Flash y ActionScript a distintos dispositivos. Tenemos la opci\u00f3n de crear estas aplicaciones con la herramienta Flash, o bien utilizar el compilador abierto Flex. Vamos a centrarnos en el desarrollo de aplicaciones Adobe AIR con Flex SDK.</p>"},{"location":"adobe-air.html#instalacion-de-flex-y-air-sdk","title":"Instalaci\u00f3n de Flex y AIR SDK","text":"<p>Vamos a ver los pasos que deberemos seguir para instalar los SDK de Flex y Adobe AIR:</p> <ul> <li>En primer lugar deberemos descargar Apache Flex SDK:</li> </ul> <p>http://flex.apache.org</p> <ul> <li> <p>Ejecutaremos el instalador, y \u00e9ste se encargar\u00e1 de descargar todos los elementos necesarios, incluyendo Adobe AIR y Flex SDK. </p> </li> <li> <p>Introduciremos el directorio <code>$FLEX_SDK_HOME/bin</code> en el <code>PATH</code> para as\u00ed tener el compilador accesible. Esto podemos hacerlo editando el fichero <code>.bash_profile</code> de nuestra carpeta personal (<code>$HOME</code>) e introduciendo al comienzo la siguiente l\u00ednea: <pre><code>export PATH=$PATH:/ruta/a/flex_sdk/bin/\n</code></pre></p> </li> </ul> <p>Con esto ya podremos utilizar las herramientas de desarrollo de Flex y Adobe AIR desde l\u00ednea de comando.</p>"},{"location":"adobe-air.html#aplicaciones-adobe-air-desde-linea-de-comando","title":"Aplicaciones Adobe AIR desde l\u00ednea de comando","text":"<p>Vamos a ver en primer lugar la forma de crear una aplicaci\u00f3n Adobe AIR en l\u00ednea de comando, y posteriormente estudiaremos la forma de hacerlo desde un entorno integrado como IntelliJ.</p>"},{"location":"adobe-air.html#creacion-de-un-proyecto-en-linea-de-comando","title":"Creaci\u00f3n de un proyecto en l\u00ednea de comando","text":"<ul> <li> <p>En primer lugar crearemos un fichero XML descriptor de la aplicaci\u00f3n como el siguiente, al que llamaremos <code>HolaMundo-app.xml</code>: <pre><code>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n&lt;application xmlns=\"http://ns.adobe.com/air/application/2.7\"&gt;\n&lt;id&gt;es.ua.eps.HolaMundo&lt;/id&gt;\n&lt;versionNumber&gt;1.0&lt;/versionNumber&gt;\n&lt;filename&gt;HolaMundo&lt;/filename&gt;\n&lt;initialWindow&gt;\n&lt;content&gt;HolaMundo.swf&lt;/content&gt;\n&lt;/initialWindow&gt;\n&lt;supportedProfiles&gt;mobileDevice&lt;/supportedProfiles&gt;\n&lt;/application&gt;\n</code></pre></p> </li> <li> <p>Tras definir el descriptor, introduciremos el c\u00f3digo ActionScript de la clase principal de la aplicaci\u00f3n. Por ejemplo, podr\u00edamos crear un fichero <code>HolaMundo.as</code> como el siguiente: <pre><code>package\n{\nimport flash.display.Sprite;\nimport flash.text.TextField;\n\npublic class HolaMundo extends Sprite\n{\npublic function HolaMundo()\n{\nvar textField:TextField = new TextField();\ntextField.text = \"Hola mundo!\";\nstage.addChild(textField);\n}\n}\n}\n</code></pre></p> </li> </ul>"},{"location":"adobe-air.html#construccion-de-un-proyecto-en-linea-de-comando","title":"Construcci\u00f3n de un proyecto en l\u00ednea de comando","text":"<p>Una vez creado un proyecto, deberemos construirlo utilizando las herramientas en l\u00ednea de comando de Flex SDK:</p> <ul> <li> <p>En primer lugar compilaremos el c\u00f3digo de nuestra aplicaci\u00f3n. Los fuentes podr\u00e1n ser clases ActionScript (<code>.as</code>) o fichero MXML (<code>.mxml</code>). En cualquiera de los dos casos deberemos compilarlos con <code>amxmlc</code>: <pre><code>amxmlc HolaMundo.as\n</code></pre> Esto generar\u00e1 un fichero SWF (en el ejemplo anterior <code>HolaMundo.swf</code>).</p> </li> <li> <p>Una vez compilado podemos lanzar la aplicaci\u00f3n con la herramienta <code>adl</code>, especificando el descriptor XML: <pre><code>adl HolaMundo-app.xml\n</code></pre> En el fichero XML se indica que la ventana inicial est\u00e1 en un fichero <code>HolaMundo.swf</code>, que es el fichero que hemos generado como resultado de la compilaci\u00f3n en el paso anterior, y ser\u00e1 el que se ejecute.</p> </li> </ul> <p>Por \u00faltimo, deberemos crear un APK o IPA para poder instalar la aplicaci\u00f3n en dispositivos Android o iOS.</p>"},{"location":"adobe-air.html#creacion-de-un-apk-para-android","title":"Creaci\u00f3n de un APK para Android","text":"<p>Para crear el APK en primer lugar deberemos crear un certificado con el que firmar la aplicaci\u00f3n: <pre><code>adt -certificate -validityPeriod 25 -cn SelfSigned 1024-RSA cert.pfx mipassword\n</code></pre></p> <p>Esto deberemos hacerlo una \u00fanica vez. Una vez tengamos un certificado, podremos generar el APK firmado con el siguiente comando: <pre><code>adt -package -target apk -storetype pkcs12 -keystore cert.p12 HolaMundo.apk HolaMundo-app.xml HolaMundo.swf\n</code></pre></p> <p>Podremos copiar este fichero APK a dispositivos Android para instalar la aplicaci\u00f3n en ellos.</p>"},{"location":"adobe-air.html#creacion-de-un-ipa-para-ios","title":"Creaci\u00f3n de un IPA para iOS","text":"<p>En el caso de iOS tendremos que generar un IPA firmado con nuestro certificado de desarrollador. Para ello deberemos exportar el certificado (y la clave privada) desde Acceso a llaveros a un fichero <code>.p12</code>, protegi\u00e9ndolo con una contrase\u00f1a. Al crear el IPA deberemos proporcionar este fichero <code>.p12</code>, su contrase\u00f1a, y el perfil de aprovisionamiento <code>.mobileprovision</code>:</p> <pre><code>adt -package -target ipa-debug\n    -keystore cert.p12 -storetype pkcs12 -storepass mipassword\n    -provisioning-profile ios.mobileprovision\n    HolaMundo.ipa HolaMundo-app.xml HolaMundo.swf iconos Default.png\n</code></pre> <p>Una vez generado el fichero IPA podremos probarlo en un dispositivo m\u00f3vil copi\u00e1ndolo a iTunes y sincronizando con el dispositivo.</p>"},{"location":"adobe-air.html#aplicaciones-adobe-air-con-intellij","title":"Aplicaciones Adobe AIR con IntelliJ","text":"<p>Podemos crear aplicaciones Adobe AIR desde el entorno IntelliJ.</p>"},{"location":"adobe-air.html#creacion-de-un-proyecto","title":"Creaci\u00f3n de un proyecto","text":"<p>Para crea un proyecto AIR en primer lugar seleccionaremos la opci\u00f3n Archivo &gt; New Project... y el tipo de proyecto Flash. Dentro de este tipo configuraremos el proyecto de la siguiente forma:</p> <p></p> <ul> <li>Como Target platform indicaremos Mobile</li> <li>Como Output type indicaremos Application</li> <li>Marcamos las plataformas para las que queremos generar la aplicaci\u00f3n (Android e iOS).</li> <li>Si marcamos la casilla Create sample app nos crear\u00e1 un fichero <code>.mxml</code> como plantilla inicial de la aplicaci\u00f3n.</li> <li>Debemos indicar el SDK de Flex y Adobe AIR a utilizar. Si no tenemos ninguno configurado todav\u00eda deberemos pulsar sobre el bot\u00f3n <code>...</code> junto a este campo para configurar uno. Este proceso de configuraci\u00f3n del SDK se explica en el siguiente apartado.</li> </ul>"},{"location":"adobe-air.html#configuracion-del-sdk","title":"Configuraci\u00f3n del SDK","text":"<p>Si no tenemos configurado todav\u00eda un SDK, pulsaremos sobre el bot\u00f3n <code>...</code> junto al campo de selecci\u00f3n de SDK para configurar uno nuevo. Esto tendremos que hacerlo s\u00f3lo la primera vez que vayamos a crear un proyecto Adobe AIR.</p> <p>En la pantalla Configure SDK a\u00f1adiremos un SDK del tipo Flex/AIR SDK:</p> <p></p> <p>Seleccionaremos en el disco el directorio donde tenemos combinados Flex SDK y AIR SDK.</p> <p>Es recomendable tambi\u00e9n que en la pesta\u00f1a Documentation Paths del SDK que acabamos de a\u00f1adir introduzcamos la URL en la que podemos encontrar la referencia de la API de ActionScript:</p> <p></p> <p>Referencia API ActionScript 3: http://help.adobe.com/en_US/FlashPlatform/reference/actionscript/3/</p>"},{"location":"adobe-air.html#ejecucion-del-proyecto","title":"Ejecuci\u00f3n del proyecto","text":"<p>Una vez creado el proyecto, podemos ejecutarlo en el escritorio mediante la opci\u00f3n Run &gt; Run 'Nombre del proyecto' o pulsando el bot\u00f3n de reproducci\u00f3n en la barra superior derecha:</p> <p></p> <p>Podemos tambi\u00e9n configurar el perfil de ejecuci\u00f3n del proyecto para indicar si queremos ejecutar en emuladores, dispositivos reales, Android o iOS. Esto lo haremos con la opci\u00f3n Run &gt; Edit Configurations.... Nos aparecer\u00e1 una pantalla como la siguiente:</p> <p></p>"},{"location":"adobe-air.html#creacion-de-artefactos","title":"Creaci\u00f3n de artefactos","text":"<p>Podemos a\u00f1adir al proyecto tanto ficheros ActionScript como ficheros MXML, pulsando sobre \u00e9l con el bot\u00f3n derecho y seleccionando la opci\u00f3n New &gt; ActionScript Class o New &gt; MXML Component:</p> <p></p> <p>Si a\u00f1adimos una clase ActionScript podr\u00edamos introducir el c\u00f3digo del Hola Mundo que realizamos en un apartado anterior.</p> <p>Podemos hacer que la clase que hemos creado sea la clase principal de la aplicaci\u00f3n entrando en File &gt; Project Structure... &gt; Modules y modificando el campo Main class de nuestro m\u00f3dulo:</p> <p></p>"},{"location":"adobe-air.html#aplicaciones-multimedia-con-adobe-air","title":"Aplicaciones multimedia con Adobe AIR","text":""},{"location":"adobe-air.html#reproduccion-de-video","title":"Reproducci\u00f3n de v\u00eddeo","text":"<p>Podemos reproducir v\u00eddeo con un objeto de tipo <code>Video</code>. En primer lugar, podemos instanciar un objeto de este tipo y a\u00f1adirlo a la pantalla con <code>addChild</code>. En este ejemplo se muestra este componente abarcando todo el espacio de la escena (stage):</p> <pre><code>video_playback=new Video(stage.width, stage.height);\nvideo_playback.x=0;\nvideo_playback.y=0;\naddChild(video_playback);\n</code></pre> <p>Una vez creado el visor de v\u00eddeo, creamos una conexi\u00f3n para acceder al v\u00eddeo remoto. En caso de querer acceder a video HTTP por descarga progresiva o v\u00eddeo local estableceremos la conexi\u00f3n pasando <code>null</code> como par\u00e1metro a <code>connect</code>. S\u00f3lo pasaremos una direcci\u00f3n cuando vayamos a conectar con un Flash Media Server:</p> <pre><code>nc = new NetConnection();\nnc.connect(null);\n</code></pre> <p>Tras crear la conexi\u00f3n, crearemos a partir de ella un <code>NetStream</code></p> <pre><code>ns_playback=new NetStream(nc);\nns_playback.addEventListener(AsyncErrorEvent.ASYNC_ERROR, asyncErrorHandler);\n</code></pre> <p>Tenemos que proporcionar obligatoriamente un m\u00e9todo para tratar los objetos de error, aunque los ignoremos, ya que de no hacerlo as\u00ed obtendremos un error:</p> <pre><code>function asyncErrorHandler(event:AsyncErrorEvent):void\n{\n}\n</code></pre> <p>Por \u00faltimo, vinculamos el <code>NetStream</code> con el reproductor de v\u00eddeo y comenzamos la reproducci\u00f3n proporcionando la URL del v\u00eddeo a reproducir:</p> <pre><code>video_playback.attachNetStream(ns_playback);\nns_playback.play(\"http://jtech.ua.es/dadm/video.mp4\");\n</code></pre> <p>A continuaci\u00f3n vemos el ejemplo completo del reproductor de v\u00eddeo:</p> <pre><code>public class Reproductor extends Sprite {\nprivate var nc:NetConnection;\nprivate var ns_playback:NetStream;\nprivate var video_playback:Video;\n\npublic function Reproductor() {\n\n// Mantiene la pantalla siempre encendida\nNativeApplication.nativeApplication.systemIdleMode = SystemIdleMode.KEEP_AWAKE;\n\nplaybackNetStream();\n}\n\nprivate function playbackNetStream():void {\n\n// Crea reproductor de video\nvideo_playback=new Video(stage.width,stage.height);\nvideo_playback.x=0;\nvideo_playback.y=0;\naddChild(video_playback);\n\n// Crea flujo de video\nnc = new NetConnection();\nnc.connect(null);\n\nns_playback=new NetStream(nc);\nns_playback.addEventListener(AsyncErrorEvent.ASYNC_ERROR, asyncErrorHandler);\n\n// Muestra el flujo en el reproductor\nvideo_playback.attachNetStream(ns_playback);\nns_playback.play(\"http://jtech.ua.es/dadm/video.mp4\");\n}\n\nfunction asyncErrorHandler(event:AsyncErrorEvent):void\n{\n}\n}\n}\n</code></pre>"},{"location":"adobe-air.html#reproductor-desde-flash-media-server","title":"Reproductor desde Flash Media Server","text":"<p>La reproductor de v\u00eddeo proveniente de un Flash Media Server difiere del caso anterior en que previamente debemos establecer la conexi\u00f3n con este servidor.</p> <p>En primer lugar realizaremos la conexi\u00f3n y definiremos un m\u00e9todo de callback para que nos avise cuando la conexi\u00f3n est\u00e9 establecida:</p> <pre><code>nc = new NetConnection();\nnc.addEventListener(NetStatusEvent.NET_STATUS, netStatusHandler);\n\nnc.connect(connectionURL);\n</code></pre> <p>Implementaremos el callback <code>netStatusHandler</code> de forma que cuando se haya establecido la conexi\u00f3n comience la reproducci\u00f3n del v\u00eddeo:</p> <pre><code>private function netStatusHandler(event:NetStatusEvent):void {\nswitch (event.info.code) {\ncase \"NetConnection.Connect.Success\" :\n// Se ha establecido la conexi\u00f3n\nplaybackVideo();\nbreak;\n}\n}\n</code></pre> <p>La reproducci\u00f3n del v\u00eddeo se har\u00e1 de forma similar al caso anterior:</p> <pre><code>private function playbackVideo():void {\nvideo_playback=new Video(cam.width,cam.height);\nvideo_playback.x=cam.width+20;\nvideo_playback.y=10;\naddChild(video_playback);\n\nns_playback=new NetStream(nc);\nns_playback.addEventListener(NetStatusEvent.NET_STATUS, netStatusHandler);\n\nvideo_playback.attachNetStream(ns_playback);\nns_playback.play(\"mp4:sample.mp4\");\n}\n</code></pre> <p>En este caso, en el m\u00e9todo <code>play</code> indicaremos el nombre del v\u00eddeo a reproducir dentro del Flash Media Server</p>"},{"location":"adobe-air.html#captura-desde-la-camara","title":"Captura desde la c\u00e1mara","text":"<p>Podemos de forma sencilla reproducir en pantalla v\u00eddeo capturado con la c\u00e1mara. En primer lugar crearemos un objeto <code>Camera</code>:</p> <pre><code>cam=Camera.getCamera();\n</code></pre> <p>Este m\u00e9todo obtiene la c\u00e1mara por defecto del dispositivo. Con <code>Camera.names</code> podr\u00edamos ver los nombres de todas las c\u00e1maras disponibles, y con <code>Camera.getCamera(nombre)</code> podr\u00edamos obtener cualquiera de ellas.</p> <p>Tambi\u00e9n podemos configurar la calidad del v\u00eddeo capturado por la c\u00e1mara:</p> <pre><code>cam.setMode(320,240,15);\ncam.setQuality(200000,0);\ncam.setKeyFrameInterval(15);\n</code></pre> <p>De la misma forma, tambi\u00e9n podr\u00edamos especificar la captura del micr\u00f3fono, en este caso mediante un objeto <code>Microphone</code>:</p> <pre><code>mic = Microphone.getMicrophone();\n\nmic.rate = 8;\nmic.setSilenceLevel(0,-1);\n</code></pre> <p>Una vez hecho esto, podemos vincular con <code>attachCamera</code> la c\u00e1mara al visor de v\u00eddeo, y con esto comenzar\u00e1 a mostrarse en pantalla:</p> <pre><code>video_playback=new Video(stage.width,stage.height);\nvideo_playback.x=0;\nvideo_playback.y=0;\n\naddChild(video_playback);\n\nvideo_playback.attachCamera(cam);\n</code></pre> <p>Podemos ver el ejemplo completo a continuaci\u00f3n:</p> <pre><code>private function cameraPlayback():void {\ncam=Camera.getCamera();\n\ncam.setMode(320,240,15);\ncam.setQuality(200000,0);\ncam.setKeyFrameInterval( 15 );\n\nvideo_playback=new Video(stage.width,stage.height);\nvideo_playback.x=0;\nvideo_playback.y=0;\n\naddChild(video_playback);\n\nvideo_playback.attachCamera(cam);\n}\n</code></pre>"},{"location":"adobe-air.html#emision-via-streaming-rtmp","title":"Emisi\u00f3n v\u00eda streaming RTMP","text":"<p>Una vez visto como mostrar v\u00eddeo de una URL, de un servidor Flash Media Server, y de la c\u00e1mara, vamos a ver c\u00f3mo podemos capturar v\u00eddeo de la c\u00e1mara y enviarlo a un servidor RTMP.</p> <p>Lo primero que deberemos hacer es establecer la conexi\u00f3n con el servidor RTMP:</p> <pre><code>nc = new NetConnection();\nnc.addEventListener(NetStatusEvent.NET_STATUS, netStatusHandler);\nnc.connect(\"rtmp://www.eps.ua.es:1935/live\");\n</code></pre> <p>Definimos el siguiente callback con el que estaremos pendientes del momento en el que se establece la conexi\u00f3n con el servidor, para empezar a publicar, y del momento en el que comienza la publicaci\u00f3n, para comenzar a mostrar un preview en nuestra aplicaci\u00f3n:</p> <pre><code>private function netStatusHandler(event:NetStatusEvent):void {\n\nswitch (event.info.code) {\ncase \"NetConnection.Connect.Success\" :\n// Se ha establecido la conexi\u00f3n\npublishVideo();\nbreak;\ncase \"NetStream.Publish.Start\" :\n// Ha comenzado la emisi\u00f3n\nplaybackVideo();\nbreak;\n}\n}\n</code></pre> <p>Cuando se haya establecido la conexi\u00f3n podremos comenzar la publicaci\u00f3n de v\u00eddeo. Esto lo haremos con el m\u00e9todo <code>publish</code> de la clase <code>NetStream</code>. Antes podremos configurar la calidad del v\u00eddeo que vamos a retransmitir:</p> <pre><code>private function publishVideo():void {\nns_publish=new NetStream(nc);\n\nvar h264Settings:H264VideoStreamSettings = new H264VideoStreamSettings();\nh264Settings.setProfileLevel( H264Profile.BASELINE, H264Level.LEVEL_3_1 );\nns_publish.videoStreamSettings = h264Settings;\n\nns_publish.addEventListener(NetStatusEvent.NET_STATUS, netStatusHandler);\n\ncam=Camera.getCamera();\n\ncam.setMode(320,240,15);\ncam.setQuality(200000,0);\ncam.setKeyFrameInterval( 15 );\n\nmic = Microphone.getMicrophone();\n\nmic.rate = 8;\nmic.setSilenceLevel(0,-1);\n\nns_publish.attachCamera(cam);\nns_publish.attachAudio(mic);\nns_publish.publish(\"miCanal.stream\");\n}\n</code></pre> <p>Una vez detectemos que la publicaci\u00f3n ha comenzado, podr\u00edamos reproducir el v\u00eddeo publicado en el servidor:</p> <pre><code>private function playbackVideo():void {\n\nvideo_playback=new Video(stage.width,stage.height);\nvideo_playback.x=0;\nvideo_playback.y=0;\naddChild(video_playback);\n\nns_playback=new NetStream(nc);\nns_playback.addEventListener(NetStatusEvent.NET_STATUS, netStatusHandler);\n\nvideo_playback.attachNetStream(ns_playback);\nns_playback.play(\"miCanal.stream\");\n}\n</code></pre> <p>A continuaci\u00f3n se muestra el ejemplo completo:</p> <pre><code>public class MiStream extends Sprite {\nprivate var connectionURL:String=\"rtmp://www.eps.ua.es/live\";\nprivate var videoURL:String=\"miCanal.stream\";\nprivate var nc:NetConnection;\nprivate var ns_publish:NetStream;\nprivate var ns_playback:NetStream;\nprivate var video_playback:Video;\nprivate var cam:Camera;\nprivate var mic:Microphone;\n\npublic function MiStream() {\n\n// Mantiene la pantalla siempre encendida\nNativeApplication.nativeApplication.systemIdleMode = SystemIdleMode.KEEP_AWAKE;\n\n// Establece conexion con la URL\n\nnc = new NetConnection();\nnc.addEventListener(NetStatusEvent.NET_STATUS, netStatusHandler);\nnc.connect(connectionURL);\n}\n\nprivate function netStatusHandler(event:NetStatusEvent):void {\n\nswitch (event.info.code) {\ncase \"NetConnection.Connect.Success\" :\n// Se ha establecido la conexi\u00f3n\npublishVideo();\nbreak;\ncase \"NetStream.Publish.Start\" :\n// Ha comenzado la emisi\u00f3n\nplaybackVideo();\nbreak;\n}\n}\n\nprivate function publishVideo():void {\nns_publish=new NetStream(nc);\n\nvar h264Settings:H264VideoStreamSettings = new H264VideoStreamSettings();\nh264Settings.setProfileLevel( H264Profile.BASELINE, H264Level.LEVEL_3_1 );\nns_publish.videoStreamSettings = h264Settings;\n\nns_publish.addEventListener(NetStatusEvent.NET_STATUS, netStatusHandler);\n\ncam=Camera.getCamera();\n\ncam.setMode(320,240,15);\ncam.setQuality(200000,0);\ncam.setKeyFrameInterval( 15 );\n\nmic = Microphone.getMicrophone();\n\nmic.rate = 8;\nmic.setSilenceLevel(0,-1);\n\nns_publish.attachCamera(cam);\nns_publish.attachAudio(mic);\nns_publish.publish(videoURL);\n}\n\nprivate function playbackVideo():void {\nvideo_playback=new Video(stage.width,stage.height);\nvideo_playback.x=0;\nvideo_playback.y=0;\naddChild(video_playback);\n\nns_playback=new NetStream(nc);\nns_playback.addEventListener(NetStatusEvent.NET_STATUS, netStatusHandler);\n\nvideo_playback.attachNetStream(ns_playback);\nns_playback.play(videoURL);\n}\n}\n</code></pre>"},{"location":"adobe-air.html#autenticacion-de-la-fuente-de-video","title":"Autenticaci\u00f3n de la fuente de v\u00eddeo","text":"<p>Si requerimos usuario y password para poder publicar v\u00eddeo RTMP en Wowza desde nuestra aplicaci\u00f3n Adobe AIR, estas credenciales se pueden proporcionar como par\u00e1metros del m\u00e9todo <code>NetConnection.connect</code>:</p> <pre><code>nc.connect(connectionURL, \"publisher\", \"mastermoviles\");\n</code></pre> <p>Sin embargo, en Wowza no podremos utilizar la configuraci\u00f3n de autenticaci\u00f3n est\u00e1ndar en RTMP, sino que necesitaremos a\u00f1adir un m\u00f3dulo adicional. De hecho, deberemos dejar abierto el acceso a la aplicaci\u00f3n v\u00eda RTMP desde la interfaz de Wowza para poder utilizar dicho m\u00f3dulo. Para ello:</p> <ul> <li>Entramos en el administrador de Wowza, en la p\u00e1gina de nuestra aplicaci\u00f3n, por ejemplo <code>live</code>.</li> <li>Entramos en la secci\u00f3n Source Security y editamos la configuraci\u00f3n.</li> <li>En RTMP Sources indicamos Open (no authentication required).</li> </ul> <p>Incorporaremos ahora la autenticaci\u00f3n RTMP mediante el m\u00f3dulo adicional <code>moduleOnConnectAuthenticate</code>. Dicho m\u00f3dulo se encuentra dentro de una colecci\u00f3n de m\u00f3dulos adicionales que se proporcionan para Wowza en la siguiente p\u00e1gina:</p> <p>http://www.wowza.com/forums/content.php?113-Module-Collection</p> <p>Para instalar y configurar el m\u00f3dulo de autenticaci\u00f3n deberemos:</p> <ul> <li>Descargar e instalar la colecci\u00f3n de m\u00f3dulos anterior. Deberemos copiar el fichero <code>wms-plugin-collection.jar</code> en el directorio <code>${WOWZA_HOME}/lib</code>, siendo <code>${WOWZA_HOME}</code> el directorio de instalaci\u00f3n de Wowza.</li> <li>A\u00f1adimos la configuraci\u00f3n del m\u00f3dulo al fichero <code>Application.xml</code> de la aplicaci\u00f3n a configurar. Por ejemplo, si queremos configurar la aplicaci\u00f3n <code>live</code>, buscaremos dicho fichero en <code>${WOWZA_HOME}/conf/live</code>. En el fichero <code>Application.xml</code>, introduciremos la siguiente configuraci\u00f3n dentro de la etiqueta <code>&lt;Modules&gt; ... &lt;/Modules&gt;</code>: <pre><code>&lt;Module&gt;\n&lt;Name&gt;moduleOnConnectAuthenticate&lt;/Name&gt;\n&lt;Description&gt;Authenticates Flash connections.&lt;/Description&gt;\n&lt;Class&gt;com.wowza.wms.plugin.collection.module.ModuleOnConnectAuthenticate&lt;/Class&gt;\n&lt;/Module&gt;\n</code></pre></li> <li>Por defecto buscar\u00e1 la lista de usuarios en el fichero <code>${WOWZA_HOME}/conf/connect.password</code>. Podemos copiar el fichero <code>publish.password</code> que se encuentra en el mismo directorio para utilizar los mismos usuarios para publicaci\u00f3n que hemos configurado en Wowza.</li> </ul> <p>Despu\u00e9s de hacer esto reiniciaremos el servidor y tendremos habilitada la autentificaci\u00f3n de fuentes RTMP con compatibilidad para el m\u00e9todo <code>NetConnection.connect</code>.</p>"},{"location":"adobe-air.html#configuracion-de-la-aplicacion-android","title":"Configuraci\u00f3n de la aplicaci\u00f3n Android","text":"<p>En el descriptor XML de la aplicaci\u00f3n podemos especificar todos los permisos que necesitar\u00edamos para esta aplicaci\u00f3n en la plataforma Android. Podr\u00edamos tener algo como lo siguiente:</p> <pre><code>&lt;application xmlns=\"http://ns.adobe.com/air/application/13.0\"&gt;\n&lt;id&gt;es.ua.eps.MiStream&lt;/id&gt;\n&lt;versionNumber&gt;0.0.1&lt;/versionNumber&gt;\n&lt;filename&gt;MiStream&lt;/filename&gt;\n&lt;initialWindow&gt;\n&lt;content&gt;MiStream.swf&lt;/content&gt;\n&lt;fullScreen&gt;true&lt;/fullScreen&gt;\n&lt;aspectRatio&gt;any&lt;/aspectRatio&gt;\n&lt;/initialWindow&gt;\n&lt;supportedProfiles&gt;mobileDevice&lt;/supportedProfiles&gt;\n\n&lt;android&gt;\n&lt;manifestAdditions&gt;\n&lt;![CDATA[\n            &lt;manifest&gt;\n                &lt;uses-feature android:name=\"android.hardware.camera\"/&gt;\n                &lt;uses-feature android:name=\"android.hardware.microphone\"/&gt;\n                &lt;uses-feature android:name=\"android.hardware.camera.autofocus\"/&gt;\n                &lt;uses-permission android:name=\"android.permission.ACCESS_CHECKIN_PROPERTIES\"/&gt;\n                &lt;uses-permission android:name=\"android.permission.ACCESS_NETWORK_STATE\"/&gt;\n                &lt;uses-permission android:name=\"android.permission.CAMERA\"/&gt;\n                &lt;uses-permission android:name=\"android.permission.CHANGE_WIFI_MULTICAST_STATE\"/&gt;\n                &lt;uses-permission android:name=\"android.permission.DISABLE_KEYGUARD\"/&gt;\n                &lt;uses-permission android:name=\"android.permission.INTERNET\"/&gt;\n                &lt;uses-permission android:name=\"android.permission.MODIFY_AUDIO_SETTINGS\"/&gt;\n                &lt;uses-permission android:name=\"android.permission.RECORD_AUDIO\"/&gt;\n                &lt;uses-permission android:name=\"android.permission.VIBRATE\"/&gt;\n                &lt;uses-permission android:name=\"android.permission.WAKE_LOCK\"/&gt;\n                &lt;uses-permission android:name=\"android.permission.WRITE_EXTERNAL_STORAGE\"/&gt;\n                &lt;uses-permission android:name=\"com.google.android.glass.permission.DEVELOPMENT\"/&gt;\n\n                &lt;application&gt;\n                    &lt;activity&gt;\n                        &lt;intent-filter&gt;\n                            &lt;action android:name=\"android.intent.action.MAIN\"/&gt;\n                            &lt;category android:name=\"android.intent.category.LAUNCHER\"/&gt;\n                        &lt;/intent-filter&gt;\n                        &lt;intent-filter&gt;\n                            &lt;action android:name=\"android.intent.action.VIEW\"/&gt;\n                            &lt;category android:name=\"android.intent.category.BROWSABLE\"/&gt;\n                            &lt;category android:name=\"android.intent.category.DEFAULT\"/&gt;\n                            &lt;data android:scheme=\"ggstream\"/&gt;\n                        &lt;/intent-filter&gt;\n                    &lt;/activity&gt;\n                &lt;/application&gt;\n\n            &lt;/manifest&gt;\n        ]]&gt;\n&lt;/manifestAdditions&gt;\n&lt;/android&gt;\n&lt;/application&gt;\n</code></pre>"},{"location":"adobe-air.html#documentacion-de-adobe-air","title":"Documentaci\u00f3n de Adobe AIR","text":"<p>Para m\u00e1s informaci\u00f3n sobre Adobe AIR, gu\u00edas y tutoriales, se puede acceder a la documentaci\u00f3n oficial de la plataforma:</p> <p>http://www.adobe.com/devnet/air/documentation.html</p>"},{"location":"captura-android.html","title":"Captura de medios en Android","text":"<p>En esta sesi\u00f3n nos centraremos en estudiar los m\u00e9todos para capturar audio y v\u00eddeo desde un dispositivo Android. Las \u00faltimas versiones del SDK de Android permiten emular la captura de video y audio en nuestros dispositivos virtuales, lo cual facilitar\u00e1 las pruebas de este tipo de aplicaciones. En concreto, es posible realizar esta simulaci\u00f3n por medio de una webcam, que se utilizar\u00e1 para captar lo que se supone que estar\u00eda captando la c\u00e1mara del dispositivo real.</p> <p>Existen dos formas b\u00e1sicas de capturar medios:</p> <ul> <li>Utilizar un intent para llamar a la aplicaci\u00f3n de captura y obtener el resultado que \u00e9sta nos proporcione.</li> <li>Crear nuestra propia aplicaci\u00f3n de captura. Este m\u00e9todo es m\u00e1s complejo pero nos permite personalizar la forma en la que se realiza la captura.</li> </ul>"},{"location":"captura-android.html#captura-de-medios-mediante-intents","title":"Captura de medios mediante intents","text":"<p>La forma m\u00e1s sencilla de capturar medios es lanzando la aplicaci\u00f3n nativa que realiza esta tarea mediante un intent. Al lanzar el intent deberemos indicar mediante un par\u00e1metro _extra_ el lugar donde queremos guardar el medio capturado. Deberemos seleccionar la ubicaci\u00f3n adecuada seg\u00fan el uso que le queramos dar.</p>"},{"location":"captura-android.html#almacenamiento-de-medios","title":"Almacenamiento de medios","text":"<p>En cualquier caso los medios deber\u00e1n ser almacenados en la tarjeta SD para no gastar el almacenamiento externo y permitir a los usuarios tener acceso a los medios capturados. B\u00e1sicamente tenemos dos opciones:</p> <ul> <li><code>Environment.getExternalStoragePublicDirectory(Environment.DIRECTORY_PICTURES)</code> indica el directorio externo donde almacenar los medios de forma p\u00fablica e independiente de la aplicaci\u00f3n. Se recomienda crear dentro de estas carpeta un subdirectorio por cada aplicaci\u00f3n.</li> </ul> <p>Este m\u00e9todo s\u00f3lo esta disponible a partir de Android 2.2 API 8API 8. Si queremos compatibilidad con APIs anteriores utilizaremos Environment.getExternalStorageDirectory()</p> <ul> <li><code>Context.getExternalFilesDir(Environment.DIRECTORY_PICTURES)</code> nos indica un directorio propio de la aplicaci\u00f3n donde almacenar los medios. Si la aplicaci\u00f3n se desinstala, todos los medios almacenados en este directorio se borrar\u00e1n.</li> </ul> <p>Para poder escribir en el soporte de almacenamiento externo deberemos solicitar el siguiente permiso:</p> <pre><code>&lt;uses-permission android:name=\"android.permission.WRITE_EXTERNAL_STORAGE\" /&gt;\n</code></pre> <p>En los dispositivos Android tambi\u00e9n contamos con un proveedor de contenido denominado Media Store. Este proveedor de contenidos consiste en una base de datos de medios almacenados en el dispositivo donde podremos registrar aquellos medios que capturemos desde nuestra aplicaci\u00f3n, de forma que sean f\u00e1cilmente accesibles desde cualquier otra aplicaci\u00f3n del dispositivo. M\u00e1s adelante veremos como registrar un fichero fotograf\u00eda, audio o v\u00eddeofotograf\u00eda, audio o v\u00eddeo en esta base de datos.</p>"},{"location":"captura-android.html#toma-de-fotografias","title":"Toma de fotograf\u00edas","text":"<p>En esta secci\u00f3n veremos c\u00f3mo tomar fotograf\u00edas desde nuestra aplicaci\u00f3n y utilizar la imagen obtenida para realizar alguna tarea. Como veremos se tratar\u00e1 ni m\u00e1s ni menos que un ejemplo clar\u00edsimo de intent impl\u00edcito, en el que pediremos al sistema que se lance una actividad que pueda tomar fotograf\u00edas. Por medio de este mecanismo de comunicaci\u00f3n obtendremos la imagen capturada o una direcci\u00f3n a la localizaci\u00f3n de la misma en el dispositivoo una direcci\u00f3n a la localizaci\u00f3n de la misma en el dispositivo para trabajar con ellas.</p> <p>Hoy en d\u00eda es posible simular la c\u00e1mara del dispositivo virtual por medio de una webcam, as\u00ed que no es necesario utilizar un dispositivo real para poder probar estos ejemplos.</p> <p>La acci\u00f3n a solicitar mediante el <code>Intent</code> impl\u00edcito ser\u00e1 <code>MediaStore.ACTION_IMAGE_CAPTURE</code> m\u00e1s adelante hablaremos de la clase <code>MediaStore</code>m\u00e1s adelante hablaremos de la clase &lt;code&gt;MediaStore&lt;/code&gt;. Lanzaremos el <code>Intent</code> por medio del m\u00e9todo <code>startActivityForResult</code>, con lo que en realidad estaremos haciendo uso de una subactividad. Recuerda que esto ten\u00eda como consecuencia que al terminar la subactividad se invoca el m\u00e9todo <code>onActivityResult</code> de la actividad padre. En este caso el identificador que se le ha dado a la subactividad es <code>TAKE_PICTURE</code>, que se habr\u00e1 definido como una constante en cualquier otro lugar de la clase:</p> <pre><code>startActivityForResult(new Intent(MediaStore.ACTION_IMAGE_CAPTURE), TAKE_PICTURE);\n</code></pre> <p>Si no hemos hecho ning\u00fan cambio al respecto en nuestro sistema, esta llamada lanzar\u00e1 la actividad nativa para la toma de fotograf\u00edas. No podemos evitar recordar una vez m\u00e1s la ventaja que esto supone para el desarrollador Android, ya que en lugar de tener que desarrollar una nueva actividad para la captura de im\u00e1genes desde cero, es posible hacer uso de los recursos del sistema.</p> <p>Seg\u00fan los par\u00e1metros del <code>Intent</code> anterior, podemos hablar de dos modos de funcionamiento en cuanto a la toma de fotograf\u00edas:</p> <ul> <li>Modo thumbnail: este es el modo de funcionamiento por defecto. El <code>Intent</code> devuelto como respuesta por la subactividad, al que podremos acceder   desde <code>onActivityResult</code>, contendr\u00e1 un par\u00e1metro extra de nombre <code>data</code>, que consistir\u00e1 en un thumbnail de tipo <code>Bitmap</code>.</li> <li>Modo de imagen completa: la captura de im\u00e1genes se realizar\u00e1 de esta forma si se especifica una URI como valor del par\u00e1metro extra <code>MediaStore.EXTRA_OUTPUT</code> del <code>Intent</code> usado para lanzar la actividad de toma de fotograf\u00edas. En este caso se guardar\u00e1 la imagen obtenida por la c\u00e1mara, en su resoluci\u00f3n completa, en el destino indicado en dicho par\u00e1metro extra.  En este caso el <code>Intent</code> de respuesta no se usar\u00e1 para devolver un thumbnail, y por lo tanto el par\u00e1metro extra <code>data</code> tendr\u00e1 como valor <code>null</code>.</li> </ul> <p>En el siguiente ejemplo tenemos el esqueleto de una aplicaci\u00f3n en el que se utiliza un <code>Intent</code> para tomar una fotograf\u00eda, ya sea en modo thumbnail o en modo de imagen completa. Seg\u00fan queramos una cosa o la otra deberemos llamar a los m\u00e9todos <code>getThumbnailPicture</code> o <code>saveFullImage</code>, respectivamente. En <code>onActivityResult</code> se determina el modo empleado examinando el valor del campo extra <code>data</code> del <code>Intent</code> de respuesta. Por \u00faltimo, una vez tomada la fotograf\u00eda, se puede almacenar en el Media Store hablamos de esto un poco m\u00e1s adelantehablamos de esto un poco m\u00e1s adelante o procesarla dentro de nuestra aplicaci\u00f3n antes de descartarla.</p> <pre><code>private static int TAKE_PICTURE = 1;\nprivate Uri ficheroSalidaUri;\n\nprivate void getThumbailPicture() {\nIntent intent = new Intent(MediaStore.ACTION_IMAGE_CAPTURE);\nstartActivityForResult(intent, TAKE_PICTURE);\n}\n\nprivate void saveFullImage() {\nIntent intent = new Intent(MediaStore.ACTION_IMAGE_CAPTURE);\nFile file = new File(Environment.getExternalStorageDirectory(), \"prueba.jpg\");\nficheroSalidaUri = Uri.fromFile(file);\nintent.putExtra(MediaStore.EXTRA_OUTPUT, ficheroSalidaUri);\nstartActivityForResult(intent, TAKE_PICTURE);\n}\n\n@Override\nprotected void onActivityResult(int requestCode, int resultCode, Intent data) {\nif (requestCode == TAKE_PICTURE) {\nUri imagenUri = null;\n// Comprobamos si el Intent ha devuelto un thumbnail\nif (data != null) {\nif (data.hasExtra(\"data\")) {\nBitmap thumbnail = data.getParcelableExtra(\"data\");\n// HACER algo con el thumbnail\n}\n}\nelse {\n// HACER algo con la imagen almacenada en ficheroSalidaUri\n}\n}\n}\n</code></pre>"},{"location":"captura-android.html#captura-de-video","title":"Captura de v\u00eddeo","text":"<p>La manera m\u00e1s sencilla de comenzar a grabar v\u00eddeo es mediante la constante <code>ACTION_VIDEO_CAPTURE</code> definida en la clase <code>MediaStore</code>, que deber\u00e1 utilizarse conjuntamente con un <code>Intent</code> que se pasar\u00e1 como par\u00e1metro a <code>startActivityForResult</code>:</p> <pre><code>startActivityForResult(new Intent(MediaStore.ACTION_VIDEO_CAPTURE), GRABAR_VIDEO);\n</code></pre> <p>Esto lanzar\u00e1 la aplicaci\u00f3n nativa de grabaci\u00f3n de v\u00eddeos en Android, permitiendo al usuario comenzar o detener la grabaci\u00f3n, revisar lo que se ha grabado, y volver a comenzar la grabaci\u00f3n en el caso en el que se desee. La ventaja como desarrolladores ser\u00e1 la misma de siempre: al utilizar el componente nativo nos ahorramos el tener que desarrollar una actividad para la captura de v\u00eddeo desde cero.</p> <p>La acci\u00f3n de captura de v\u00eddeo que se pasa como par\u00e1metro al <code>Intent</code> acepta dos p\u00e1rametros extra opcionales, cuyos identificadores se definen como constantes en la clase <code>MediaStore</code>:</p> <ul> <li><code>EXTRA_OUTPUT</code>: por defecto el v\u00eddeo grabado ser\u00e1 guardado en el Media Store. Para almacenarlo en cualquier otro lugar indicaremos una URI como par\u00e1metro extra utilizando este identificador.</li> <li><code>EXTRA_VIDEO_QUALITY</code>: mediante un entero podemos especificar la calidad del v\u00eddeo capturado. S\u00f3lo hay dos valores posibles: 0 para tomar v\u00eddeos en baja resoluci\u00f3n   y 1 para tomar v\u00eddeos en alta resoluci\u00f3n este \u00faltimo valor es el que se toma por defectoeste \u00faltimo valor es el que se toma por defecto.</li> </ul> <p>A continuaci\u00f3n se puede ver un ejemplo en el que se combinan todos estos conceptos vistos hasta ahora:</p> <pre><code>private static int GRABAR_VIDEO = 1;\nprivate static int ALTA_CALIDAD = 1;\nprivate static int BAJA_CALIDAD = 0;\n\nprivate void guardarVideo(Uri uri) {\nIntent intent = new Intent(MediaStore.ACTION_VIDEO_CAPTURE);\n\n// Si se define una uri se especifica que se desea almacenar el\n// v\u00eddeo en esa localizaci\u00f3n. En caso contrario se har\u00e1 uso\n// del Media Store\nif (uri != null)\nintent.putExtra(MediaStore.EXTRA_OUTPUT, output);\n\n// En la siguiente l\u00ednea podr\u00edamos utilizar cualquiera de las\n// dos constantes definidas anteriormente: ALTA_CALIDAD o BAJA_CALIDAD\nintent.putExtra(MediaStore.EXTRA_VIDEO_QUALITY, ALTA_CALIDAD);\n\nstartActivityForResult(intent, GRABAR_VIDEO);\n}\n\n@Override\nprotected void onActivityResult(int requestCode, int resultCode, Intent data) {\nif (requestCode == GRABAR_VIDEO) {\nUri videoGrabado = data.getData();\n// Hacer algo con el v\u00eddeo\n}\n}\n</code></pre>"},{"location":"captura-android.html#captura-de-medios-desde-nuestra-actividad","title":"Captura de medios desde nuestra actividad","text":"<p>En el caso en el que se desee reemplazar la aplicaci\u00f3n nativa de captura o se quiera tener m\u00e1s control sobre la grabaci\u00f3n ser\u00e1 posible utilizar la clase <code>MediaRecorder</code>.</p> <p>Para poder capturar audio y video desde nuestra propia actividad, sin lanzar la aplicaci\u00f3n nativa, ser\u00e1 necesario incluir los siguientes permisos en el manifest:</p> <pre><code>&lt;uses-permission android:name=\"android.permission.CAMERA\"/&gt;\n&lt;uses-permission android:name=\"android.permission.RECORD_AUDIO\"/&gt;\n</code></pre>"},{"location":"captura-android.html#la-camara","title":"La c\u00e1mara","text":"<p>Tanto si queremos tomar fotograf\u00edas como grabar v\u00eddeo necesitaremos tener acceso a la c\u00e1mara del dispositivo. Para ello deberemos solicitar el permiso <code>CAMERA</code>:</p> <pre><code>&lt;uses-permission android:name=\"android.permission.CAMERA\" /&gt;\n</code></pre> <p>Tambi\u00e9n deberemos indicar en el manifest que la aplicaci\u00f3n utiliza la caracter\u00edstica de la c\u00e1mara:</p> <pre><code>&lt;uses-feature android:name=\"android.hardware.camera\" /&gt;\n</code></pre> <p>Podemos especificar diferentes requerimientos de hardware de la c\u00e1mara para nuestra aplicaci\u00f3n:</p> <ul> <li>En el caso de que la aplicaci\u00f3n pueda utilizar la c\u00e1mara, pero no sea obligatorio contar con una c\u00e1mara para poder ejecutar nuestra aplicaci\u00f3n, podremos indicar que dicha caracter\u00edstica no es obligatoria con:</li> </ul> <pre><code>&lt;uses-feature android:name=\"android.hardware.camera\" android:required=\"false\" /&gt;\n</code></pre> <p>Dentro del c\u00f3digo de la aplicaci\u00f3n, podremos detectar si la c\u00e1mara est\u00e1 presente con:</p> <pre><code>boolean tieneCamara = context.getPackageManager().hasSystemFeature(PackageManager.FEATURE_CAMERA);\n</code></pre> <ul> <li>Actualmente es habitual encontrar dispositivos que incorporan m\u00e1s de una c\u00e1mara. Si necesitamos que el dispositivo cuente con c\u00e1mara frontal se puede especificar el siguiente requerimiento:</li> </ul> <pre><code>&lt;uses-feature android:name=\"android.hardware.camera.front\" /&gt;\n</code></pre> <p>Podemos consultar el n\u00famero de c\u00e1maras con las que cuenta el dispositivo con:</p> <pre><code>int numCamaras = Camera.getNumberOfCameras();\n</code></pre> <ul> <li>Tambi\u00e9n podemos solicitar otras caracter\u00edsticas, como que cuente con autofocus o flash.</li> </ul> <p>Para acceder a la c\u00e1mara utilizaremos el m\u00e9todo <code>Camera.open()</code>. Este m\u00e9todo abrir\u00e1 la c\u00e1mara principal del dispositivo, si queremos utilizar otra c\u00e1mara podremos utilizar <code>Camara.open(int numCamara)</code>.</p> <p>Podemos utilizar el objeto <code>Camera</code> para mostrar la previsualizaci\u00f3n de la c\u00e1mara mientras hacemos captura de foto o v\u00eddeo. Para ello necesitaremos una vista de tipo <code>SurfaceView</code>:</p> <pre><code>public class CameraPreview extends SurfaceView\nimplements SurfaceHolder.Callback {\nprivate SurfaceHolder mHolder;\nprivate Camera mCamera;\n\npublic CameraPreview(Context context, Camera camera) {\nsuper(context);\nmCamera = camera;\n\nmHolder = getHolder();\nmHolder.addCallback(this);\n}\n\npublic void surfaceCreated(SurfaceHolder holder) {\n// Pone en marcha el preview de la camara\ntry {\nmCamera.setPreviewDisplay(holder);\nmCamera.startPreview();\n} catch (IOException e) { }\n}\n\npublic void surfaceDestroyed(SurfaceHolder holder) {\n}\n\npublic void surfaceChanged(SurfaceHolder holder, int format, int w, int h) {\nif (mHolder.getSurface() != null){\n\n// Detiene el preview con el formato anterior\ntry {\nmCamera.stopPreview();\n} catch (Exception e){ }\n\n// Reanuda el preview con el nuevo formato\ntry {\nmCamera.setPreviewDisplay(mHolder);\nmCamera.startPreview();\n} catch (Exception e){ }\n}\n}\n}\n</code></pre> <p>Podemos utilizar la vista anterior en una actividad:</p> <pre><code>public class CapturaActivity extends Activity {\n\nprivate Camera mCamera;\nprivate CameraPreview mPreview;\n\n@Override\npublic void onCreate(Bundle savedInstanceState) {\nsuper.onCreate(savedInstanceState);\ntry {\nmCamera = Camera.open();\nmPreview = new CameraPreview(this, mCamera);\nsetContentView(mPreview);\n} catch(Exception e) {\n// No tiene acceso a la camara\n}\n}\n}\n</code></pre> <p>Dentro de esta actividad podremos capturar fotos directamente a trav\u00e9s del objeto <code>Camera</code> mientras se previsualiza, o capturar v\u00eddeo utilizando un objeto <code>MediaRecorder</code>.</p> <p>Es importante liberar la c\u00e1mara cuando no se vaya a utilizar m\u00e1s con <code>Camera.release()</code>. Un lugar adecuado para hacer esto puede ser el m\u00e9todo <code>onDestroy()</code> de la actividad:</p> <pre><code>        @Override\npublic void onDestroy() {\nsuper.onDestroy();\nmCamera.release();\n}\n</code></pre>"},{"location":"captura-android.html#captura-de-fotografias","title":"Captura de fotograf\u00edas","text":"<p>Podemos utilizar el objeto <code>Camera</code> para tomar fotograf\u00edas. Para ello llamaremos al m\u00e9todo <code>takePicture</code> pasando como par\u00e1metro un listener de tipo <code>PictureCallback</code>, al que se llamar\u00e1 cuando la imagen haya sido tomada:</p> <pre><code>mCamera.takePicture(null, null, new PictureCallback() {\n\n@Override\npublic void onPictureTaken(byte[] data, Camera camera) {\n// Grabar data en directorio de medios\n...\n}\n});\n</code></pre> <p>El primer par\u00e1metro de <code>takePicture</code> es opcional y nos permite definir un callback para el obturador. El segundo y tercer par\u00e1metro sirve para especificar un callback para guardar la imagen en formato RAW y JPEG respectivamente.</p> <p>Por ejemplo, podr\u00edamos guardar la imagen JPEG en el almacenamiento externo con:</p> <pre><code>try {\nFile file = new File(Environment.getExternalStorageDirectory(),\n\"prueba.jpg\");\nFileOutputStream fos = new FileOutputStream(file);\nfos.write(data);\nfos.close();\n} catch (Exception e) { }\n</code></pre>"},{"location":"captura-android.html#api-de-camara-2","title":"API de Camara 2","text":"<p>A partir de Android 5.0 _Lollipop__Lollipop_ aparece una nueva versi\u00f3n de API de c\u00e1mara <code>android.hardware.camera2</code>&lt;code&gt;android.hardware.camera2&lt;/code&gt;, que nos permite tener un mayor control sobre este dispositivo, pasando la antigua API a estar desaprobada. Sin embargo, la nueva API no es compatible con versiones anteriores de Android, por lo que si queremos mantener la compatibilidad con versiones de Android anteriores a la 5.0 API 21API 21 deberemos utilizar la antigua c\u00e1mara, o bien c\u00f3digo separado para cada versi\u00f3n:</p> <pre><code>if (Build.VERSION.SDK_INT &gt;= Build.VERSION_CODES.LOLLIPOP) {\n// Camara 2\n} else {\n// Camara 1\n}\n</code></pre> <p>La nueva API de c\u00e1mara deja desaprobada la clase <code>Camera</code>, y en su lugar utilizar <code>CameraDevice</code>. Para acceder a las c\u00e1maras disponibles se proporciona la clase <code>CameraManager</code>.</p>"},{"location":"captura-android.html#captura-de-video-con-mediarecorder","title":"Captura de v\u00eddeo con <code>MediaRecorder</code>","text":"<p>Podemos utilizar la clase <code>MediaRecorder</code> para capturar audio o video desde nuestras propias actividades. La creaci\u00f3n de un objeto de esta clase es sencilla:</p> <pre><code>MediaRecorder mediaRecorder = new MediaRecorder();\n</code></pre> <p>La clase <code>MediaRecorder</code> permite especificar el origen del audio o v\u00eddeo, el formato del fichero de salida y los codecs a utilizar. Como en el caso de la clase <code>MediaPlayer</code>, la clase <code>MediaRecorder</code> maneja la grabaci\u00f3n mediante una m\u00e1quina de estados. Esto quiere decir que el orden en el cual se inicializa y se realizan operaciones con los objetos de este tipo es importante. En resumen, los pasos para utilizar un objeto <code>MediaRecorder</code> ser\u00edan los siguientes:</p> <ul> <li>Crear un nuevo objeto <code>MediaRecorder</code>.</li> <li>Asignarle las fuentes a partir de las cuales grabar el contenido.</li> <li>Definir el formato de salida.</li> <li>Especificar las caracter\u00edsticas del v\u00eddeo: codec, framerate y resoluci\u00f3n de salida.</li> <li>Seleccionar un fichero de salida.</li> <li>Prepararse para la grabaci\u00f3n.</li> <li>Realizar la grabaci\u00f3n.</li> <li>Terminar la grabaci\u00f3n.</li> </ul> <p>Una vez finalizamos la grabaci\u00f3n hemos de hacer uso del m\u00e9todo <code>release</code> del objeto <code>MediaRecorder</code> para liberar todos sus recursos asociados:</p> <pre><code>mediaRecorder.release();\n</code></pre>"},{"location":"captura-android.html#configurando-y-controlando-la-grabacion-de-video","title":"Configurando y controlando la grabaci\u00f3n de v\u00eddeo","text":"<p>Como se ha indicado anteriormente, antes de grabar se deben especificar la fuente de entrada, el formato de salida, el codec de audio o v\u00eddeo y el fichero de salida, en ese estricto orden.</p> <p>Los m\u00e9todos <code>setAudioSource</code> y <code>setVideoSource</code> permiten especificar la fuente de datos por medio de constantes est\u00e1ticas definidas en  <code>MediaRecorder.AudioSource</code> y <code>MediaRecorder.VideoSource</code>, respectivamente. El siguiente paso consiste en especificar el formato de salida por medio del m\u00e9todo <code>setOutputFormat</code> que recibir\u00e1 como par\u00e1metro una constante entre las definidas en <code>MediaRecorder.OutputFormat</code>. A continuaci\u00f3n usamos el m\u00e9todo <code>setAudioEnconder</code> o <code>setVideoEncoder</code> para especificar el codec usado para la grabaci\u00f3n, utilizando alguna de las constantes definidas en <code>MediaRecorder.AudioEncoder</code> o <code>MediaRecorder.VideoEncoder</code>, respectivamente. Es en este punto en el que podremos definir el framerate o la resoluci\u00f3n de salida si se desea. Finalmente indicamos la localizaci\u00f3n del fichero donde se guardar\u00e1 el contenido grabado por medio del m\u00e9todo <code>setOutputFile</code>. El \u00faltimo paso antes de la grabaci\u00f3n ser\u00e1 la invocaci\u00f3n del m\u00e9todo <code>prepare</code>.</p> <p>El siguiente c\u00f3digo muestra c\u00f3mo configurar un objeto <code>MediaRecorder</code> para capturar audio y v\u00eddeo del micr\u00f3fono y la c\u00e1mara usando un codec est\u00e1ndar y grabando el resultado en la tarjeta SD:</p> <pre><code>MediaRecorder mediaRecorder = new MediaRecorder();\n\nmCamera.unlock();\nmediaRecorder.setCamera(mCamera);\n\n// Configuramos las fuentes de entrada\nmediaRecorder.setAudioSource(MediaRecorder.AudioSource.MIC);\nmediaRecorder.setVideoSource(MediaRecorder.VideoSource.CAMERA);\n\n// Seleccionamos el formato de salida\nmediaRecorder.setOutputFormat(MediaRecorder.OutputFormat.DEFAULT);\n\n// Seleccionamos el codec de audio y v\u00eddeo\nmediaRecorder.setAudioEncoder(MediaRecorder.AudioEncoder.DEFAULT);\nmediaRecorder.setVideoEncoder(MediaRecorder.VideoEncoder.DEFAULT);\n\n// Especificamos el fichero de salida\nmediaRecorder.setOutputFile(\"/mnt/sdcard/mificherodesalida.mp4\");\n\n// Nos preparamos para grabar\nmediaRecorder.prepare();\n</code></pre> <p>Recuerda que los m\u00e9todos que hemos visto en el ejemplo anterior deben invocarse en ese orden concreto, ya que de lo contrario se lanzar\u00e1 una excepci\u00f3n de tipo Illegal State Exception.</p> <p>Para comenzar la grabaci\u00f3n, una vez inicializados todos los par\u00e1metros, utilizaremos el m\u00e9todo <code>start</code>:</p> <pre><code>mediaRecorder.start();\n</code></pre> <p>Cuando se desee finalizar la grabaci\u00f3n se deber\u00e1 hacer uso en primer lugar del m\u00e9todo <code>stop</code>, y a continuaci\u00f3n invocar el m\u00e9todo <code>reset</code>. Una vez seguidos estos pasos es posible volver a utilizar el objeto invocando de nuevo a <code>setAudioSource</code> y <code>setVideoSource</code>. Llama a <code>release</code> para liberar los recursos asociados al objeto <code>MediaRecorder</code> el objeto no podr\u00e1 volver a ser usado, se tendr\u00e1 que crear de nuevoel objeto no podr\u00e1 volver a ser usado, se tendr\u00e1 que crear de nuevo:</p> <pre><code>mediaRecorder.stop();\nmediaRecorder.reset();\nmediaRecorder.release();\n</code></pre>"},{"location":"captura-android.html#previsualizacion","title":"Previsualizaci\u00f3n","text":"<p>Durante la grabaci\u00f3n de v\u00eddeo es recomendable mostrar una previsualizaci\u00f3n de lo que se est\u00e1 captando a trav\u00e9s de la c\u00e1mara en tiempo real. Para ello utilizaremos el m\u00e9todo <code>setPreviewDisplay</code>, que nos permitir\u00e1 asignar un objeto <code>Surface</code> sobre el cual mostrar dicha previsualizaci\u00f3n.</p> <p>El comportamiento en este caso es muy parecido al de la clase <code>MediaPlayer</code> para la reproducci\u00f3n de v\u00eddeo. Debemos definir una actividad que incluya una vista de tipo <code>SurfaceView</code> en su interfaz y que implemente la interfaz <code>SurfaceHolder.Callback</code>. Una vez que el objeto <code>SurfaceHolder</code> ha sido creado podemos asignarlo al objeto <code>MediaRecorder</code> invocando al m\u00e9todo <code>setPreviewDisplay</code>, tal como se puede ver en el siguiente c\u00f3digo. El v\u00eddeo comenzar\u00e1 a previsualizarse tan pronto como se haga uso del m\u00e9todo <code>prepare</code>.</p> <pre><code>public class CapturaActivity extends Activity implements SurfaceHolder.Callback\n{\n\nprivate MediaRecorder mMediaRecorder;\n\n@Override\npublic void onCreate(Bundle savedInstanceState) {\nsuper.onCreate(savedInstanceState);\nsetContentView(R.layout.main);\n\nSurfaceView surface = (SurfaceView)findViewById(R.id.surface);\nSurfaceHolder holder = surface.getHolder();\nholder.addCallback(this);\nholder.setType(SurfaceHolder.SURFACE_TYPE_PUSH_BUFFERS);\nholder.setFixedSize(400, 300);\n}\n\npublic void surfaceCreated(SurfaceHolder holder) {\nif (mMediaRecorder != null) {\ntry {\nmMediaRecorder.setAudioSource(MediaRecorder.AudioSource.MIC);\nmMediaRecorder.setVideoSource(MediaRecorder.VideoSource.CAMERA);\n\nmMediaRecorder.setOutputFormat(MediaRecorder.OutputFormat.DEFAULT);\n\nmMediaRecorder.setAudioEncoder(MediaRecorder.AudioEncoder.DEFAULT);\nmMediaRecorder.setVideoEncoder(MediaRecorder.VideoEncoder.DEFAULT);\n\nmMediaRecorder.setOutputFile(\"/sdcard/myoutputfile.mp4\");\n\n// Asociando la previsualizaci\u00f3n a la superficie\nmMediaRecorder.setPreviewDisplay(holder.getSurface());\nmMediaRecorder.prepare();\n} catch (IllegalArgumentException e) {\nLog.d(\"MEDIA_PLAYER\", e.getMessage());\n} catch (IllegalStateException e) {\nLog.d(\"MEDIA_PLAYER\", e.getMessage());\n} catch (IOException e) {\nLog.d(\"MEDIA_PLAYER\", e.getMessage());\n}\n}\n}\n\npublic void surfaceDestroyed(SurfaceHolder holder) {\nmMediaRecorder.release();\n}\n\npublic void surfaceChanged(SurfaceHolder holder,\nint format, int width, int height) { }\n}\n</code></pre>"},{"location":"captura-android.html#agregar-ficheros-multimedia-en-el-media-store","title":"Agregar ficheros multimedia en el Media Store","text":"<p>El comportamiento por defecto en Android con respecto al acceso de contenido multimedia es que los ficheros multimedia generados u obtenidos por una aplicaci\u00f3n no podr\u00e1n ser accedidos por el resto. En el caso de que deseemos que un nuevo fichero multimedia s\u00ed pueda ser accedido desde el exterior de nuestra aplicaci\u00f3n deberemos almacenarlo en el Media Store, que mantiene una base de datos de la metainformaci\u00f3n de todos los ficheros almacenados tanto en dispositivos externos como internos del terminal telef\u00f3nico.</p> <p>El Media Store es un proveedor de contenidos, y por lo tanto utilizaremos el mecanismo est\u00e1ndar para acceso a dichos proveedores para acceder a la informaci\u00f3n que contiene.</p> <p>Existen varias formas de incluir un fichero multimedia en el Media Store. La m\u00e1s sencilla es hacer uso de la clase <code>MediaScannerConnection</code>, que permitir\u00e1 determinar autom\u00e1ticamente de qu\u00e9 tipo de fichero se trata, de tal forma que se pueda a\u00f1adir sin necesidad de proporcionar ninguna informaci\u00f3n adicional.</p> <p>La clase <code>MediaScannerConnection</code> proporciona un m\u00e9todo <code>scanFile</code> para realizar esta tarea. Sin embargo, antes de escanear un fichero se deber\u00e1 llamar al m\u00e9todo <code>connect</code> y esperar una conexi\u00f3n al Media Store. La llamada a <code>connect</code> es as\u00edncrona, lo cual quiere decir que deberemos crear un objeto <code>MediaScannerConnectionClient</code> que nos notifique en el momento en el que se complete la conexi\u00f3n. Esta misma clase tambi\u00e9n puede ser utilizada para que se lleve a cabo una notificaci\u00f3n en el momento en el que el escaneado se haya completado, de tal forma que ya podremos desconectarnos del Media Store.</p> <p>En el siguiente ejemplo de c\u00f3digo podemos ver un posible esqueleto para un objeto <code>MediaScannerConnectionClient</code>. En este c\u00f3digo se hace uso de una instancia de la clase <code>MediaScannerConnection</code> para manejar la conexi\u00f3n y escanear el fichero. El m\u00e9todo <code>onMediaScannerConected</code> ser\u00e1 llamado cuando la conexi\u00f3n ya se haya establecido, con lo que ya ser\u00e1 posible escanear el fichero. Una vez se complete el escaneado se llamar\u00e1 al m\u00e9todo <code>onScanCompleted</code>, en el que lo m\u00e1s aconsejable es llevar a cabo la desconexi\u00f3n del Media Store.</p> <pre><code>MediaScannerConnectionClient mediaScannerClient = new MediaScannerConnectionClient() {\nprivate MediaScannerConnection msc = null;\n{\nmsc = new MediaScannerConnection(getApplicationContext(), this);\nmsc.connect();\n}\n\npublic void onMediaScannerConnected() {\nmsc.scanFile(\"/mnt/sdcard/DCIM/prueba.mp4\", null);\n}\n\npublic void onScanCompleted(String path, Uri uri) {\n\n// Realizar otras acciones adicionales\n\nmsc.disconnect();\n}\n};\n</code></pre>"},{"location":"captura-android.html#ejercicios","title":"Ejercicios","text":""},{"location":"captura-android.html#grabacion-de-video-con-mediarecorder","title":"Grabaci\u00f3n de v\u00eddeo con MediaRecorder","text":"<p>En este ejercicio optativo utilizaremos la aplicaci\u00f3n Video que se te proporciona en las plantillas para crear una aplicaci\u00f3n que permita guardar v\u00eddeo, mostr\u00e1ndolo en pantalla mientras \u00e9ste se graba. La interfaz de la actividad principal tiene dos botones, Grabar y Parar, y una vista <code>SurfaceView</code> sobre la que se previsualizar\u00e1 el v\u00eddeo siendo grabado.</p> <p>Debes seguir los siguientes pasos:</p> <ul> <li>A\u00f1ade los permisos necesarios en el Manifest de la aplicaci\u00f3n para poder grabar audio y v\u00eddeo y para poder guardar el resultado en la tarjeta SD recuerda   que el siguiente c\u00f3digo debe aparecer antes del elemento <code>application</code>recuerda   que el siguiente c\u00f3digo debe aparecer antes del elemento &lt;code&gt;application&lt;/code&gt;:</li> </ul> <pre><code>&lt;uses-permission android:name=\"android.permission.CAMERA\"/&gt;\n&lt;uses-permission android:name=\"android.permission.RECORD_AUDIO\"/&gt;\n&lt;uses-permission android:name=\"android.permission.WRITE_EXTERNAL_STORAGE\"/&gt;\n</code></pre> <ul> <li>A\u00f1ade un atributo a la clase <code>VideoActivity:</code></li> </ul> <pre><code>MediaRecorder mediaRecorder;\n</code></pre> <ul> <li>Inicializa el objeto <code>MediaRecorder</code> en el m\u00e9todo <code>onCreate</code>:</li> </ul> <pre><code>mediaRecorder = new MediaRecorder();\n</code></pre> <ul> <li>Para poder previsualizar el v\u00eddeo en el <code>SurfaceView</code> hemos de obtener su holder. Como esta operaci\u00f3n es as\u00edncrona, debemos a\u00f1adir los manejadores adecuados, de tal forma que s\u00f3lo se pueda reproducir la previsualizaci\u00f3n cuando todo est\u00e9 listo. El primer paso consiste en hacer que la clase <code>VideoActivity</code> implemente la interfaz <code>SurfaceHolder.Callback</code>. Para implementar esta interfaz deber\u00e1s a\u00f1adir los siguiente m\u00e9todos a la clase:</li> </ul> <pre><code>public void surfaceCreated(SurfaceHolder holder) {\n// TODO: asociar la superficie al MediaRecorder\n}\n\npublic void surfaceDestroyed(SurfaceHolder holder) {\n// TODO: liberar los recursos\n}\n</code></pre> <ul> <li>A\u00f1adimos en <code>onCreate</code> el c\u00f3digo necesario para obtener el holder de la superficie y asociarle como manejador la propia clase <code>VideoActivity</code>:</li> </ul> <pre><code>m_holder = superficie.getHolder();\nm_holder.addCallback(this);\nm_holder.setType(SurfaceHolder.SURFACE_TYPE_PUSH_BUFFERS);\n</code></pre> <ul> <li>Gracias al m\u00e9todo <code>surfaceCreated</code> podremos asociar el objeto <code>MediaRecorder</code> al holder del <code>SurfaceView</code>. Dentro de esta misma funci\u00f3n le daremos al atributo booleano <code>preparado</code> el valor true, lo cual nos permitir\u00e1 saber que ya podemos iniciar la reproducci\u00f3n:</li> </ul> <pre><code>mediaRecorder.setPreviewDisplay(holder.getSurface());\npreparado = true;\n</code></pre> <ul> <li> <p>En el m\u00e9todo <code>surfaceDestroyed</code> simplemente invocaremos el m\u00e9todo <code>release</code> del objeto <code>MediaRecorder</code>, para liberar los recursos del objeto   al finalizar la actividad.</p> </li> <li> <p>Se ha a\u00f1adido un m\u00e9todo <code>configurar</code> a la clase <code>VideoActivity</code> que se utilizar\u00e1 para indicar la fuente de audio y v\u00eddeo, el nombre del fichero donde   guardaremos el v\u00eddeo grabado, y algunos par\u00e1metros m\u00e1s. En esa funci\u00f3n debes a\u00f1adir el siguiente c\u00f3digo. F\u00edjate c\u00f3mo se ha incluido una llamada a <code>prepare</code> al final:</p> </li> </ul> <pre><code>if (mediaRecorder != null) {\ntry {\n\n// Inicializando el objeto MediaRecorder\nmediaRecorder.setAudioSource(MediaRecorder.AudioSource.MIC);\nmediaRecorder.setVideoSource(MediaRecorder.VideoSource.CAMERA);\n\nmediaRecorder.setOutputFormat(MediaRecorder.OutputFormat.THREE_GPP);\n\nmediaRecorder.setAudioEncoder(MediaRecorder.AudioEncoder.DEFAULT);\nmediaRecorder.setVideoEncoder(MediaRecorder.VideoEncoder.DEFAULT);\n\nFile file = new File(Environment.getExternalStoragePublicDirectory(Environment.DIRECTORY_PICTURES),\"video.mp4\");\nmediaRecorder.setOutputFile(file.getPath());\n\nmediaRecorder.prepare();\n\n} catch (IllegalArgumentException e) {\nLog.d(\"MEDIA_PLAYER\", e.getMessage());\n} catch (IllegalStateException e) {\nLog.d(\"MEDIA_PLAYER\", e.getMessage());\n} catch (IOException e) {\nLog.d(\"MEDIA_PLAYER\", e.getMessage());\n}\n}\n</code></pre> <ul> <li> <p>S\u00f3lo queda introducir el c\u00f3digo necesario para iniciar y detener la reproducci\u00f3n. En el manejador del bot\u00f3n Grabar invocaremos al m\u00e9todo <code>start</code> del objeto <code>MediaRecorder</code>, sin olvidar realizar una llamada previa al m\u00e9todo <code>configurar</code>.</p> </li> <li> <p>En el manejador del bot\u00f3n Parar invocamos en primer lugar el m\u00e9todo <code>stop</code> y en segundo lugar el m\u00e9todo <code>reset</code> del objeto <code>MediaRecorder</code>. Con esto podr\u00edamos volver a utilizar este objeto llamando a <code>configurar</code> y a <code>start</code>.</p> </li> </ul> <p>A la hora de redactar estos ejercicios exist\u00eda un bug que imped\u00eda volver a utilizar un objeto <code>MediaRecorder</code> tras haber usado <code>reset</code>. Puede que sea necesario que tras hacer un <code>reset</code> debas invocar el m\u00e9todo <code>release</code> y crear una nueva instancia del objeto <code>MediaRecorder</code> con el operador <code>new</code>.</p>"},{"location":"captura-ios.html","title":"Captura y procesamiento de medios en iOS","text":"<p>Vamos a estudiar las formas en las que podemos capturar medios desde dispositivos iOS (fotograf\u00edas y audio/v\u00eddeo), y posteriormente procesarlos.</p>"},{"location":"captura-ios.html#fotografias-y-galeria-multimedia","title":"Fotograf\u00edas y galer\u00eda multimedia","text":"<p>La forma m\u00e1s sencilla de realizar una captura con la c\u00e1mara del dispositivo es tomar una fotograf\u00eda. Para ello contamos con un controlador predefinido que simplificar\u00e1 esta tarea, ya que s\u00f3lo deberemos ocuparnos de instanciarlo, mostrarlo y obtener el resultado:</p> <p>Swift <pre><code>var picker = UIImagePickerController()\npicker.sourceType = .camera\nself.present(picker, animated: true, completion: nil)\n</code></pre> Objective-C <pre><code>UIImagePickerController *picker = [[UIImagePickerController alloc] init];\npicker.sourceType = UIImagePickerControllerSourceTypeCamera;\n[self presentModalViewController:picker animated:YES];\n</code></pre></p> <p>Podemos observar que podemos cambiar la fuente de la que obtener la fotograf\u00eda. En el ejemplo anterior hemos especificado la c\u00e1mara del dispositivo, sin embargo, tambi\u00e9n podremos hacer que seleccione la imagen de la colecci\u00f3n de fotos del usuario (<code>UIImagePickerControllerSourceTypePhotoLibrary</code>), o del carrete de la c\u00e1mara (<code>UIImagePickerControllerSourceTypeSavedPhotosAlbum</code>):</p> <p>Swift <pre><code>picker.sourceType = .savedPhotosAlbum\n</code></pre> Objective-C <pre><code>picker.sourceType = UIImagePickerControllerSourceTypeSavedPhotosAlbum;\n</code></pre></p> <p>Si lo que queremos es almacenar una fotograf\u00eda en el carrete de fotos del dispositivo podemos utilizar la funci\u00f3n <code>UIImageWriteToSavedPhotosAlbum</code>:</p> <p>Swift <pre><code>var image : UIImage = ...;\nUIImageWriteToSavedPhotosAlbum(image, self, selector(self.guardada), nil)\n</code></pre> Objective-C <pre><code>UIImage *image = ...;\n\nUIImageWriteToSavedPhotosAlbum(image, self, @selector(guardada:), nil);\n</code></pre></p> <p>Como primer par\u00e1metro debemos proporcionar la imagen a guardar. Despu\u00e9s podemos proporcionar de forma opcional un callback mediante target y selector para que se nos notifique cuando la imagen haya sido guardada. Por \u00faltimo, podemos especificar tambi\u00e9n de forma opcional cualquier informaci\u00f3n sobre el contexto que queramos que se le pase al callback anterior, en caso de haberlo especificado.</p> <p>Por ejemplo, si queremos tomar una fotograf\u00eda y guardarla en el carrete del dispositivo, podemos crear un delegado de <code>UIImagePickerController</code>, de forma que cuando la fotograf\u00eda haya sido tomada llame a la funci\u00f3n anterior para almacenarla. Para ello debemos crear un objeto que adopte el delegado <code>UIImagePickerControllerDelegate</code> y establecer dicho objeto en el propiedad <code>delegate</code> del controlador. Deberemos definir el siguiente m\u00e9todo del delegado:</p> <p>Swift <pre><code>func imagePickerController(picker: UIImagePickerController,   didFinishPickingMediaWithInfo info: [String : AnyObject]) {\n let pickedImage = info[UIImagePickerControllerOriginalImage]\n UIImageWriteToSavedPhotosAlbum(pickedImage as! UIImage, self, #selector(self.guardada), nil)\n}\n</code></pre> Objective-C <pre><code>- (void)imagePickerController:(UIImagePickerController *)picker\ndidFinishPickingMediaWithInfo:(NSDictionary *)info\n{\nUIImage *imagen =\n[info valueForKey: UIImagePickerControllerOriginalImage];\nUIImageWriteToSavedPhotosAlbum(imagen, self, @selector(guardada:),\nnil);\n}\n</code></pre></p> <p>Este controlador nos permitir\u00e1 capturar tanto im\u00e1genes como v\u00eddeo. Por defecto el controlador se mostrar\u00e1 con la interfaz de captura de c\u00e1mara nativa del dispositivo. Sin embargo, podemos personalizar esta interfaz con los m\u00e9todos <code>showsCameraControls</code>, <code>cameraOverlayView</code>, y <code>cameraViewTransform</code>. Si estamos utilizando una vista personalizada, podremos controlar la toma de fotograf\u00edas y la captura de v\u00eddeo con los m\u00e9todos <code>takePicture</code>, <code>startVideoCapture</code> y <code>stopVideoCapture</code>.</p> <p>Si queremos tener un mayor control sobre la forma en la que se almacenan los diferentes tipos de recursos multimedia en el dispositivo deberemos utilizar el framework Assets. Con esta librer\u00eda podemos por ejemplo guardar metadatos con las fotograf\u00edas, como puede ser la localizaci\u00f3n donde fue tomada.</p>"},{"location":"captura-ios.html#captura-avanzada-de-video","title":"Captura avanzada de v\u00eddeo","text":"<p>A partir de iOS 4.0 en el framework AVFoundation se incorpora la posibilidad de acceder a la fuente de captura de v\u00eddeo a bajo nivel. Para ello tenemos un objeto <code>AVCaptureSession</code> que representa la sesi\u00f3n de captura, y se encarga de coordinar la entrada y la salida de audio y v\u00eddeo, y los objetos <code>AVCaptureInput</code> y  <code>AVCaptureOutput</code> que nos permiten establecer la fuente y el destino de estos medios. De esta forma podemos hacer por ejemplo que la fuente de v\u00eddeo sea un dispositivo de captura (por ejemplo la c\u00e1mara), y que la salida se nos proporcione como datos crudos de cada fotograma obtenido, para as\u00ed poder procesarlo y mostrarlo nosotros como creamos conveniente.</p>"},{"location":"captura-ios.html#entrada-de-la-sesion","title":"Entrada de la sesi\u00f3n","text":"<p>Especificaremos la entrada mediante un objeto de tipo <code>AVCaptureInput</code> (o subclases suyas). Si queremos que la fuente de v\u00eddeo se obtenga de un dispositivo de captura, utilizaremos como entrada un objeto de la subclase <code>AVCaptureDeviceInput</code>, que inicializaremos proporcionando un objeto <code>AVCaptureDevice</code> que definir\u00e1 el dispositivo del cual queremos capturar:</p> <p>Swift <pre><code>var captureDevice = AVCaptureDevice.defaultDevice(withMediaType: AVMediaTypeVideo)!\nvar captureInput = try! AVCaptureDeviceInput(device: captureDevice)\n</code></pre> Objective-C <pre><code>AVCaptureDevice *captureDevice = [AVCaptureDevice defaultDeviceWithMediaType:AVMediaTypeVideo];\nAVCaptureDeviceInput *captureInput = [AVCaptureDeviceInput deviceInputWithDevice:captureDevice error: nil];\n</code></pre></p> <p>En este ejemplo estamos obteniendo el dispositivo de captura por defecto que nos proporcione v\u00eddeo (la c\u00e1mara), pero podr\u00edamos solicitar otros tipos de dispositivos de entrada.</p> <p>Podr\u00edamos tambi\u00e9n recorrer la lista de todos los dispositivos disponibles y comprobar sus caracter\u00edsticas (por ejemplo si tiene flash o autofocus, o si es la c\u00e1mara frontal o trasera). </p>"},{"location":"captura-ios.html#salida-de-la-sesion","title":"Salida de la sesi\u00f3n","text":"<p>La salida se especificar\u00e1 mediante subclases de <code>AVCaptureOutput</code>. Seg\u00fan el destino de la captura tenemos: * <code>AVCaptureMovieFileOutput</code>: Nos permite grabar el v\u00eddeo capturado en un fichero. * <code>AVCaptureVideoDataOutput</code>: Nos permite procesar los fotogramas de v\u00eddeo capturados en tiempo real (nos da acceso al framebuffer). * <code>AVCaptureAudioDataOutput</code>: Nos permite procesar audio capturado en tiempo real. * <code>AVCaptureStillImageOutput</code>: Nos permite tomar fotograf\u00edas a partir de la fuente de entrada.</p>"},{"location":"captura-ios.html#captura-de-fotogramas","title":"Captura de fotogramas","text":"<p>Por ejemplo, para establecer la salida de tipo <code>AVCaptureStillImageOutput</code> podemos hacer lo siguiente:</p> <p>Swift <pre><code>var captureOutput = AVCaptureStillImageOutput()\n</code></pre> Objective-C <pre><code>captureOutput = [[AVCaptureStillImageOutput alloc] init];\n</code></pre></p> <p>Con este tipo de salida de captura, en todo momento podremos tomar un fotograma a partir de la entrada con:</p> <p>Objective-C <pre><code>AVCaptureConnection *connection = [[self.captureOutput connections] objectAtIndex:0];\n\n[self.captureOutput captureStillImageAsynchronouslyFromConnection:connection completionHandler:^(CMSampleBufferRef imageDataSampleBuffer, NSError *error) {\n\nNSData *data = [AVCaptureStillImageOutput jpegStillImageNSDataRepresentation:imageDataSampleBuffer];\nUIImage *image = [UIImage imageWithData:data];\n[self.ivPreview performSelectorOnMainThread: @selector(setImage:)\nwithObject: image\nwaitUntilDone: YES];\n\n}];\n</code></pre></p> <p>La clase <code>AVCaptureStillImageOutput</code> ha sido desaprobada en iOS 10.0 y por lo tanto no soporta nuevas caracter\u00edsticas, como la obtenci\u00f3n de datos en crudo o im\u00e1genes en vivo. En iOS 10.0 y posteriores se recomienda utilizar <code>AVCapturePhotoOutput</code> en su lugar.</p>"},{"location":"captura-ios.html#procesamiento-en-tiempo-real","title":"Procesamiento en tiempo real","text":"<p>Para capturar en memoria y poder procesar fotogramas podemos utiliza el tipo de salida <code>AVCaptureVideoDataOutput</code>:</p> <p>Swift <pre><code>var captureOutput = AVCaptureVideoDataOutput()\ncaptureOutput.alwaysDiscardsLateVideoFrames = true\n\nvar queue = DispatchQueue(label: \"cameraQueue\")\ncaptureOutput.setSampleBufferDelegate(self, queue: queue)\n\ncaptureOutput.videoSettings = [kCVPixelBufferPixelFormatTypeKey as AnyHashable : Int(kCVPixelFormatType_32BGRA)]\n</code></pre></p> <p>Objective-C <pre><code>AVCaptureVideoDataOutput *captureOutput = [[AVCaptureVideoDataOutput alloc] init];\ncaptureOutput.alwaysDiscardsLateVideoFrames = YES;\n//captureOutput.minFrameDuration = CMTimeMakeWithSeconds(1, 1);\n\ndispatch_queue_t queue = dispatch_queue_create(\"cameraQueue\", NULL);\n[captureOutput setSampleBufferDelegate: self queue: queue];\n\nNSDictionary *videoSettings = @{ (NSString*)kCVPixelBufferPixelFormatTypeKey : @(kCVPixelFormatType_32BGRA) };    [captureOutput setVideoSettings: videoSettings];\n</code></pre></p> <p>En este caso tenemos que proporcionar un delegado de tipo <code>AVCaptureVideoDataOutputSampleBufferDelegate</code>, que tendr\u00e1 que definir un m\u00e9todo como el siguiente que ser\u00e1 invocado cada vez que se capture un fotograma:</p> <p>Swift <pre><code>func captureOutput(_ captureOutput: AVCaptureOutput, didOutputSampleBuffer sampleBuffer: CMSampleBuffer, from connection: AVCaptureConnection) { // Procesar datos de sampleBuffer}\n</code></pre></p> <p>Objective-C <pre><code>- (void)captureOutput:(AVCaptureOutput *)captureOutput\ndidOutputSampleBuffer:(CMSampleBufferRef)sampleBuffer\nfromConnection:(AVCaptureConnection *)connection {\n\n// Procesar datos de sampleBuffer\n\n}\n</code></pre></p> <p>Podemos utilizar este m\u00e9todo para procesar el v\u00eddeo.</p>"},{"location":"captura-ios.html#sesion-de-captura","title":"Sesi\u00f3n de captura","text":"<p>La sessi\u00f3n de captura coordina la entrada y la salida. Podemos establecer diferentes presets para la sesi\u00f3n, seg\u00fan la calidad con la que queramos capturar el medio. En el siguiente ejemplo utilizamos un preset para capturar v\u00eddeo en 720p, y tras ello a\u00f1adimos la entrada y la salida de la sesi\u00f3n:</p> <p>Swift <pre><code> var captureSession = AVCaptureSession()\n\n // Establece preset\n if captureSession.canSetSessionPreset(AVCaptureSessionPreset1280x720) {\n   captureSession.sessionPreset = AVCaptureSessionPreset1280x720\n }\n else {\n   print(\"Preset no compatible\")\n }\n\n // Establece entrada\n if captureSession.canAddInput(captureInput) {\n   captureSession.addInput(captureInput)\n }\n else {\n   print(\"No se puede a\u00f1adir entrada\")\n }\n\n // Establece salida\n if captureSession.canAddOutput(captureOutput) {\n   captureSession.addOutput(captureOutput)\n }\n else {\n   print(\"No se puede a\u00f1adir salida\")\n }\n</code></pre></p> <p>Objective-C <pre><code>AVCaptureSession *captureSession = [[AVCaptureSession alloc] init];\n\n// Establece preset\nif([captureSession canSetSessionPreset:AVCaptureSessionPreset1280x720]) {\ncaptureSession.sessionPreset = AVCaptureSessionPreset1280x720;\n} else {\nNSLog(@\"Preset no compatible\");\n}\n\n// Establece entrada\nif([captureSession canAddInput:captureInput]) {\n[captureSession addInput: captureInput];\n} else {\nNSLog(@\"No se puede a\u00f1adir entrada\");\n}\n\n// Establece salida\nif([captureSession canAddOutput:captureOutput]) {\n[captureSession addOutput: captureOutput];\n} else {\nNSLog(@\"No se puede a\u00f1adir salida\");\n}\n</code></pre></p> <p>Despu\u00e9s de configurar la sesi\u00f3n, deberemos iniciar la captura con <code>startRunning</code>:</p> <p>Swift <pre><code>captureSession.startRunning()\n</code></pre></p> <p>Objective-C <pre><code>[captureSession startRunning];\n</code></pre></p>"},{"location":"captura-ios.html#ejemplo-de-captura-y-procesamiento-de-fotogramas","title":"Ejemplo de captura y procesamiento de fotogramas","text":"<p>En el siguiente ejemplo creamos una sesi\u00f3n de captura que tiene como entrada el dispositivo de captura de v\u00eddeo, y como salida los fotogramas del v\u00eddeo sin compresi\u00f3n como datos crudos. Tras configurar la entrada, la salida, y la sesi\u00f3n de captura, ponemos dicha sesi\u00f3n en funcionamiento con <code>startRunning</code>:</p> <p>Swift <pre><code> // Entrada del dispositivo de captura de video \n var captureDevice = AVCaptureDevice.defaultDevice(withMediaType: AVMediaTypeVideo)!\n var captureInput = try! AVCaptureDeviceInput(device: captureDevice)\n\n // Salida como fotogramas \"crudos\" (sin comprimir)\n var captureOutput = AVCaptureVideoDataOutput()\n captureOutput.alwaysDiscardsLateVideoFrames = true\n\n var queue = DispatchQueue(label: \"cameraQueue\")\n captureOutput.setSampleBufferDelegate(self, queue: queue)\n captureOutput.videoSettings = [kCVPixelBufferPixelFormatTypeKey as AnyHashable : Int(kCVPixelFormatType_32BGRA)]\n\n // Creaci\u00f3n de la sesi\u00f3n de captura\n self.captureSession = AVCaptureSession()\n self.captureSession?.sessionPreset = AVCaptureSessionPreset1280x720\n self.captureSession?.addInput(captureInput)\n self.captureSession?.addOutput(captureOutput)\n self.captureSession?.startRunning()\n</code></pre></p> <p>Objective-C <pre><code>   // Entrada del dispositivo de captura de video\nAVCaptureDevice *captureDevice = [AVCaptureDevice defaultDeviceWithMediaType:AVMediaTypeVideo];\nAVCaptureDeviceInput *captureInput = [AVCaptureDeviceInput deviceInputWithDevice:captureDevice error: nil];\n\n// Salida como fotogramas \"crudos\" (sin comprimir)\nAVCaptureVideoDataOutput *captureOutput = [[AVCaptureVideoDataOutput alloc] init];\ncaptureOutput.alwaysDiscardsLateVideoFrames = YES;\n\ndispatch_queue_t queue = dispatch_queue_create(\"cameraQueue\", NULL);\n[captureOutput setSampleBufferDelegate: self queue: queue];\n\nNSDictionary *videoSettings = @{ (NSString*)kCVPixelBufferPixelFormatTypeKey : @(kCVPixelFormatType_32BGRA) };    [captureOutput setVideoSettings: videoSettings];\n\n// Creaci\u00f3n de la sesi\u00f3n de captura\nself.captureSession = [[AVCaptureSession alloc] init];\nself.captureSession.sessionPreset = AVCaptureSessionPreset1280x720;\n[self.captureSession addInput: captureInput];\n[self.captureSession addOutput: captureOutput];\n\n[self.captureSession startRunning];\n}\n</code></pre></p> <p>Una vez haya comenzado la sesi\u00f3n de captura, se comenzar\u00e1n a producir fotogramas del v\u00eddeo capturado. Para consumir estos fotogramas deberemos implementar el m\u00e9todo delegado <code>captureOutput:didOutputSampleBuffer:fromConnection:</code></p> <p>Swift <pre><code>func captureOutput(_ captureOutput: AVCaptureOutput!, didOutputSampleBuffer sampleBuffer: CMSampleBuffer!, from connection: AVCaptureConnection!) {\n   var image = self.imageFromSampleBuffer(sampleBuffer)\n   self.ivPreview.performSelector(onMainThread:   #selector(self.setImage), withObject: image, waitUntilDone: true)\n}\n</code></pre></p> <p>Objective-C <pre><code>- (void)captureOutput:(AVCaptureOutput *)captureOutput\ndidOutputSampleBuffer:(CMSampleBufferRef)sampleBuffer\nfromConnection:(AVCaptureConnection *)connection {\n\nUIImage *image = [self imageFromSampleBuffer:sampleBuffer];\n\n[self.ivPreview performSelectorOnMainThread: @selector(setImage:)\nwithObject: image\nwaitUntilDone: YES];\n}\n</code></pre></p> <p>Vemos que el primer paso consiste en transformar el buffer del fotograma actual en un objeto <code>UIImage</code> que podamos mostrar. Para ello podemos definir un m\u00e9todo como el siguiente:</p> <p>Swift <pre><code>func imageFromSampleBuffer(sampleBuffer: CMSampleBuffer) -&gt; UIImage {\n\n // Get a CMSampleBuffer's Core Video image buffer for the media data\n var imageBuffer = CMSampleBufferGetImageBuffer(sampleBuffer)\n\n // Lock the base address of the pixel buffer\n CVPixelBufferLockBaseAddress(imageBuffer!, CVPixelBufferLockFlags(rawValue: 0))\n\n // Get the number of bytes per row for the pixel buffer\n var baseAddress = CVPixelBufferGetBaseAddress(imageBuffer!)\n\n // Get the number of bytes per row for the pixel buffer\n var bytesPerRow = CVPixelBufferGetBytesPerRow(imageBuffer!)\n\n // Get the pixel buffer width and height\n var width = CVPixelBufferGetWidth(imageBuffer!)\n var height = CVPixelBufferGetHeight(imageBuffer!)\n\n // Create a device-dependent RGB color space\n var colorSpace = CGColorSpaceCreateDeviceRGB()\n\n // Create a bitmap graphics context with the sample buffer data\n let bitmapInfo = CGBitmapInfo(rawValue: CGImageAlphaInfo.noneSkipFirst.rawValue | CGBitmapInfo.byteOrder32Little.rawValue)\n\nvar output = self.procesaImagen(UInt8(baseAddress))\n\n var context = CGContext(data: output, width: width, height: height, bitsPerComponent: 8, bytesPerRow: bytesPerRow, space: colorSpace, bitmapInfo: bitmapInfo.rawValue)\n\n // Create a Quartz image from the pixel data in the bitmap graphics context\n var quartzImage = context!.makeImage();\n\n // Unlock the pixel buffer\n CVPixelBufferUnlockBaseAddress(imageBuffer!,CVPixelBufferLockFlags(rawValue: 0));\n\n // Create an image object from the Quartz image\n let image = UIImage(cgImage: quartzImage!)\n     return image\n }\n</code></pre></p> <p>Objective-C <pre><code>- (UIImage *) imageFromSampleBuffer:(CMSampleBufferRef) sampleBuffer\n{\n// Obtiene y bloquea el framebuffer del actual fotograma\nCVImageBufferRef imageBuffer = CMSampleBufferGetImageBuffer(sampleBuffer);\nCVPixelBufferLockBaseAddress(imageBuffer, 0);\n\n// Obtiene el puntero al inicio del framebuffer\nvoid *baseAddress = CVPixelBufferGetBaseAddress(imageBuffer);\n\n// Obtiene las dimensiones de la imagen y bytes por fila del framebuffer\nsize_t bytesPerRow = CVPixelBufferGetBytesPerRow(imageBuffer);\nsize_t width = CVPixelBufferGetWidth(imageBuffer);\nsize_t height = CVPixelBufferGetHeight(imageBuffer);\n\n// Procesa la imagen\nvoid *output = [self procesaImagen:(uint8_t *)baseAddress];\n\n// Crea espacio de color RGB dependiente del dispositivo\nCGColorSpaceRef colorSpace = CGColorSpaceCreateDeviceRGB();\n\n// Crea un contexto gr\u00e1fico para dibujar en un bitmap y vuelca en \u00e9l el contenido del fotograma\nCGContextRef context = CGBitmapContextCreate(output, width, height, 8,\nbytesPerRow, colorSpace, kCGBitmapByteOrder32Little | kCGImageAlphaPremultipliedFirst);\n// Crea una imagen a partir del contenido del contexto gr\u00e1fico\nCGImageRef quartzImage = CGBitmapContextCreateImage(context);\n// Desbloquea el framebuffer\nCVPixelBufferUnlockBaseAddress(imageBuffer,0);\n\n// Libera el contexto gr\u00e1fico\nCGContextRelease(context);\nCGColorSpaceRelease(colorSpace);\n\n// Crea una imagen UIImage a partir de la CGImage\nUIImage *image = [UIImage imageWithCGImage:quartzImage];\n\n// Libera la CGImage\nCGImageRelease(quartzImage);\n\nreturn (image);\n}\n</code></pre></p> <p>En el m\u00e9todo anterior observamos que podemos procesar y modificar el buffer del fotograma antes de obtener una <code>UIImage</code> a partir de \u00e9l. Por ejemplo, podr\u00edamos convertir la imagen a escala de grises con:</p> <pre><code>- (uint8_t *)procesaImagen:(uint8_t *)input {\nstatic long size = WIDTH * HEIGHT;\nstatic uint8_t result[WIDTH * HEIGHT*4];\n\nuint8_t *output = result;\n\nfor(int i=0;i&lt;size;i++) {\nuint8_t b = *input++;\nuint8_t g = *input++;\nuint8_t r = *input++;\nuint8_t a = *input++;\n\nuint8_t gray = (r+g+b)/3;\n\n*output++ = gray;\n*output++ = gray;\n*output++ = gray;\n*output++ = a;\n}\n\nreturn result;\n}\n</code></pre>"},{"location":"captura-ios.html#procesamiento-de-imagenes-en-ios","title":"Procesamiento de im\u00e1genes en iOS","text":"<p>El procesamiento de im\u00e1genes es una operaci\u00f3n altamente costosa, por lo que supone un aut\u00e9ntico reto llevarla a un dispositivo m\u00f3vil de forma eficiente, especialmente si queremos ser capaces de procesar v\u00eddeo en tiempo real. Una de las aplicaciones del procesamiento de im\u00e1genes es el tratamiento de fotograf\u00edas mediante una serie de filtros. Tambi\u00e9n podemos encontrar numerosas aplicaciones relacionadas con el campo de la visi\u00f3n por computador, como la detecci\u00f3n de movimiento, el seguimiento de objetos, o el reconocimiento de caras.</p> <p>Estas operaciones suponen una gran carga de procesamiento, por lo que si queremos realizarlas de forma eficiente deberemos realizar un fuerte trabajo de optimizaci\u00f3n. Implementar directamente los algoritmos de procesamiento de im\u00e1genes sobre la CPU supone una excesiva carga para la aplicaci\u00f3n y resulta poco eficiente. Sin embargo, podemos llevar este procesamiento a unidades m\u00e1s adecuadas para esta tarea, y as\u00ed descargar la carga de trabajo de la CPU. Encontramos dos opciones:</p> <ul> <li>Utilizar la unidad NEON de los procesadores con juego de instrucciones ARMv7. Se trata de una unidad SIMD (Single Instruction Multiple Data), con la cual podemos vectorizar las operaciones de procesamiento de imagen y ejecutarlas de una forma mucho m\u00e1s eficiente, ya que en cada operaci\u00f3n del procesador en lugar de operar sobre un \u00fanico dato, lo haremos sobre un vector de ellos. El mayor inconveniente de esta opci\u00f3n es el trabajo que llevar\u00e1 vectorizar los algoritmos de procesamiento a aplicar. Como ventaja tenemos que el juego de instrucciones que podemos utilizar funcionar\u00e1 en cualquier dispositivo ARMv7, y la pr\u00e1ctica totalidad de dispositivos que hay actualmente en el mercado disponen de este juego de instrucciones. De esta forma, el c\u00f3digo que escribamos ser\u00e1 compatible con cualquier dispositivo, independientemente del sistema operativo que incorporen.</li> </ul> <p>http://www.arm.com/products/processors/technologies/neon.php</p> <ul> <li>Utilizar la GPU (Graphics Processing Unit). Podemos programar shaders, es decir, programas que se ejecutan sobre la unidad de procesamiento gr\u00e1fica, que esta especializada en operaciones de manipulaci\u00f3n de gr\u00e1ficos con altos niveles de paralelismo. El lenguaje en el que se programan los shaders dentro de OpenGL es GLSL. Con esta tecnolog\u00eda podemos desarrollar filtros que se ejecuten de forma optimizada por la GPU descargando as\u00ed totalmente a la CPU del procesamiento. Para utilizar esta opci\u00f3n deberemos estar familiarizados con los gr\u00e1ficos por computador y con el lenguaje GLSL.</li> </ul> <p>Con cualquiera de las opciones anteriores tendremos que invertir un gran esfuerzo en la implementaci\u00f3n \u00f3ptima de las funciones de procesado. Sin embargo, a partir de iOS 5 se incorpora un nuevo framework conocido como Core Image que nos permite realizar este procesamiento de forma \u00f3ptima sin tener que entrar a programar a bajo nivel. Este framework ya exist\u00eda anteriormente en MacOS, pero con la versi\u00f3n 5 de iOS ha sido trasladado a la plataforma m\u00f3vil. Por el momento, la versi\u00f3n de iOS de Core Image es una versi\u00f3n reducida, en la que encontramos una menor cantidad de filtros disponibles y adem\u00e1s, al contrario de lo que ocurre en MacOS, no podemos crear de momento nuestros propios filtros. Aun as\u00ed, contamos con un buen n\u00famero de filtros (alrededor de 50) que podemos configurar y combinar para as\u00ed aplicar distintos efectos a las im\u00e1genes, y que nos permiten realizar tareas complejas de visi\u00f3n artificial como el reconocimiento de caras. Vamos a continuaci\u00f3n a ver c\u00f3mo trabajar con esta librer\u00eda.</p>"},{"location":"captura-ios.html#representacion-de-imagenes-en-core-image","title":"Representaci\u00f3n de im\u00e1genes en Core Image","text":"<p>En el framework Core Image las im\u00e1genes se representan mediante la clase <code>CIImage</code>. Este tipo de im\u00e1genes difiere de las representaciones que hemos visto anteriormente (<code>UIImage</code> y <code>CGImageRef</code>) en que <code>CIImage</code> no contiene una representaci\u00f3n final de la imagen, sino que lo que contiene es una imagen inicial y una serie de filtros que se deben aplicar para obtener la imagen final a representar. La imagen final se calcular\u00e1 en el momento en el que la imagen <code>CIImage</code> final sea renderizada.</p> <p>Podemos crear una imagen de este tipo a partir de im\u00e1genes de Core Graphics:</p> <p>Swift <pre><code>var cgImage = UIImage(named: \"imagen.png\")!.cgImage!\nvar ciImage = CIImage(cgImage: cgImage)\n</code></pre></p> <p>Objective-C <pre><code>CGImageRef cgImage = [UIImage imageNamed: @\"imagen.png\"].CGImage;\nCIImage *ciImage = [CIImage imageWithCGImage: cgImage];\n</code></pre></p> <p>Tambi\u00e9n podemos encontrar inicializadores de <code>CIImage</code> que crean la imagen a partir de los contenidos de una URL o directamente a partir de los datos crudos (<code>NSData</code>) correspondientes a los distintos formatos de imagen soportados (JPEG, GIF, PNG, etc).</p> <p>Podemos tambi\u00e9n hacer la transformaci\u00f3n inversa, y crear un objeto <code>UIImage</code> a partir de una imagen de tipo <code>CIImage</code>. Esto lo haremos con el inicializador <code>initWithCIImage:</code>, y podremos obtener la representaci\u00f3n de la imagen como <code>CIImage</code> mediante la propiedad <code>CIImage</code> de <code>UIImage</code>. Dicha imagen podr\u00e1 ser dibujada en el contexto gr\u00e1fico como se ha visto en sesiones anteriores:</p> <p>Swift <pre><code>var uiImage = UIImage(ciImage: ciImage)\nvar ciImage = uiImage.ciImage!\n...\nuiImage.draw(at: CGPoint.zero)\n</code></pre></p> <p>Objective-C <pre><code>UIImage *uiImage = [UIImage imageWithCIImage: ciImage];\n...\n\nCIImage *ciImage = uiImage.CIImage;\n...\n\n[uiImage drawAtPoint: CGPointZero];\n</code></pre></p> <p>Cuando queramos crear una imagen para mostrar en la interfaz (<code>UIImage</code>) a partir de una imagen de Core Image (<code>CIImage</code>), deberemos llevar cuidado porque la imagen puede no mostrarse correctamente en determinados \u00e1mbitos. Por ejemplo, no se ver\u00e1 correctamente si la mostramos en un <code>UIImageView</code>, pero si que funcionar\u00e1 si la dibujamos directamente en el contexto gr\u00e1fico con sus m\u00e9todos <code>drawAtPoint:</code> o <code>drawInRect:</code>. La raz\u00f3n de este comportamiento se debe a que la representaci\u00f3n interna de la imagen variar\u00e1 seg\u00fan la forma en la que se cree. Si una imagen <code>UIImage</code> se crea a partir de una imagen de tipo <code>CGImageRef</code>, su propiedad <code>CGImage</code> apuntar\u00e1 a la imagen a partir de la cual se cre\u00f3, pero su propiedad <code>CIImage</code> ser\u00e1 <code>nil</code>. Sin embargo, si creamos una imagen a partir de una <code>CIImage</code> ocurrir\u00e1 al contrario, su propiedad <code>CGImage</code> ser\u00e1 <code>NULL</code> mientras que su propiedad <code>CIImage</code> apuntar\u00e1 a la imagen inicial. Esto causar\u00e1 que aquellos componentes cuyo funcionamiento se base en utilizar la propiedad <code>CGImage</code> dejen de funcionar.</p> <p>La clase <code>CIImage</code> tiene adem\u00e1s una propiedad <code>extent</code> que nos proporciona las dimensiones de la imagen como un dato de tipo <code>CGRect</code>. M\u00e1s adelante veremos que resulta de utilidad para renderizar la imagen.</p>"},{"location":"captura-ios.html#filtros-de-core-image","title":"Filtros de Core Image","text":"<p>Los filtros que podemos aplicar sobre la imagen se representan con la clase <code>CIFilter</code>. Podemos crear diferentes filtros a partir de su nombre:</p> <p>Swift <pre><code>var filter = CIFilter(name: \"CISepiaTone\")\n</code></pre></p> <p>Objective-C <pre><code>CIFilter *filter = [CIFilter filterWithName: @\"CISepiaTone\"];\n</code></pre></p> <p>Otros filtros que podemos encontrar son:</p> <ul> <li><code>CIAffineTransform</code></li> <li><code>CIColorControls</code></li> <li><code>CIColorMatrix</code></li> <li><code>CIConstantColorGenerator</code></li> <li><code>CICrop</code></li> <li><code>CIExposureAdjust</code></li> <li><code>CIGammaAdjust</code></li> <li><code>CIHighlightShadowAdjust</code></li> <li><code>CIHueAdjust</code></li> <li><code>CISourceOverCompositing</code></li> <li><code>CIStraightenFilter</code></li> <li><code>CITemperatureAndTint</code></li> <li><code>CIToneCurve</code></li> <li><code>CIVibrance</code></li> <li><code>CIWhitePointAdjust</code></li> </ul> <p>Todos los filtros pueden recibir una serie de par\u00e1metros de entrada, que variar\u00e1n seg\u00fan el filtro. Un par\u00e1metro com\u00fan que podemos encontrar en casi todos ellos es la imagen de entrada a la que se aplicar\u00e1 el filtro. Adem\u00e1s, podremos tener otros par\u00e1metros que nos permitan ajustar el funcionamiento del filtro. Por ejemplo, en el caso del filtro para convertir la imagen a tono sepia tendremos un par\u00e1metro que nos permitir\u00e1 controlar la intensidad de la imagen sepia:</p> <p>Swift <pre><code>let filter = CIFilter(name: \"CISepiaTone\")\nfilter?.setValue(ciImage, forKey: kCIInputImageKey)\nfilter?.setValue(0.5, forKey: kCIInputIntensityKey)\n</code></pre> Objective-C <pre><code>CIFilter *filter =\n[CIFilter filterWithName:@\"CISepiaTone\"\nkeysAndValues:\nkCIInputImageKey, ciImage,\n@\"inputIntensity\", [NSNumber numberWithFloat:0.8],\nnil];\n</code></pre></p> <p>Podemos ver que para la propiedad correspondiente a la imagen de entrada tenemos la constante <code>kCIInputImageKey</code>, aunque tambi\u00e9n podr\u00edamos especificarla como la cadena <code>@\"inputImage\"</code>. Las propiedades de los filtros tambi\u00e9n pueden establecerse independientemente utilizando KVC:</p> <p>Swift <pre><code>filter?.setValue(ciImage, forKey: kCIInputImageKey)\nfilter?.setValue(0.8, forKey: kCIInputIntensityKey)\n</code></pre></p> <p>Objective-C <pre><code>[filter setValue: ciImage forKey: @\"inputImage\"];\n[filter setValue: [NSNumber numberWithFloat:0.8]\nforKey: @\"inputIntensity\"];\n</code></pre></p> <p>En la documentaci\u00f3n de Apple no aparece la lista de filtros disponibles para iOS (si que tenemos la lista completa para MacOS, pero varios de esos filtros no est\u00e1n disponibles en iOS). Podemos obtener la lista de los filtros disponibles en nuestra plataforma desde la aplicaci\u00f3n con los m\u00e9todos <code>filterNamesInCategories:</code> y <code>filterNamesInCategory:</code>. Por ejemplo, podemos obtener la lista de todos los filtros con:</p> <p>Swift <pre><code>var filters = CIFilter.filterNames(inCategories: nil)\n</code></pre></p> <p>Objective-C <pre><code>NSArray *filters = [CIFilter filterNamesInCategories: nil];\n</code></pre></p> <p>Cada objeto de la lista ser\u00e1 de tipo <code>CIFilter</code>, y podremos obtener de \u00e9l sus atributos y las caracter\u00edsticas de cada uno de ellos mediante la propiedad <code>attributes</code>. Esta propiedad nos devolver\u00e1 un diccionario con todos los par\u00e1metros de entrada y salida del filtro, y las caracter\u00edsticas de cada uno de ellos. Por ejemplo, de cada par\u00e1metro nos dir\u00e1 el tipo de dato que se debe indicar, y sus limitaciones (por ejemplo, si es num\u00e9rico sus valores m\u00ednimo y m\u00e1ximo). Como alternativa, tambi\u00e9n podemos obtener el nombre del filtro con su propiedad <code>name</code> y las listas de sus par\u00e1metros de entrada y salida con <code>inputKeys</code> y <code>outputKeys</code> respectivamente.</p> <p>La propiedad m\u00e1s importante de los filtros es <code>outputImage</code>. Esta propiedad nos da la imagen producida por el filtro en forma de objeto <code>CIImage</code>:</p> <p>Swift <pre><code>let outputImage = UIImage(ciImage: (filter?.outputImage)!)\n</code></pre></p> <p>Objective-C <pre><code>CIImage *filteredImage = filter.outputImage;\n</code></pre></p> <p>Al obtener la imagen resultante el filtro no realiza el procesamiento. Simplemente anota en la imagen las operaciones que se deben hacer en ella. Es decir, la imagen que obtenemos como imagen resultante, realmente contiene la imagen original y un conjunto de filtros a aplicar. Podemos encadenar varios filtros en una imagen:</p> <p>Swift <pre><code>for filter: CIFilter in filters {\n   filter[kCIInputImageKey] = filteredImage\n   filteredImage = filter.outputImage\n}\n</code></pre></p> <p>Objective-C <pre><code>for(CIFilter *filter in filters) {\n[filter setValue: filteredImage forKey: kCIInputImageKey];\nfilteredImage = filter.outputImage;\n}\n</code></pre></p> <p>Con el c\u00f3digo anterior vamos encadenando una serie de filtros en la imagen <code>CIImage</code> resultante, pero el procesamiento todav\u00eda no se habr\u00e1 realizado. Los filtros realmente se aplicar\u00e1n cuando rendericemos la imagen, bien en pantalla, o bien en forma de imagen <code>CGImageRef</code>.</p> <p>Por ejemplo, podemos renderizar la imagen directamente en el contexto gr\u00e1fico actual. Ese ser\u00e1 el momento en el que se aplicar\u00e1n realmente los filtros a la imagen, para mostrar la imagen resultante en pantalla:</p> <p>Swift <pre><code>override func draw(_ rect: CGRect)\n{\n    UIImage(ciImage: filteredImage).draw(at: CGPoint.zero)\n}\n</code></pre> Objective-C <pre><code>- (void)drawRect:(CGRect)rect\n{\n[[UIImage imageWithCIImage: filteredImage] drawAtPoint:CGPointZero];\n}\n</code></pre></p> <p>A continuaci\u00f3n veremos c\u00f3mo controlar la forma en la que se realiza el renderizado de la imagen mediante el contexto de Core Image.</p>"},{"location":"captura-ios.html#contexto-de-core-image","title":"Contexto de Core Image","text":"<p>El componente central del framework Core Image es la clase <code>CIContext</code> que representa el contexto de procesamiento de im\u00e1genes, que ser\u00e1 el motor que se encargar\u00e1 de aplicar diferentes filtros a las im\u00e1genes. Este contexto puede se de dos tipos:</p> <ul> <li>CPU: El procesamiento se realiza utilizando la CPU. La imagen resultante se obtiene como imagen de tipo Core Graphics (<code>CGImageRef</code>).</li> <li>GPU: El procesamiento se realiza utilizando la GPU, y la imagen se renderiza utilizando OpenGL ES 2.0.</li> </ul> <p>El contexto basado en CPU es m\u00e1s sencillo de utilizar, pero su rendimiento es mucho peor. Con el contexto basado en GPU se descarga totalmente a la CPU del procesamiento de la imagen, por lo que ser\u00e1 mucho m\u00e1s eficiente. Sin embargo, para utilizar la GPU nuestra aplicaci\u00f3n siempre debe estar en primer plano. Si queremos procesar im\u00e1genes en segundo plano deberemos utilizar el contexto basado en CPU.</p>"},{"location":"captura-ios.html#procesamiento-en-contexto-de-cpu","title":"Procesamiento en contexto de CPU","text":"<p>Para crear un contexto basado en CPU utilizaremos el m\u00e9todo <code>contextWithOption:</code></p> <p>Swift <pre><code>var context = CIContext(options: nil)\n</code></pre></p> <p>Objective-C <pre><code>CIContext *context = [CIContext contextWithOptions:nil];\n</code></pre></p> <p>Con este tipo de contexto la imagen se renderizar\u00e1 como <code>CGImageRef</code> mediante el m\u00e9todo <code>createCGImage:fromRect:</code>. Hay que especificar la regi\u00f3n de la imagen que queremos renderizar. Si queremos renderizar la imagen entera podemos utilizar el atributo <code>extent</code> de <code>CIImage</code>, que nos devuelve sus dimensiones:</p> <p>Swift <pre><code>var cgiimg = context.createCGImage((filter?.outputImage)!, from: (filter?.outputImage?.extent)!)\n</code></pre></p> <p>Objective-C <pre><code>CGImageRef cgImage = [context createCGImage:filteredImage\nfromRect:filteredImage.extent];\n</code></pre></p>"},{"location":"captura-ios.html#procesamiento-en-contexto-de-gpu","title":"Procesamiento en contexto de GPU","text":"<p>En el caso del contexto basado en GPU, en primer lugar deberemos crear el contexto OpenGL en nuestra aplicaci\u00f3n. Esto se har\u00e1 de forma autom\u00e1tica en el caso en el que utilicemos la plantilla de Xcode de aplicaci\u00f3n basada en OpenGL, aunque podemos tambi\u00e9n crearlo de forma sencilla en cualquier aplicaci\u00f3n con el siguiente c\u00f3digo:</p> <p>Swift <pre><code>let api: EAGLRenderingAPI = EAGLRenderingAPI.openGLES2 \nvar glContext = EAGLContext(api: api)\n</code></pre></p> <p>Objective-C <pre><code>EAGLContext *glContext =\n[[EAGLContext alloc] initWithAPI:kEAGLRenderingAPIOpenGLES2];\n</code></pre></p> <p>Una vez contamos con el contexto de OpenGL, podemos crear el contexto de Core Image basado en GPU con el m\u00e9todo <code>contextWithEAGLContext:</code></p> <p>Swift <pre><code>var context = CIContext(glContext, options: nil)\n</code></pre></p> <p>Objective-C <pre><code>CIContext *context = [CIContext contextWithEAGLContext: glContext];\n</code></pre></p> <p>Para realizar el procesamiento en tiempo real, si no necesitamos una alta fidelidad de color, se recomienda desactivar el uso del color space:</p> <p>Swift <pre><code>var options = [kCIContextWorkingColorSpace: NSNull()]\nvar context = CIContext(glContext, options: options)\n</code></pre></p> <p>Objective-C <pre><code>NSDictionary *options = @{ kCIContextWorkingColorSpace : [NSNull null] };\nCIContext *context = [CIContext contextWithEAGLContext:glContext options:options];\n</code></pre></p> <p>En este caso, para renderizar la imagen deberemos utilizar el m\u00e9todo <code>drawImage:atPoint:fromRect:</code> o <code>drawImage:inRect:fromRect:</code> del objeto <code>CIContext</code>. Con estos m\u00e9todos la imagen se renderizar\u00e1 en una capa de OpenGL. Para hacer esto podemos utilizar una vista de tipo <code>GLKView</code>. Podemos crear esta vista de la siguiente forma:</p> <p>Swift <pre><code>var rect : CGRect = CGRect(x: 0, y: 0, width: 320, height: 480)\nvar glkView = GLKView(frame: rect, context: glContext!)\nglkView.delegate = self\n</code></pre></p> <p>Objective-C <pre><code>GLKView *glkView = [[GLKView alloc] initWithFrame: CGRect(0,0,320,480)\ncontext: glContext];\nglkView.delegate = self;\n</code></pre></p> <p>El delegado de la vista OpenGL deber\u00e1 definir un m\u00e9todo <code>glkView:drawInRect:</code> en el que deberemos definir la forma de renderizar la vista OpenGL. Aqu\u00ed podemos hacer que se renderice la imagen filtrada:</p> <p>Swift <pre><code>func glkView(_ view: GLKView!, drawIn rect: CGRect) {\n ...\n context.draw(filteredImage, atPoint: CGPoint.zero,fromRect: filteredImage.extent())\n}\n</code></pre></p> <p>Objective-C <pre><code>- (void)glkView:(GLKView *)view drawInRect:(CGRect)rect {\n...\n\n[context drawImage: filteredImage\natPoint: CGPointZero\nfromRect: filteredImage.extent];\n}\n</code></pre></p> <p>Para hacer que la vista OpenGL actualice su contenido deberemos llamar a su m\u00e9todo <code>display</code>:</p> <p>Swift <pre><code>glkView.display()\n</code></pre></p> <p>Objective-C <pre><code>[glkView display];\n</code></pre></p> <p>Esto se har\u00e1 normalmente cuando hayamos definido nuevos filtros para la imagen, y queramos que se actualice el resultado en pantalla.</p> <p>La inicializaci\u00f3n del contexto es una operaci\u00f3n costosa que se debe hacer una \u00fanica vez. Una vez inicializado, notaremos que el procesamiento de las im\u00e1genes es mucho m\u00e1s fluido.</p>"},{"location":"captura-ios.html#procesamiento-asincrono","title":"Procesamiento as\u00edncrono","text":"<p>El procesamiento de la imagen puede ser una operaci\u00f3n lenta, a pesar de estar optimizada. Por lo tanto, al realizar esta operaci\u00f3n desde alg\u00fan evento (por ejemplo al pulsar un bot\u00f3n, o al modificar en la interfaz alg\u00fan factor de ajuste del filtro a aplicar) deber\u00edamos realizar la operaci\u00f3n en segundo plano. Podemos utilizar para ello la clase <code>NSThread</code>, o bien las facilidades para ejecutar c\u00f3digo en segundo plano que se incluyeron a partir de iOS 4.0, basadas en bloques.</p> <pre><code>dispatch_async(\ndispatch_get_global_queue(DISPATCH_QUEUE_PRIORITY_BACKGROUND, 0),\n^(void) {\n// Codigo en segundo plano (renderizar la imagen\nCGImageRef cgImage = [context createCGImage:filteredImage\nfromRect:filteredImage.extent];\n...\n}\n);\n</code></pre> <p>Con esto podemos ejecutar un bloque de c\u00f3digo en segundo plano. El problema que encontramos es que dicho bloque de c\u00f3digo no se encuentra en el hilo de la interfaz, por lo que no podr\u00e1 acceder a ella. Para solucionar este problema deberemos mostrar la imagen obtenida en la interfaz dentro de un bloque que se ejecute en el hilo de la UI:</p> <pre><code>dispatch_async(\ndispatch_get_global_queue(DISPATCH_QUEUE_PRIORITY_BACKGROUND, 0),\n^(void) {\n...\ndispatch_async(dispatch_get_main_queue(), ^(void) {\nself.imageView.image = [UIImage imageWithCGImage: cgImage];\n});\n}\n);\n</code></pre> <p>Con esto podemos ejecutar un bloque de c\u00f3digo de forma as\u00edncrona dentro del hilo principal de la UI, y de esta forma podremos mostrar la imagen obtenida en segundo plano en la interfaz.</p>"},{"location":"captura-ios.html#deteccion-de-caras","title":"Detecci\u00f3n de caras","text":"<p>A parte de los filtros vistos anteriormente, Core Image tambi\u00e9n incluye detectores de caracter\u00edsticas en im\u00e1genes,  como por ejemplo detectores de caras, de texto, o de c\u00f3digos QR. La API est\u00e1 dise\u00f1ada para poder ser ampliada en el futuro.</p> <p>Los detectores los crearemos mediante la clase <code>CIDetector</code>. Deberemos proporcionar el tipo de detector a utilizar, por ejemplo <code>CIDetectorTypeFace</code> para el detector de caras. Podemos adem\u00e1s especificar una serie de par\u00e1metros, como el nivel de precisi\u00f3n que queremos obtener:</p> <p>Swift <pre><code>let options = [CIDetectorAccuracy: CIDetectorAccuracyHigh]\nlet faceDetector = CIDetector(ofType: CIDetectorTypeFace, context: nil, options: options)!\n</code></pre></p> <p>Objective-C <pre><code>CIDetector* detector = [CIDetector detectorOfType:CIDetectorTypeFace\ncontext:nil\noptions:[NSDictionary dictionaryWithObject:CIDetectorAccuracyHigh\nforKey:CIDetectorAccuracy]];\n</code></pre></p> <p>Una vez creado el detector, podemos ejecutarlo para que procese la imagen (de tipo <code>CIImage</code>) en busca de las caracter\u00edsticas deseadas (en este caso estas caracter\u00edsticas son las caras):</p> <p>Swift <pre><code>let features = faceDetector.features(in: ciImage)\n</code></pre></p> <p>Objetive-C <pre><code>NSArray* features = [detector featuresInImage:ciImage];\n</code></pre></p> <p>Las caracter\u00edsticas obtenidas se encapsulan en objetos de tipo <code>CIFeature</code>. Una propiedad b\u00e1sica de las caracter\u00edsticas es la regi\u00f3n que ocupan en la imagen. Esto se representa mediante su propiedad <code>bounds</code>, de tipo <code>CGRect</code>, que nos indicar\u00e1 el \u00e1rea de la imagen en la que se encuentra la cara. Pero adem\u00e1s, en el caso concreto del reconocimiento de caras, las caracter\u00edsticas obtenidas son un subtipo espec\u00edfico de <code>CIFeature</code> (<code>CIFaceFeature</code>), que adem\u00e1s de la regi\u00f3n ocupada por la cara nos proporcionar\u00e1 la regi\u00f3n ocupada por componentes de la cara (boca y ojos).</p> <p>Es decir, este detector nos devolver\u00e1 un array con tantos objetos <code>CIFaceFeature</code> como caras encontradas en la imagen, y de cada cara sabremos el \u00e1rea que ocupa y la posici\u00f3n de los ojos y la boca, en caso de que los haya encontrado.</p>"},{"location":"captura-ios.html#ejercicios","title":"Ejercicios","text":""},{"location":"captura-ios.html#procesamiento-de-imagen","title":"Procesamiento de imagen","text":"<p>En este ejercicio procesaremos una imagen con CoreImage tanto utilizando la CPU como la GPU. En el proyecto <code>ProcesamientoImagen</code> tenemos toda la infraestructura necesaria ya creada. En <code>viewDidLoad</code> se inicializa la imagen <code>CIImage</code> original, y los contextos CPU y GPU. Tenemos dos sliders que nos permitir\u00e1n aplicar filtros con diferentes niveles de intensidad. En la parte superior de la pantalla tenemos una imagen (<code>UIImageView</code>) con un slider para aplicar el filtro utilizando la CPU, y en la mitad inferior tenemos una vista OpenGL (<code>GLKView</code>) y un slider para aplicar el filtro en ella utilizando la GPU. Se pide:</p> <ul> <li> <p>Implementar el filtrado utilizando CPU, en el m\u00e9todo <code>sliderCpuCambia:</code> que se ejecutar\u00e1 cada vez que el slider superior cambie de valor. Utilizaremos el filtro de color sepia (<code>CISepiaTone</code>), al que proporcionaremos como intensidad el valor del slider.</p> </li> <li> <p>Implementar el filtrado utilizando GPU, en el m\u00e9todo <code>sliderCpuCambia:</code> que se ejecutar\u00e1 cada vez que el slider inferior cambie de valor. Utilizaremos el mismo filtro que en el caso anterior, pero en este caso guardaremos la imagen resultante en la propiedad <code>imagenFiltrada</code> y haremos que se redibuje la vista OpenGL para que muestre dicha imagen. Mueve los dos sliders. \u00bfCu\u00e1l de ellos se mueve con mayor fluidez?</p> </li> <li> <p>Vamos a encadenar un segundo filtro, tanto para el contexto CPU como GPU. El filtro ser\u00e1 <code>CIHueAdjust</code>, que se aplicar\u00e1 justo despu\u00e9s del filtro sepia. Consulta la documentaci\u00f3n de filtros de Apple para saber qu\u00e9 par\u00e1metros son necesarios. Se utilizar\u00e1 el mismo slider que ya tenemos para darle valor a este par\u00e1metro, es decir, el mismo slider dar\u00e1 valor simult\u00e1neamente a los par\u00e1metros de los dos filtros.</p> </li> <li> <p>Por \u00faltimo, vamos a permitir guardar la foto procesada mediante CPU en el \u00e1lbum de fotos del dispositivo. Para ello deberemos introducir en el m\u00e9todo <code>agregarFoto:</code> el c\u00f3digo que se encargue de realizar esta tarea, tomando la foto de <code>self.imageView.image</code>. Este m\u00e9todo se ejecutar\u00e1 al pulsar sobre el bot\u00f3n que hay junto a la imagen superior.</p> </li> </ul>"},{"location":"difusion.html","title":"Difusi\u00f3n de medios","text":""},{"location":"dispositivos-vestibles.html","title":"Dispositivos vestibles","text":"<p>Existen distintos tipos de dispositivos que act\u00faan como complementos a los tel\u00e9fonos m\u00f3viles y tables, como por ejemplo relojes y gafas, que son conocidos como dispositivos vestibles (wearable devices).</p> <p>Estos dispositivos cuentan con una interfaz m\u00e1s limitada, con una pantalla m\u00e1s reducida y sin la posibilidad de mostrar un teclado t\u00e1ctil, pero nos permiten tener un acceso m\u00e1s r\u00e1pido a informaci\u00f3n b\u00e1sica, como por ejemplo a notificaciones que recibamos en el m\u00f3vil.</p> <p>Vamos a estudiar estos dispositivos mediante dos ejemplos: los relojes Android y las Google Glass. En ambos casos, suple las limitaciones de la interfaz con:</p> <ul> <li>Comandos de voz que podemos utilizar para lanzar las aplicaciones.</li> <li>Interfaz basadas en tarjetas (cards).</li> <li>Gestos para navegar por la aplicaci\u00f3n.</li> </ul>"},{"location":"dispositivos-vestibles.html#google-glass","title":"Google Glass","text":"<p>El desarrollo de aplicaciones para Google Glass se realiza utilizando Glass Development Kit (GDK) que se encuentra bajo la API 19.</p> <p>En primer lugar deberemos entrar en Android SDK Manager y nos aseguramos de que en el bloque Android 4.4.2 (API 19) tengamos instalado el _item Glass Development Kit Preview. En caso de no tenerlo instalado, lo instalaremos.</p> <p></p> <p>Una vez instalado GDK, para crear un proyecto para Glass deberemos crear un nuevo Android Application Project y en el bloque Compile With seleccionaremos Glass Development Kit Preview (Google Inc.) (API 19).</p> <p></p> <p>Aunque el proyecto para Google Glass se puede crear con Eclipse + ADT como hemos visto, tendremos mayores facilidades si utilizamos Android Studio, ya que nos permite crear una plantilla espec\u00edfica para este tipo de proyectos.</p> <p>Al desarrollar aplicaciones con GDK contaremos con la API de Android y con algunas clases adicionales incluidas en GDK como por ejemplo reconocer los gestos habituales en las aplicaciones para estos dispositivos, crear y mostrar tarjetas, o lanzar actividades con comandos de voz.</p> <p>Vamos a ver a continuaci\u00f3n los elementos principales que deber\u00eda tener una aplicaci\u00f3n destinada a las Google Glass.</p>"},{"location":"dispositivos-vestibles.html#triggers-de-voz","title":"Triggers de voz","text":"<p>Dado que una de las ventajas de las Google Glass es poder utilizarlas sin ocupar las manos, ser\u00e1 importante dar la posibilidad de lanzar nuestras actividades mediante comandos de voz. Esto se har\u00e1 pronunciando \"Ok glass\" seguido de la aplicaci\u00f3n que queremos ejecutar.</p> <p></p> <p>Para implementar esta funcionalidad, en la declaraci\u00f3n de nuestra actividad en <code>AndroidManifest.xml</code> a\u00f1adiremos un <code>&lt;intent-filter&gt;</code> de tipo <code>VOICE_TRIGGER</code>, y una etiqueta <code>&lt;meta-data&gt;</code> con la que indicaremos cu\u00e1l ser\u00e1 el comando de voz que podr\u00e1 lanzar la actividad:</p> <pre><code>&lt;activity\nandroid:name=\"ua.es.jtech.glass.MainActivity\"\nandroid:label=\"@string/app_name\" &gt;\n&lt;intent-filter&gt;\n&lt;action android:name=\"com.google.android.glass.action.VOICE_TRIGGER\" /&gt;\n&lt;/intent-filter&gt;\n\n&lt;meta-data\nandroid:name=\"com.google.android.glass.VoiceTrigger\"\nandroid:resource=\"@xml/voice_trigger_start\" /&gt;\n&lt;/activity&gt;\n</code></pre> <p>Los datos sobre el trigger a utilizar se definen en el fichero <code>res/xml/voice_trigger_start.xml</code>:</p> <pre><code>&lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt;\n&lt;trigger command=\"START_BROADCASTING\" /&gt;\n</code></pre> <p>Tenemos una serie de triggers predefinidos para lanzar la aplicaci\u00f3n. Por ejemplo, en el caso anterior nuestra actividad se abrir\u00e1 cuando pronunciemos \"Ok glass\" seguido del comando \"Start broadcasting\".</p> <p>Podemos obtener la lista de posibles comandos de voz consultando la lista de elementos de la enumeraci\u00f3n <code>VoiceTriggers.Command</code> de GDK:</p> <p>https://developers.google.com/glass/develop/gdk/reference/com/google/android/glass/app/VoiceTriggers.Command</p>"},{"location":"dispositivos-vestibles.html#tarjetas","title":"Tarjetas","text":"<p>La interfaz de las aplicaciones Google Glass normalmente se componen de una serie de tarjetas, que habitualmente tienen fondo negro y texto blanco. Podemos definirlas en XML o de forma programada mediante <code>CardBuilder</code></p> <p></p>"},{"location":"dispositivos-vestibles.html#tarjetas-con-cardbuilder","title":"Tarjetas con <code>CardBuilder</code>","text":"<p>La clase <code>CardBuilder</code> nos permite construir tarjetas que se ajusten a la disposici\u00f3n recomendada. Tenemos definidos diferentes posibles layouts en <code>CardBuilder.Layout</code> que podremos aplicar a las tarjetas en el momento de su creaci\u00f3n.</p> <pre><code>View view = new CardBuilder(context, CardBuilder.Layout.TEXT)\n.setText(\"Texto del contenido de la tarjeta\")\n.setFootnote(\"Nota a pie\")\n.setTimestamp(\"ahora\")\n.getView();\n</code></pre> <p>Una vez obtenida la vista de la tarjeta, podremos a\u00f1adirla como contenido a la actividad o bien a un objeto <code>CardScrollView</code> para as\u00ed poder navegar a trav\u00e9s de varias tarjetas mediante scroll.</p>"},{"location":"dispositivos-vestibles.html#tarjetas-en-xml","title":"Tarjetas en XML","text":"<p>Si ninguno de los layout definidos en <code>CardBuilder.Layout</code> se ajusta a nuestras necesidades, podemos crear nuestras propias tarjetas en XML.</p> <p>Para definir el layout de las tarjetas en XML podemos definir una serie de dimensiones est\u00e1ndar en el fichero <code>res/values/dimens.xml</code>:</p> <pre><code>&lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt;\n\n&lt;resources&gt;\n\n&lt;!-- Margen recomendado para los bordes de la tarjeta. --&gt;\n&lt;dimen name=\"glass_card_margin\"&gt;40px&lt;/dimen&gt;\n\n&lt;!-- Margen recomendado entre la parte inferior de la tarjeta\n         y el pie. Se ajusta de forma que la linea de base del texto\n         del pie se situe a 40px de la parte inferior de la tarjeta.\n    --&gt;\n&lt;dimen name=\"glass_card_footer_margin\"&gt;33px&lt;/dimen&gt;\n\n&lt;!-- Margen recomendado para la columna izquierda. --&gt;\n&lt;dimen name=\"glass_card_two_column_margin\"&gt;30px&lt;/dimen&gt;\n\n&lt;!-- Maxima altura del contenido del cuerpo de la tarjeta. --&gt;\n&lt;dimen name=\"glass_card_body_height\"&gt;240px&lt;/dimen&gt;\n\n&lt;!-- Anchura de la columna izquierda. --&gt;\n&lt;dimen name=\"glass_card_left_column_width\"&gt;240px&lt;/dimen&gt;\n\n&lt;/resources&gt;\n</code></pre> <p>Una vez definidas estas dimensiones, podemos utilizar el siguiente layout como plantilla para las tarjetas:</p> <pre><code>&lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt;\n&lt;FrameLayout xmlns:android=\"http://schemas.android.com/apk/res/android\"\nxmlns:tools=\"http://schemas.android.com/tools\"\nandroid:id=\"@+id/layout\"\nandroid:layout_width=\"match_parent\"\nandroid:layout_height=\"match_parent\"\n&gt;\n\n&lt;RelativeLayout\nandroid:id=\"@+id/body_layout\"\nandroid:layout_width=\"match_parent\"\nandroid:layout_height=\"@dimen/glass_card_body_height\"\nandroid:layout_marginLeft=\"@dimen/glass_card_margin\"\nandroid:layout_marginTop=\"@dimen/glass_card_margin\"\nandroid:layout_marginRight=\"@dimen/glass_card_margin\"\ntools:ignore=\"UselessLeaf\"\n&gt;\n\n&lt;!-- Introducir aqu\u00ed el contenido de la tarjeta. --&gt;\n\n&lt;/RelativeLayout&gt;\n\n&lt;LinearLayout\nandroid:id=\"@+id/footer_container\"\nandroid:layout_width=\"match_parent\"\nandroid:layout_height=\"wrap_content\"\nandroid:layout_gravity=\"bottom|left\"\nandroid:layout_marginLeft=\"@dimen/glass_card_margin\"\nandroid:layout_marginBottom=\"@dimen/glass_card_footer_margin\"\nandroid:layout_marginRight=\"@dimen/glass_card_margin\"\nandroid:orientation=\"horizontal\"\n&gt;\n\n&lt;TextView\nandroid:id=\"@+id/footer\"\nandroid:layout_width=\"0dip\"\nandroid:layout_height=\"wrap_content\"\nandroid:layout_weight=\"1\"\nandroid:ellipsize=\"end\"\nandroid:singleLine=\"true\"\nandroid:textAppearance=\"?android:attr/textAppearanceSmall\"\n/&gt;\n\n&lt;TextView\nandroid:id=\"@+id/timestamp\"\nandroid:layout_width=\"wrap_content\"\nandroid:layout_height=\"wrap_content\"\nandroid:layout_marginLeft=\"@dimen/glass_card_margin\"\nandroid:ellipsize=\"end\"\nandroid:singleLine=\"true\"\nandroid:textAppearance=\"?android:attr/textAppearanceSmall\"\n/&gt;\n\n&lt;/LinearLayout&gt;\n\n&lt;/FrameLayout&gt;\n</code></pre> <p>En este layout tenemos una secci\u00f3n principal de contenido, y adem\u00e1s tambi\u00e9n podemos introducir un pie y un timestamp.</p> <p>Podemos tambi\u00e9n crear tarjetas con dos columnas utilizando la siguiente plantilla:</p> <pre><code>&lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt;\n&lt;RelativeLayout xmlns:android=\"http://schemas.android.com/apk/res/android\"\nxmlns:tools=\"http://schemas.android.com/tools\"\nandroid:layout_width=\"match_parent\"\nandroid:layout_height=\"match_parent\"\n&gt;\n\n&lt;RelativeLayout\nandroid:id=\"@+id/left_column\"\nandroid:layout_width=\"@dimen/glass_card_left_column_width\"\nandroid:layout_height=\"match_parent\"\n&gt;\n\n&lt;!-- Introducir contenido de la columna izquierda. --&gt;\n\n&lt;/RelativeLayout&gt;\n\n&lt;RelativeLayout\nandroid:layout_width=\"wrap_content\"\nandroid:layout_height=\"@dimen/glass_card_body_height\"\nandroid:layout_alignParentRight=\"true\"\nandroid:layout_alignParentTop=\"true\"\nandroid:layout_marginLeft=\"@dimen/glass_card_two_column_margin\"\nandroid:layout_marginRight=\"@dimen/glass_card_margin\"\nandroid:layout_marginTop=\"@dimen/glass_card_margin\"\nandroid:layout_toRightOf=\"@+id/left_column\"\ntools:ignore=\"UselessLeaf\"\n&gt;\n\n&lt;!-- Introducir contenido de la columna derecha. --&gt;\n\n&lt;/RelativeLayout&gt;\n\n&lt;LinearLayout\nandroid:id=\"@+id/footer_container\"\nandroid:layout_width=\"wrap_content\"\nandroid:layout_height=\"wrap_content\"\nandroid:layout_alignParentBottom=\"true\"\nandroid:layout_alignParentRight=\"true\"\nandroid:layout_gravity=\"bottom|left\"\nandroid:layout_marginBottom=\"@dimen/glass_card_footer_margin\"\nandroid:layout_marginLeft=\"@dimen/glass_card_two_column_margin\"\nandroid:layout_marginRight=\"@dimen/glass_card_margin\"\nandroid:layout_toRightOf=\"@+id/left_column\"\nandroid:orientation=\"horizontal\"\n&gt;\n\n&lt;TextView\nandroid:id=\"@+id/footer\"\nandroid:layout_width=\"0dip\"\nandroid:layout_height=\"wrap_content\"\nandroid:layout_weight=\"1\"\nandroid:ellipsize=\"end\"\nandroid:singleLine=\"true\"\nandroid:textAppearance=\"?android:attr/textAppearanceSmall\"\n/&gt;\n\n&lt;TextView\nandroid:id=\"@+id/timestamp\"\nandroid:layout_width=\"wrap_content\"\nandroid:layout_height=\"wrap_content\"\nandroid:layout_marginLeft=\"@dimen/glass_card_margin\"\nandroid:ellipsize=\"end\"\nandroid:singleLine=\"true\"\nandroid:textAppearance=\"?android:attr/textAppearanceSmall\"\n/&gt;\n\n&lt;/LinearLayout&gt;\n\n&lt;/RelativeLayout&gt;\n</code></pre>"},{"location":"dispositivos-vestibles.html#gestos","title":"Gestos","text":"<p>En las aplicaciones Google Glass encontramos una serie de gestos est\u00e1ndar:</p> <ul> <li>El barrido lateral a la izquierda o la derecha nos permite movernos entre las diferentes tarjetas disponibles.</li> <li>El barrido hacia abajo nos permite salir de la actividad actual.</li> <li>El tap nos permite seleccionar la tarjeta actual.</li> </ul> <p>Podemos utilizar el objeto <code>GestureDetector</code> para reconocer los diferentes gestos disponibles:</p> <pre><code>public class MainActivity extends Activity {\n\nprivate final GestureDetector.BaseListener mBaseListener = new GestureDetector.BaseListener() {\n@Override\npublic boolean onGesture(Gesture gesture) {\nif (gesture == Gesture.TAP) {\nmAudioManager.playSoundEffect(Sounds.TAP);\n\n// Realizar acci\u00f3n\n\nreturn true;\n} else {\nreturn false;\n}\n}\n};\n\nprivate AudioManager mAudioManager;\n\nprivate GestureDetector mGestureDetector;\n\n@Override\nprotected void onCreate(Bundle savedInstanceState) {\nsuper.onCreate(savedInstanceState);\nsetContentView(R.layout.activity_main);\n\nmAudioManager = (AudioManager) getSystemService(Context.AUDIO_SERVICE);\nmGestureDetector = new GestureDetector(this).setBaseListener(mBaseListener);\n}\n\n}\n</code></pre>"},{"location":"dispositivos-vestibles.html#relojes","title":"Relojes","text":"<p>Para el desarrollo de aplicaciones para este tipo de dispositivos utilizaremos Android 4.4W (versi\u00f3n para wearables), y la librer\u00eda de soporte v4 (o v13) que incluye soporte para wearables.</p> <p>Las aplicaciones para wearables desarrolladas con esta versi\u00f3n de la API siempre deber\u00e1n distribuirse dentro de una aplicaci\u00f3n para dispositivos m\u00f3viles. Durante el desarrollo podemos instalar la aplicaci\u00f3n wear directamente en el dispositivo o emulador del wearable, pero cuando la distribuyamos siempre deber\u00e1 instalarse como complemento de la aplicaci\u00f3n m\u00f3vil.</p> <p></p> <p>Entre los dispositivos soportados por esta API, encontramos relojes que tienen pantalla rectangular y tambi\u00e9n tenemos relojes con pantalla circular. Esto supondr\u00e1 una dificultad ya que elementos que se ven correctamente en una pantalla cuadrada pueden aparecer cortados en una circular.</p>"},{"location":"dispositivos-vestibles.html#creacion-del-emulador","title":"Creaci\u00f3n del emulador","text":"<p>Para crear un emulador de un dispositivos wear deberemos especificar los siguientes datos:</p> <ul> <li>Target: Android 4.4W</li> <li>Device: Android Wear Round o Android Wear Square, seg\u00fan si queremos emulador un reloj con pantalla rectangular o circular.</li> </ul> <p>Se podr\u00e1 emparejar el emulador o dispositivo wearable con un dispositivo m\u00f3vil, instalando en este \u00faltimo la aplicaci\u00f3n Android Wear.</p>"},{"location":"dispositivos-vestibles.html#creacion-de-un-proyecto","title":"Creaci\u00f3n de un proyecto","text":"<p>Es recomendable utilizar Android Studio para la creaci\u00f3n de proyectos wear, ya que est\u00e1 preparado para generar plantillas para este tipo de proyectos.</p> <p></p> <p>Al crear un proyecto se deber\u00e1 crear la aplicaci\u00f3n m\u00f3vil y la aplicaci\u00f3n wear adjunta. Tendremos que especificar:</p> <ul> <li>Para tel\u00e9fono/tablet el SDK m\u00ednimo ser\u00e1 la API 9 (2.3)</li> <li>Para dispositivo wear el SDK m\u00ednimo ser\u00e1 la API 20 (4.4W)</li> </ul> <p>Podremos crear una actividad inicial tanto para la aplicaci\u00f3n m\u00f3vil como para la aplicaci\u00f3n wear. El entorno nos da la opci\u00f3n de crear una plantilla de una actividad wear vac\u00eda:</p> <p></p> <p>De la actividad wear se crear\u00e1 un layout general, y dos layouts alternativos seg\u00fan si el dispositivo es rect\u00e1ngular o circular:</p> <p></p>"},{"location":"dispositivos-vestibles.html#comandos-de-voz","title":"Comandos de voz","text":"<p>Al igual que en el caso de Glass, podemos utilizar una serie de comandos de voz predefinidos para lanzar las aplicaciones. En este caso, con estos comandos el sistema lanzar\u00e1 un intent que nuestra aplicaci\u00f3n puede capturar para ejecutarse cuando se produzca. Por ejemplo tenemos:</p> <pre><code>&lt;activity android:name=\"MiTaxiActivity\"&gt;\n&lt;intent-filter&gt;\n&lt;action android:name=\"com.google.android.gms.actions.RESERVE_TAXI_RESERVATION\" /&gt;\n&lt;/intent-filter&gt;\n&lt;/activity&gt;\n</code></pre> <p>Cuando digamos \u201cOk google, get me a taxi\u201d se producir\u00e1 el intent <code>RESERVE_TAXI_RESERVATION</code> que lanzar\u00e1 la actividad anterior, al haber incluido dicha acci\u00f3n como <code>&lt;intent-filter&gt;</code>.</p> <p>Podemos ver la lista completa de comandos de voz en:</p> <p>http://developer.android.com/training/wearables/apps/voice.html</p> <p>Si ninguno de los comandos disponibles se adapta a nuestra aplicaci\u00f3n, podemos lanzarla simplemente diciendo \u201cOk google, start [nombre de la aplicaci\u00f3n]\u201d. Para ello definiremos en la actividad principal una etiqueta <code>android:label</code> en la que indicaremos el nombre que deberemos pronunciar para ejecutar nuestra aplicaci\u00f3n:</p> <pre><code>&lt;application&gt;\n&lt;activity android:name=\"MainActivity\" android:label=\"Vision\"&gt;\n&lt;intent-filter&gt;\n&lt;action android:name=\"android.intent.action.MAIN\" /&gt;\n&lt;category android:name=\"android.intent.category.LAUNCHER\" /&gt;\n&lt;/intent-filter&gt;\n&lt;/activity&gt;\n&lt;/application&gt;\n</code></pre> <p>En esta caso anterior la aplicaci\u00f3n se ejecutar\u00e1 cuando digamos \u201cOk google, start Vision\u201d.</p> <p>Dentro de nuestras actividades tambi\u00e9n podremos utilizar el sistema de reconocimiento del habla de Android para poder manejarlas mediante voz.</p>"},{"location":"dispositivos-vestibles.html#interfaz-de-usuario","title":"Interfaz de usuario","text":"<p>Como hemos comentado anteriormente, la existencia de relojes circulares y rect\u00e1ngulares puede ser una complicaci\u00f3n a la hora de dise\u00f1ar la interfaz de nuestras aplicaciones wear. Existen dos enfoques para hacer que las aplicaciones se adapten correctamente a cada forma:</p> <ul> <li>Utilizar layouts alternativos para cada tipo de dispositivo</li> <li>Utilizar un layout que adapte de forma autom\u00e1tica los m\u00e1rgenes</li> </ul>"},{"location":"dispositivos-vestibles.html#layouts-alternativos","title":"Layouts alternativos","text":"<p>Una posible soluci\u00f3n consiste en especificar diferentes layouts seg\u00fan si tenemos pantallas rectangulares o circulares. Para ello utilizamos un elemento <code>WatchViewStub</code> en el layout, en el cual especificamos dos layouts alternativos seg\u00fan el tipo de forma del dispositivo:</p> <pre><code>&lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt;\n&lt;android.support.wearable.view.WatchViewStub\nxmlns:android=\"http://schemas.android.com/apk/res/android\"\nxmlns:app=\"http://schemas.android.com/apk/res-auto\"\nxmlns:tools=\"http://schemas.android.com/tools\"\nandroid:id=\"@+id/watch_view_stub\"\nandroid:layout_width=\"match_parent\"\nandroid:layout_height=\"match_parent\"\napp:rectLayout=\"@layout/rect_activity_mi_actividad_wear\"\napp:roundLayout=\"@layout/round_activity_mi_actividad_wear\"\ntools:context=\".MiActividadWear\"\ntools:deviceIds=\"wear\"&gt;\n&lt;/android.support.wearable.view.WatchViewStub&gt;\n</code></pre> <p>Por ejemplo, el layout para dispositivos rectangulares podr\u00eda ser como el siguiente:</p> <pre><code>&lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt;\n&lt;LinearLayout xmlns:android=\"http://schemas.android.com/apk/res/android\"\nxmlns:tools=\"http://schemas.android.com/tools\"\nandroid:layout_width=\"match_parent\"\nandroid:layout_height=\"match_parent\"\nandroid:orientation=\"vertical\"\ntools:context=\".MiActividadWear\"\ntools:deviceIds=\"wear_square\"&gt;\n\n&lt;TextView\nandroid:id=\"@+id/text\"\nandroid:layout_width=\"wrap_content\"\nandroid:layout_height=\"wrap_content\"\nandroid:text=\"@string/hello_square\" /&gt;\n&lt;/LinearLayout&gt;\n</code></pre> <p></p> <p>Por otro lado, para dispositivos circulares podr\u00edamos tener:</p> <pre><code>&lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt;\n&lt;RelativeLayout xmlns:android=\"http://schemas.android.com/apk/res/android\"\nxmlns:tools=\"http://schemas.android.com/tools\"\nandroid:layout_width=\"match_parent\"\nandroid:layout_height=\"match_parent\"\ntools:context=\".MiActividadWear\"\ntools:deviceIds=\"wear_round\"&gt;\n\n&lt;TextView\nandroid:id=\"@+id/text\"\nandroid:layout_width=\"wrap_content\"\nandroid:layout_height=\"wrap_content\"\nandroid:layout_centerHorizontal=\"true\"\nandroid:layout_centerVertical=\"true\"\nandroid:text=\"@string/hello_round\" /&gt;\n&lt;/RelativeLayout&gt;\n</code></pre> <p></p>"},{"location":"dispositivos-vestibles.html#adaptacion-automatica-de-los-margenes","title":"Adaptaci\u00f3n autom\u00e1tica de los m\u00e1rgenes","text":"<p>Contamos con el layout <code>BoxInsetLayout</code> que permite adaptar la interfaz a ambos tipos de pantallas:</p> <pre><code>&lt;android.support.wearable.view.BoxInsetLayout\nxmlns:android=\"http://schemas.android.com/apk/res/android\"\nxmlns:app=\"http://schemas.android.com/apk/res-auto\"\nandroid:background=\"@drawable/fondo\"\nandroid:layout_height=\"match_parent\"\nandroid:layout_width=\"match_parent\"\nandroid:padding=\"15dp\"&gt;\n\n&lt;FrameLayout\nandroid:id=\"@+id/frame_layout\"\nandroid:layout_width=\"match_parent\"\nandroid:layout_height=\"match_parent\"\nandroid:padding=\"5dp\"\napp:layout_box=\"all\"&gt;\n\n&lt;!-- Introducir contenido del layout --&gt;\n\n&lt;/FrameLayout&gt;\n&lt;/android.support.wearable.view.BoxInsetLayout&gt;\n</code></pre> <p>Este layout permitir\u00e1 ajustar los m\u00e1rgenes de su contenido de forma que queden dentro del espacio de la pantalla. Con el atributo <code>app:layout_box=\"all\"</code> haremos que el marco del layout quede dentro de todos los l\u00edmites (superior, inferior, derecho e izquierdo) de la pantalla del reloj.</p> <p>En Android 4.4W2 existe un bug que cause que <code>BoxInsetLayout</code> no funcione correctamente en dispositivos redondos.</p>"},{"location":"dispositivos-vestibles.html#tarjetas_1","title":"Tarjetas","text":"<p>Podemos a\u00f1adir tarjetas a un layout como el anterior con las clase <code>CardFragment</code>. Las tarjetas creadas con esta clase tendr\u00e1n un t\u00edtulo, descripci\u00f3n e icono.</p> <pre><code>FragmentManager fragmentManager = getFragmentManager();\nFragmentTransaction fragmentTransaction = fragmentManager.beginTransaction();\nCardFragment cardFragment = CardFragment.create(\"Titulo\",\n\"Descripcion\",\nR.drawable.icono);\nfragmentTransaction.add(R.id.frame_layout, cardFragment);\nfragmentTransaction.commit();\n</code></pre> <p>Con <code>CardFrame</code> podemos a\u00f1adir la tarjeta directamente en el layout XML. Podemos tambi\u00e9n introducir esta tarjeta en un cuadro con scroll, utilizando el elemento <code>CardScrollView</code>.</p> <pre><code>&lt;android.support.wearable.view.BoxInsetLayout\nxmlns:android=\"http://schemas.android.com/apk/res/android\"\nxmlns:app=\"http://schemas.android.com/apk/res-auto\"\nandroid:background=\"#aaf\"\nandroid:layout_height=\"match_parent\"\nandroid:layout_width=\"match_parent\"&gt;\n\n&lt;android.support.wearable.view.CardScrollView\nandroid:id=\"@+id/card_scroll_view\"\nandroid:layout_height=\"match_parent\"\nandroid:layout_width=\"match_parent\"\napp:layout_box=\"bottom\"&gt;\n\n&lt;android.support.wearable.view.CardFrame\nandroid:layout_height=\"wrap_content\"\nandroid:layout_width=\"fill_parent\"&gt;\n\n&lt;LinearLayout\nandroid:layout_height=\"wrap_content\"\nandroid:layout_width=\"match_parent\"\nandroid:orientation=\"vertical\"\nandroid:paddingLeft=\"5dp\"&gt;\n&lt;TextView\nandroid:layout_height=\"wrap_content\"\nandroid:layout_width=\"match_parent\"\nandroid:text=\"Titulo\"\nandroid:textColor=\"@color/black\"\nandroid:textSize=\"20sp\"/&gt;\n&lt;TextView\nandroid:layout_height=\"wrap_content\"\nandroid:layout_width=\"match_parent\"\nandroid:text=\"Descripci\u00f3n\"\nandroid:textColor=\"@color/black\"\nandroid:textSize=\"14sp\"/&gt;\n&lt;/LinearLayout&gt;\n&lt;/android.support.wearable.view.CardFrame&gt;\n&lt;/android.support.wearable.view.CardScrollView&gt;\n&lt;/android.support.wearable.view.BoxInsetLayout&gt;\n</code></pre>"},{"location":"dispositivos-vestibles.html#listas","title":"Listas","text":"<p>Para introducir una lista utilizaremos la vista <code>WearableListView</code>. Al igual que en el caso de <code>ListView</code> ser\u00e1 necesario crear un layout para los items (que implementar\u00e1 <code>WearableListView.Item</code>), y un adaptador (de tipo <code>WearableListView.Adapter</code>) para poblar la lista.</p> <p>Tambi\u00e9n tenemos <code>GridViewPager</code> para crear un selector 2D.</p>"},{"location":"dispositivos-vestibles.html#pantallas-de-confirmacion","title":"Pantallas de confirmaci\u00f3n","text":"<p>Contamos con actividad <code>ConfirmationActivity</code> que podemos utilizar para mostrar confirmaciones de alguna acci\u00f3n que se ha desarrollado. Esta ser\u00e1 la forma est\u00e1ndar de mostrar confirmaciones en este tipo de dispositivos. Para poder utilizar esta actividad deberemos declararla en el manifest, aunque no la tengamos que crear nosotros:</p> <pre><code>&lt;activity\nandroid:name=\"android.support.wearable.activity.ConfirmationActivity\"&gt;\n&lt;/activity&gt;\n</code></pre> <p>Una vez declarada, podremos lanzarla mediante un intent:</p> <pre><code>Intent intent = new Intent(this, ConfirmationActivity.class);\nintent.putExtra(ConfirmationActivity.EXTRA_ANIMATION_TYPE,\nConfirmationActivity.SUCCESS_ANIMATION);\nintent.putExtra(ConfirmationActivity.EXTRA_MESSAGE,\n\"Enviado\");\nstartActivity(intent);\n</code></pre> <p>Tambi\u00e9n podemos la vista <code>DelayedConfirmationView</code> para dar al usuario un tiempo para cancelar la acci\u00f3n antes de que se confirme.</p>"},{"location":"formatos.html","title":"Formatos de audio y v\u00eddeo","text":"<p>Una de las principales caracter\u00edsticas de los dispositivos m\u00f3viles es la gran heterogeneidad que existe entre los dispositivos existentes. Una de las repercusiones de este hecho es la necesidad de dise\u00f1ar interfaces que se adapten a los diferentes tipos de pantalla (conocido como dise\u00f1o responsive). Esto tambi\u00e9n tiene una fuerte repercusi\u00f3n en las funcionalidades multimedia de las aplicaciones. Deberemos ser capaces de proporcionar audio y v\u00eddeo compatible y de calidad.</p>"},{"location":"formatos.html#audio-y-video-en-dispositivos-moviles","title":"Audio y v\u00eddeo en dispositivos m\u00f3viles","text":"<p>Existen diferentes formas de llevar audio y v\u00eddeo a los dispositivos m\u00f3viles:</p> <ul> <li>Streaming: El contenido multimedia se reproduce de forma remota. Se obtienen peque\u00f1os fragmentos del medio conforme se reproduce, pero nunca se llega a descargar un fichero entero.</li> <li>Descarga progresiva: Descargamos un fichero de audio o v\u00eddeo mediante HTTP. El contenido multimedia se puede reproducir mientras se descarga el fichero.</li> <li>Podcast: Contienen contenido epis\u00f3dico. Encontramos diferentes aplicaciones que nos permiten suscribirnos a podcasts, y que se encargar\u00e1n de descargar los \u00faltimos episodios de forma autom\u00e1tica cuando contemos con red WiFi. De esta forma podremos reproducirlos offline posteriormente.</li> <li>Incluido en la aplicaci\u00f3n: Podemos distribuir ficheros de audio y v\u00eddeo con nuestras aplicaciones. En este caso no har\u00e1 falta conectar a Internet para reproducir los medios, estar\u00e1n almacenados localmente.</li> </ul> <p>En esta lista podemos ver ordenadas las formas de suministrar el contenido multimedia desde la m\u00e1s remota a la m\u00e1s local. Independientemente de cu\u00e1l de estos mecanismos vayamos a utilizar, en primer lugar lo m\u00e1s importante ser\u00e1 establecer un formato adecuado para el audio y el v\u00eddeo.</p> <p>Copiar v\u00eddeos a la tarjeta SD</p> <p>En esta secci\u00f3n veremos la manera de a\u00f1adir archivos a la tarjeta de memoria de nuestro dispositivo virtual Android, de tal forma que podamos almacenar clips de v\u00eddeo. Se deben seguir los siguientes pasos:</p> <ul> <li>En primer lugar el emulador debe encontrarse en funcionamiento, y por supuesto, el dispositivo emulado debe hacer uso de una tarjeta SD.</li> <li>En Android Studio debemos abrir la ventana Android Device Monitor . Para ello hacemos uso de la opci\u00f3n Tools &gt; Android &gt; Android Device Monitor</li> </ul> <p></p> <ul> <li>A continuaci\u00f3n, en dicha ventana, seleccionamos la pesta\u00f1a File Explorer. El contenido de la tarjeta de memoria se halla (normalmente) en la carpeta <code>/mnt/sdcard</code> o <code>storage/sdcard</code>.</li> <li>Dentro de dicha carpeta deberemos introducir nuestros archivos de v\u00eddeo, dentro del directorio DCIM. Al hacer esto ya podr\u00e1n reproducirse desde la aplicaci\u00f3n nativa de reproducci\u00f3n de v\u00eddeo y tambi\u00e9n desde nuestras propias aplicaciones.  Podemos introducir un archivo de video con el rat\u00f3n, arrastrando un fichero desde otra carpeta al interior de la carpeta DCIM, aunque tambi\u00e9n podemos hacer uso de los controles que aparecen en la parte superior derecha de la perspectiva DDMS, cuando la pesta\u00f1a File Explorer est\u00e1 seleccionada. La funci\u00f3n de estos botones es, respectivamente: guardar en nuestra m\u00e1quina real alg\u00fan archivo de la tarjeta de memoria virtual, guardar en la tarjeta de memoria virtual un archivo, y eliminar el archivo seleccionado.</li> </ul> <p></p> <p>A veces es necesario volver a arrancar el terminal emulado para poder acceder a los v\u00eddeos insertados en la tarjeta de memoria desde la aplicaci\u00f3n Galer\u00eda de Android.</p>"},{"location":"formatos.html#formatos-soportados","title":"Formatos soportados","text":"<p>En primer lugar vamos a estudiar los formatos soportados por cada plataforma, sus caracter\u00edsticas, y cu\u00e1les son los m\u00e1s adecuados a utilizar en cada caso.</p>"},{"location":"formatos.html#formatos-de-audio-en-ios","title":"Formatos de audio en iOS","text":"<p>Entre los formatos de audio soportados en iOS encontramos formatos con un sistema de compresi\u00f3n complejo para el cual contamos con hardware espec\u00edfico que se encarga de realizar la descompresi\u00f3n, y de esta forma liberar la CPU de esta tarea. Estos formatos son:</p> <ul> <li>AAC (MPEG-4 Advanced Audio Coding)</li> <li>ALAC (Apple Lossless)</li> <li>HE-AAC (MPEG-4 High Efficiency AAC, sin descompresor software)</li> <li>MP3 (MPEG-1 audio layer 3)</li> </ul> <p>Con estos formatos podemos conseguir un alto nivel de compresi\u00f3n, y gracias al hardware de descompresi\u00f3n con el que est\u00e1 equipado el dispositivo pueden reproducirse de forma eficiente sin bloquear otras tareas. Sin embargo, dicho hardware s\u00f3lo puede soportar la reproducci\u00f3n simult\u00e1nea de un fichero de audio.</p> <p>Si queremos reproducir varios ficheros simult\u00e1neamente, el resto de ficheros deber\u00e1n ser descomprimidos por software, lo cu\u00e1l supone una gran carga para la CPU. Debemos evitar que esto ocurra. Por lo tanto, los formatos anteriores deber\u00e1n ser utilizados \u00fanicamente cuando no se vaya a reproducir m\u00e1s de un fichero de estos tipos simult\u00e1neamente.</p> <p>Por otro lado, contamos con soporte para formatos sin compresi\u00f3n, o con una compresi\u00f3n sencilla. Estos formatos son:</p> <ul> <li>Linear PCM (sin compresi\u00f3n)</li> <li>IMA4 (IMA/ADPCM)</li> <li>iLBC (internet Low Bitrate Codec, formato para transmisi\u00f3n del habla)</li> <li>\u00b5-law and a-law</li> </ul> <p>En estos casos no hay ning\u00fan problema en reproducir varios ficheros simult\u00e1neamente, ya que o no es necesarios descomprimirlos, como el caso de PCM, o su descompresi\u00f3n no supone apenas carga para la CPU, como el resto de casos.</p> <p>Si no tenemos problemas de espacio, el formato PCM ser\u00e1 el m\u00e1s adecuado, concretamente el tipo LEI16 (Little-Endian Integer 16-bit). En caso contrario, podemos utilizar AAC para la m\u00fasica de fondo (una \u00fanica instancia simult\u00e1nea, ya que se descodifica por hardware), e IMA4 para los efectos especiales, ya que nos permite reproducir varias instancias simult\u00e1neas con un bajo coste.</p> <p>Podemos utilizar tambi\u00e9n diferentes tipos de fichero para el audio, como <code>.wav</code>, <code>.mp3</code>, <code>.aac</code>, <code>.aiff</code> o <code>.caf</code>. El tipo de fichero preferido es este \u00faltimo (<code>.caf</code>, Core Audio File Format), ya que puede contener cualquier codificaci\u00f3n de audio de las vistas anteriormente.</p>"},{"location":"formatos.html#formatos-de-video-en-ios","title":"Formatos de v\u00eddeo en iOS","text":"<p>Los formatos de video soportados son todos aquellos ficheros con extension <code>mov</code>, <code>mp4</code>, <code>m4v</code>, y <code>3gp</code> que cumplan las siguientes restricciones de codificaci\u00f3n:</p> <ul> <li>H.264, hasta 1.5 Mbps, 640 x 480, 30 fps, versi\u00f3n de baja complejidad del H.264 Baseline Profile con audio AAC-LC de hasta 160 Kbps, 48 kHz, stereo</li> <li>H.264, hasta 768 Kbps, 320 x 240, 30 fps, Baseline Profile hasta nivel 1.3 con audio AAC-LC de hasta 160 Kbps, 48 kHz, stereo</li> <li>MPEG-4, hasta 2.5 Mbps, 640 x 480, 30 frames per second, Simple Profile con audio AAC-LC de hasta 160 Kbps, 48 kHz, stereo</li> </ul> <p>Estos formatos est\u00e1n soportados por todos los dispositivos iOS. A partir de iPhone 4 y iPad se soportan adem\u00e1s:</p> <ul> <li>H.264 hasta 720p, 30 fps, Main Profile nivel 3.1. Audio AAC-LC de hasta 160 Kbps, 48kHz, stereo.</li> </ul> <p>A partir de iPhone 5 se a\u00f1ade:</p> <ul> <li>H.264 hasta 1080p, 30 fps, High Profile nivel 4.1. Audio AAC-LC de hasta 160 Kbps, 48kHz, stereo.</li> </ul> <p>Se dispone de m\u00e1s informaci\u00f3n en la documentaci\u00f3n sobre iOS Media Layer:</p> <p>https://developer.apple.com/library/ios/documentation/Miscellaneous/Conceptual/iPhoneOSTechOverview/MediaLayer/MediaLayer.html</p>"},{"location":"formatos.html#formatos-de-audio-y-video-en-android","title":"Formatos de audio y v\u00eddeo en Android","text":"<p>Android incorpora la posibilidad de reproducir tanto audio como v\u00eddeo en diversos formatos. Los formatos de audio soportados son los siguientes:</p> <ul> <li>AAC LC/LT</li> <li>HE-AACv1 (AAC+)</li> <li>HE-AACv2 (Enhanced ACC+)</li> <li>AAC ELD</li> <li>AMR-NB</li> <li>AMR-WB</li> <li>FLAC</li> <li>MP3</li> <li>MIDI</li> <li>Ogg Vorbis</li> <li>PCM/Wave</li> </ul> <p>Con respecto al v\u00eddeo, los formatos soportados son:</p> <ul> <li>H.263</li> <li>H.264 AVC</li> <li>MPEG-4 SP</li> <li>VP8</li> </ul> <p>Para m\u00e1s informaci\u00f3n sobre los formatos de audio y v\u00eddeo soportados en Android se puede consultar la siguiente p\u00e1gina:</p> <p>https://developer.android.com/guide/appendix/media-formats.html</p>"},{"location":"formatos.html#perfiles-de-audio-y-video","title":"Perfiles de audio y v\u00eddeo","text":"<p>A partir de la informaci\u00f3n anterior es dif\u00edcil establecer un perfil general de v\u00eddeo que est\u00e9 soportado por todos los dispositivos y al mismo tiempo aproveche sus caracter\u00edsticas, pero si que podemos obtener una serie de perfiles orientados a determinadas familia de dispositivos.</p> <p>Podemos observar que la codificaci\u00f3n de v\u00eddeo y audio que mayor compatibilidad y calidad nos proporciona es H.264 y AAC respectivamente. Por lo tanto, basaremos los perfiles en estos codecs.</p>"},{"location":"formatos.html#perfil-general","title":"Perfil general","text":"<p>Si queremos tener v\u00eddeo compatible con la mayor\u00eda de los dispositivos actuales podemos utilizar un perfil como el siguiente:</p> <ul> <li>V\u00eddeo H.264 Baseline profile  Resoluci\u00f3n de 480 x 320  Framerate de 30 fps ** _Bitrate de 1500 kbps</li> <li>Audio AAC  Sample rate de 44100 kHz  Bitrate de 128 kbps</li> </ul>"},{"location":"formatos.html#perfil-de-alta-calidad","title":"Perfil de alta calidad","text":"<p>El perfil anterior no aprovechar\u00e1 las caracter\u00edsticas de aquellos dispositivos que soporten v\u00eddeo HD. Por ello es conveniente definir otro perfil para los dispositivos m\u00e1s avanzados:</p> <ul> <li>V\u00eddeo H.264 Baseline profile  Resoluci\u00f3n de 1280 x 720  Framerate de 30 fps ** _Bitrate de 5000 kbps</li> <li>Audio AAC  Sample rate de 48000 kHz  Bitrate de 160 kbps</li> </ul>"},{"location":"formatos.html#perfil-para-dispositivos-legacy","title":"Perfil para dispositivos legacy","text":"<p>Normalmente con los dos perfiles anteriores deber\u00eda ser suficiente, pero en algunos casos puede interesarnos tambi\u00e9n dirigirnos a dispositivos legacy (no smartphones). En este caso podemos crear un perfil limitado como el siguiente:</p> <ul> <li>V\u00eddeo 3GP, codificaci\u00f3n MPEG-4 (parte 2)  Resoluci\u00f3n entre 176 x 144 y 320 x 240  Framerate entre 5 y 15 fps ** _Bitrate entre 52 y 192 kbps</li> <li>Audio AAC-LC (Low Complexity)  Sample rate de 16000 kHz  Bitrate entre 16 y 24 kbps</li> </ul>"},{"location":"formatos.html#conversion-de-formato","title":"Conversi\u00f3n de formato","text":"<p>Vamos a ver ahora c\u00f3mo convertir el audio y v\u00eddeo al formato deseado.</p>"},{"location":"formatos.html#conversion-de-audio-con-afconvert","title":"Conversi\u00f3n de audio con <code>afconvert</code>","text":"<p>En MacOS contamos con la herramienta <code>afconvert</code>, que nos permite convertir el audio a los diferentes formatos soportados por la plataforma. Se trata de una herramienta en l\u00ednea de comando que se utiliza de la siguiente forma:</p> <pre><code>afconvert -d [out data format] -f [out file format] [in file] [out file]\n</code></pre> <p>Por ejemplo, en caso de querer convertir el audio al formato preferido (PCM LEI16 en un fichero <code>.caf</code>), utilizaremos el siguiente comando:</p> <pre><code>afconvert -f caff -d LEI16 sonido.wav sonido.caf\n</code></pre> <p>Tambi\u00e9n podemos utilizar esta herramienta para convertir a formatos con compresi\u00f3n. En tal caso, deberemos especificar el bit-rate del fichero resultante:</p> <pre><code>afconvert -f caff -d aac -b 131072 musica.caf musica.caf\n</code></pre> <p>Tambi\u00e9n contamos con herramientas para reproducir audio en l\u00ednea de comando, y para obtener informaci\u00f3n sobre un fichero de audio. Estas herramientas son <code>afplay</code> y <code>afinfo</code>.</p>"},{"location":"formatos.html#conversion-con-vlc","title":"Conversi\u00f3n con VLC","text":"<p>Podemos utilizar VLC para convertir audio y v\u00eddeo tanto mediante un asistente de la aplicaci\u00f3n como mediante l\u00ednea de comando. En primer lugar mostramos diferentes codecs de audio y v\u00eddeo y formatos de encapsulamiento que podemos utilizar:</p> <p>Codecs de video:</p> <code>vcodec</code> Codec de v\u00eddeo <code>mp1v</code> MPEG-1 <code>mp2v</code> MPEG-2 <code>mp4v</code> MPEG-4 <code>h263</code> H.263 <code>h264</code> H.264 <code>theo</code> Theora <code>VP8</code> VP8 Video <p>Codecs de audio:</p> <code>acodec</code> Codec de audio <code>mpga</code> MPEG Audio <code>mp3</code> MPEG Layer 3 <code>mp4a</code> MP4 Audio <code>vorb</code> Vorbis <p>Formatos de fichero (encapsulamiento):</p> <code>mux</code> Formato de fichero <code>mpg1</code> MPEG-1 <code>ts</code> MPEG-TS <code>ps</code> MPEG-PS <code>mp4</code> MPEG-4 <code>avi</code> AVI <code>ogg</code> OGG <code>dummy</code> MP3 <code>wav</code> WAVE <p>Podemos realizar la conversi\u00f3n en l\u00ednea de comando de la siguiente forma:</p> <pre><code>vlc -I dummy /&lt;ruta_origen&gt;/video.mpg :sout='#transcode{vcodec=mp2v, \nvb=4096,acodec=mp2a,ab=192,scale=1,channels=2,deinterlace,audio-\nsync}:std{access=file,mux=ps,dst=/&lt;ruta_destino&gt;/video.mpg' vlc://quit\n</code></pre> <p>Donde <code>ab</code> indica el bitrate de audio, y <code>vb</code> el bitrate de v\u00eddeo.</p> <p>Tambi\u00e9n podemos hacer la conversi\u00f3n desde la aplicaci\u00f3n VLC, con la opci\u00f3n Archivo &gt; Convertir / Emitir ....</p>"},{"location":"formatos.html#conversion-con-ffmpeg","title":"Conversi\u00f3n con <code>ffmpeg</code>","text":"<p>Una de las aplicaciones que mayor flexibilidad nos da para convertir audio y v\u00eddeo es <code>ffmpeg</code>. La forma m\u00e1s b\u00e1sica de lanzar esta herramienta es como se muestra a continuaci\u00f3n:</p> <pre><code>ffmpeg -i entrada.ext salida.ext\n</code></pre> <p>En este caso la codificaci\u00f3n de audio y v\u00eddeo se induce a partir de la extensi\u00f3n especificada en el nombre del fichero de salida.</p> <p>Podemos especificar con precisi\u00f3n el formato del fichero de salida con los siguientes par\u00e1metros:</p> Caracter\u00edstica Par\u00e1metro Formato de fichero <code>-f avi</code> Resoluci\u00f3n <code>-s 640x480</code> Fotogramas por segundo <code>-r 24</code> Codec de audio <code>-c:a / -acodec aac</code> Codec de v\u00eddeo <code>-c:v / vcodec h264</code> Bitrate de audio <code>-b:a 128k</code> Bitrate de v\u00eddeo <code>-b:v 1500k</code> Frecuencia de audio <code>-ar 44100</code> Canales de audio <code>-ac 2</code> Perfil <code>-profile:v baseline</code> <p>Podemos ver todos los formatos de fichero disponibles con:</p> <pre><code>ffmpeg -formats\n</code></pre> <p>Tambi\u00e9n podemos ver los c\u00f3decs disponibles con:</p> <pre><code>ffmpeg -codecs\n</code></pre> <p>En caso de utilizar v\u00eddeo H.264 podemos especificar tambi\u00e9n el perfil y su nivel:</p> Perfil Par\u00e1metros Baseline 3.0 <code>-profile:v baseline -level 3.0</code> Baseline 3.1 <code>-profile:v baseline -level 3.1</code> Main 3.1 <code>-profile:v main -level 3.1</code> Main 4.0 <code>-profile:v main -level 4.0</code> High 4.0 <code>-profile:v high -level 4.0</code> High 4.1 <code>-profile:v high -level 4.1</code> <p>El siguiente ejemplo convierte un v\u00eddeo MPEG-2 a formato H.264/AAC con perfil baseline 3.0:</p> <pre><code>ffmpeg -i entrada.m2ts\n       -strict -2\n       -vcodec h264\n       -s 480x320\n       -b:v 1500k\n       -profile:v baseline -level 3.0\n       -acodec aac\n       -ar 22050\n-b:a 128k\n       -ac 2\nsalida.mp4\n</code></pre>"},{"location":"formatos.html#ejercicios","title":"Ejercicios","text":""},{"location":"formatos.html#conversion-de-videos","title":"Conversi\u00f3n de v\u00eddeos","text":"<p>En los recursos de la sesi\u00f3n encontramos una serie de v\u00eddeos que podremos utilizar en nuestras aplicaciones. Todos estos v\u00eddeos se encuentran en 1080p, por lo que para poder reproducirlos correctamente en cualquier m\u00f3vil antes deberemos convertirlos a un formato adecuado.</p> <p>Utiliza un perfil est\u00e1ndar que nos permita suministrar estos v\u00eddeos a la gran mayor\u00eda de dispositivos utilizados en la actualidad. Prueba a reproducir estos v\u00eddeos desde el dispositivo Android introduci\u00e9ndolos en la tarjeta SD.</p> <p>Ayuda: Puedes copiar los v\u00eddeos a la tarjeta SD desde Android Device Monitor. Dentro de esta ventana, selecciona la pesta\u00f1a File Explore en la ventana central y ah\u00ed podr\u00e1s subir o descargar ficheros del dispositivo.</p> <p>Por ejemplo, puedes copiar el video en la carpeta <code>/mnt/sdcard</code> (o similar). En tal caso, para abrirlo puedes ir al navegador del dispositivo y poner como URL:</p> <pre><code>file:///mnt/sdcard/video.mp4\n</code></pre> <p>ATENCI\u00d3N: El acceso a las URLs de tipo <code>file:</code> est\u00e1 soportado a partir de Android 2.1.</p>"},{"location":"graficos-android.html","title":"Gr\u00e1ficos de alto rendimiento en Android","text":"<p>El principal objetivo de la sesi\u00f3n consiste en hacer una introducci\u00f3n a los componentes de la interfaz de Android que nos dar\u00e1n soporte para poder reproducir gr\u00e1ficos de alto rendimiento (v\u00eddeo o gr\u00e1ficos 3D).</p>"},{"location":"graficos-android.html#surface-view","title":"Surface View","text":"<p>Para mostrar gr\u00e1ficos propios podr\u00edamos usar un componente que herede de <code>View</code>. Estos componentes funcionan bien si no necesitamos realizar repintados continuos o mostrar gr\u00e1ficos 3D.</p> <p>Sin embargo, en el caso de tener una aplicaci\u00f3n con una gran carga gr\u00e1fica, como puede ser un videojuego, un reproductor de v\u00eddeo, o una aplicaci\u00f3n que muestre gr\u00e1ficos 3D, en lugar de <code>View</code> deberemos utilizar <code>SurfaceView</code>. Esta \u00faltima clase nos proporciona una superficie en la que podemos dibujar desde un hilo en segundo plano, lo cual libera al hilo principal de la aplicaci\u00f3n de la carga gr\u00e1fica.</p> <p>Vamos a ver en primer lugar c\u00f3mo crear subclases de <code>SurfaceView</code>, y las diferencias existentes con <code>View</code>.</p> <p>Para crear una vista con <code>SurfaceView</code> tendremos que crear una nueva subclase de dicha clase (en lugar de <code>View</code>). Pero en este caso no bastar\u00e1 con definir el m\u00e9todo <code>onDraw</code>, ahora deberemos crearnos un hilo independiente y proporcionarle la superficie en la que dibujar (<code>SurfaceHolder</code>). Adem\u00e1s, en nuestra subclase de <code>SurfaceView</code> tambi\u00e9n implementaremos la interfaz <code>SurfaceHolder.Callback</code> que nos permitir\u00e1 estar al tanto de cuando la superficie se crea, cambia, o se destruye.</p> <p>Cuando la superficie sea creada pondremos en marcha nuestro hilo de dibujado, y lo pararemos cuando la superficie sea destruida. A continuaci\u00f3n mostramos un ejemplo de dicha clase:</p> <pre><code>public class VistaSurface extends SurfaceView\nimplements SurfaceHolder.Callback {\nHiloDibujo hilo = null;\n\npublic VistaSurface(Context context) {\nsuper(context);\n\nSurfaceHolder holder = this.getHolder();\nholder.addCallback(this);\n}\n\npublic void surfaceChanged(SurfaceHolder holder, int format,\nint width, int height) {\n// La superficie ha cambiado (formato o dimensiones)\n}\n\npublic void surfaceCreated(SurfaceHolder holder) {\nhilo = new HiloDibujo(holder, this);\nhilo.start();\n}\n\npublic void surfaceDestroyed(SurfaceHolder holder) {\n\nhilo.detener();\ntry {\nhilo.join();\n} catch (InterruptedException e) { }\n}\n}\n</code></pre> <p>Como vemos, la clase <code>SurfaceView</code> simplemente se encarga de obtener la superficie y poner en marcha o parar el hilo de dibujado. En este caso la acci\u00f3n estar\u00e1 realmente en el hilo, que es donde especificaremos la forma en la que se debe dibujar el componente. Vamos a ver a continuaci\u00f3n c\u00f3mo podr\u00edamos implementar dicho hilo:</p> <pre><code>class HiloDibujo extends Thread {\nSurfaceHolder holder;\nVistaSurface vista;\nboolean continuar = true;\n\npublic HiloDibujo(SurfaceHolder holder, VistaSurface vista) {\nthis.holder = holder;\nthis.vista = vista;\ncontinuar = true;\n}\n\npublic void detener() {\ncontinuar = false;\n}\n\n@Override\npublic void run() {\nwhile (continuar) {\nCanvas c = null;\ntry {\nc = holder.lockCanvas(null);\nsynchronized (holder) {\n// Dibujar aqui los graficos\nc.drawColor(Color.BLUE);\n}\n} finally {\nif (c != null) {\nholder.unlockCanvasAndPost(c);\n}\n}\n}\n}\n}\n</code></pre> <p>Podemos ver que en el bucle principal de nuestro hilo obtenermos el lienzo (<code>Canvas</code>) a partir de la superficie (<code>SurfaceHolder</code>) mediante el m\u00e9todo <code>lockCanvas</code>. Esto deja el lienzo bloqueado para nuestro uso, por ese motivo es importante asegurarnos de que siempre se desbloquee. Para tal fin hemos puesto <code>unlockCanvasAndPost</code> dentro del bloque <code>finally</code>. Adem\u00e1s debemos siempre dibujar de forma sincronizada con el objeto <code>SurfaceHolder</code>, para as\u00ed evitar problemas de concurrencia en el acceso a su lienzo.</p> <p>Para aplicaciones como videojuegos 2D sencillos un c\u00f3digo como el anterior puede ser suficiente (la clase <code>View</code> ser\u00eda demasiado lenta para un videojuego). Sin embargo, lo realmente interesante es utilizar <code>SurfaceView</code> junto a OpenGL, para as\u00ed poder mostrar gr\u00e1ficos 3D, o escalados, rotaciones y otras transformaciones sobre superficies 2D de forma eficiente.</p> <p>A continuaci\u00f3n veremos un ejemplo de c\u00f3mo utilizar OpenGL (concretamente OpenGL ES) vinculado a nuestra <code>SurfaceView</code>.</p> <p>Realmente la implementaci\u00f3n de nuestra clase que hereda de <code>SurfaceView</code> no cambiar\u00e1, simplemente modificaremos nuestro hilo, que es quien realmente realiza el dibujado. Toda la inicializaci\u00f3n de OpenGL deber\u00e1 realizarse dentro de nuestro hilo (en el m\u00e9todo <code>run</code>), ya que s\u00f3lo se puede acceder a las operaciones de dicha librer\u00eda desde el mismo hilo en el que se inicializ\u00f3. En caso de que intent\u00e1semos acceder desde otro hilo obtendr\u00edamos un error indicando que no existe ning\u00fan contexto activo de OpenGL.</p> <p>En este caso nuestro hilo podr\u00eda contener el siguiente c\u00f3digo:</p> <pre><code>public void run() {\ninitEGL();\ninitGL();\n\nTriangulo3D triangulo = new Triangulo3D();\nfloat angulo = 0.0f;\n\nwhile(continuar) {\ngl.glClear(GL10.GL_COLOR_BUFFER_BIT |\nGL10.GL_DEPTH_BUFFER_BIT);\n\n// Dibujar gr\u00e1ficos aqu\u00ed\ngl.glMatrixMode(GL10.GL_MODELVIEW);\ngl.glLoadIdentity();\ngl.glTranslatef(0, 0, -5.0f);\ngl.glRotatef(angulo, 0, 1, 0);\n\ngl.glEnableClientState(GL10.GL_VERTEX_ARRAY);\ntriangulo.dibujar(gl);\n\negl.eglSwapBuffers(display, surface);\nangulo += 1.0f;\n}\n}\n</code></pre> <p>En primer lugar debemos inicializar la interfaz EGL, que hace de v\u00ednculo entre la plataforma nativa y la librer\u00eda OpenGL:</p> <pre><code>EGL10 egl;\nGL10 gl;\nEGLDisplay display;\nEGLSurface surface;\nEGLContext contexto;\nEGLConfig config;\n\nprivate void initEGL() {\negl = (EGL10)EGLContext.getEGL();\ndisplay = egl.eglGetDisplay(EGL10.EGL_DEFAULT_DISPLAY);\n\nint [] version = new int[2];\negl.eglInitialize(display, version);\n\nint [] atributos = new int[] {\nEGL10.EGL_RED_SIZE, 5,\nEGL10.EGL_GREEN_SIZE, 6,\nEGL10.EGL_BLUE_SIZE, 5,\nEGL10.EGL_DEPTH_SIZE, 16,\nEGL10.EGL_NONE\n};\n\nEGLConfig [] configs = new EGLConfig[1];\nint [] numConfigs = new int[1];\negl.eglChooseConfig(display, atributos, configs,\n1, numConfigs);\n\nconfig = configs[0];\nsurface = egl.eglCreateWindowSurface(display,\nconfig, holder, null);\ncontexto = egl.eglCreateContext(display, config,\nEGL10.EGL_NO_CONTEXT, null);\negl.eglMakeCurrent(display, surface, surface, contexto);\n\ngl = (GL10)contexto.getGL();\n}\n</code></pre> <p>A continuaci\u00f3n debemos proceder a la inicializaci\u00f3n de la interfaz de la librer\u00eda OpenGL:</p> <pre><code>private void initGL() {\nint width = vista.getWidth();\nint height = vista.getHeight();\ngl.glViewport(0, 0, width, height);\ngl.glMatrixMode(GL10.GL_PROJECTION);\ngl.glLoadIdentity();\n\nfloat aspecto = (float)width/height;\nGLU.gluPerspective(gl, 45.0f, aspecto, 1.0f, 30.0f);\ngl.glClearColor(0.5f, 0.5f, 0.5f, 1);\n}\n</code></pre> <p>Una vez hecho esto, ya s\u00f3lo nos queda ver c\u00f3mo dibujar una malla 3D. Vamos a ver como ejemplo el dibujo de un tri\u00e1ngulo:</p> <pre><code>public class Triangulo3D {\n\nFloatBuffer buffer;\n\nfloat[] vertices = {\n-1f, -1f, 0f,\n1f, -1f, 0f,\n0f, 1f, 0f };\n\n\npublic Triangulo3D() {\nByteBuffer bufferTemporal = ByteBuffer\n.allocateDirect(vertices.length*4);\nbufferTemporal.order(ByteOrder.nativeOrder());\nbuffer = bufferTemporal.asFloatBuffer();\nbuffer.put(vertices);\nbuffer.position(0);\n}\n\npublic void dibujar(GL10 gl) {\ngl.glVertexPointer(3, GL10.GL_FLOAT, 0, buffer);\ngl.glDrawArrays(GL10.GL_TRIANGLES, 0, 3);\n}\n}\n</code></pre> <p>Para finalizar, es importante que cuando la superficie se destruya se haga una limpieza de los recursos utilizados por OpenGL:</p> <pre><code>private void cleanupGL() {\negl.eglMakeCurrent(display, EGL10.EGL_NO_SURFACE,\nEGL10.EGL_NO_SURFACE, EGL10.EGL_NO_CONTEXT);\negl.eglDestroySurface(display, surface);\negl.eglDestroyContext(display, contexto);\negl.eglTerminate(display);\n}\n</code></pre> <p>Podemos llamar a este m\u00e9todo cuando el hilo se detenga (debemos asegurarnos que se haya detenido llamando a <code>join</code> previamente).</p> <p>A partir de Android 1.5 se incluye la clase <code>GLSurfaceView</code>, que ya incluye la inicializaci\u00f3n del contexto GL y nos evita tener que hacer esto manualmente. Esto simplificar\u00e1 bastante el uso de la librer\u00eda. Vamos a ver a continuaci\u00f3n un ejemplo de c\u00f3mo trabajar con dicha clase.</p> <p>En este caso ya no ser\u00e1 necesario crear una subclase de  <code>GLSurfaceView</code>, ya que la inicializaci\u00f3n y gesti\u00f3n del hilo de OpenGL siempre es igual. Lo \u00fanico que nos interesar\u00e1 cambiar es lo que se muestra en la escena. Para ello deberemos crear una subclase de <code>GLSurfaceViewRenderer</code> que nos obliga a definir los siguientes m\u00e9todos:</p> <pre><code>public class MiRenderer implements GLSurfaceView.Renderer {\n\nTriangulo3D triangulo;\nfloat angulo;\n\npublic MiRenderer() {\ntriangulo = new Triangulo3D();\nangulo = 0;\n}\n\npublic void onSurfaceCreated(GL10 gl, EGLConfig config) {\n}\n\npublic void onSurfaceChanged(GL10 gl, int w, int h) {\n// Al cambiar el tama\u00f1o cambia la proyecci\u00f3n\nfloat aspecto = (float)w/h;\ngl.glViewport(0, 0, w, h);\n\ngl.glMatrixMode(GL10.GL_PROJECTION);\ngl.glLoadIdentity();\nGLU.gluPerspective(gl, 45.0f, aspecto, 1.0f, 30.0f);\n}\n\npublic void onDrawFrame(GL10 gl) {\ngl.glClearColor(0.5f, 0.5f, 0.5f, 1.0f);\ngl.glClear(GL10.GL_COLOR_BUFFER_BIT |\nGL10.GL_DEPTH_BUFFER_BIT);\n\n// Dibujar gr\u00e1ficos aqu\u00ed\ngl.glMatrixMode(GL10.GL_MODELVIEW);\ngl.glLoadIdentity();\ngl.glTranslatef(0, 0, -5.0f);\ngl.glRotatef(angulo, 0, 1, 0);\n\ngl.glEnableClientState(GL10.GL_VERTEX_ARRAY);\ntriangulo.dibujar(gl);\n\nangulo += 1.0f;\n}\n}\n</code></pre> <p>Podemos observar que ser\u00e1 el m\u00e9todo <code>onDrawFrame</code> en el que deberemos escribir el c\u00f3digo para mostrar los gr\u00e1ficos. Con hacer esto ser\u00e1 suficiente, y no tendremos que encargarnos de crear el hilo ni de inicializar ni destruir el contexto.</p> <p>Para mostrar estos gr\u00e1ficos en la vista deberemos proporcionar nuestro renderer al objeto <code>GLSurfaceView</code>:</p> <pre><code>vista = new GLSurfaceView(this);\nvista.setRenderer(new MiRenderer());\nsetContentView(vista);\n</code></pre> <p>Por \u00faltimo, ser\u00e1 importante transmitir los eventos <code>onPause</code> y <code>onResume</code> de nuestra actividad a la vista de OpenGL, para as\u00ed liberar a la aplicaci\u00f3n de la carga gr\u00e1fica cuando permanezca en segundo plano. El c\u00f3digo completo de la actividad quedar\u00eda como se muestra a continuaci\u00f3n:</p> <pre><code>public class MiActividad extends Activity {\nGLSurfaceView vista;\n\n@Override\nprotected void onCreate(Bundle savedInstanceState) {\nsuper.onCreate(savedInstanceState);\nvista = new GLSurfaceView(this);\nvista.setRenderer(new MiRenderer());\nsetContentView(vista);\n}\n\n@Override\nprotected void onPause() {\nsuper.onPause();\nvista.onPause();\n}\n\n@Override\nprotected void onResume() {\nsuper.onResume();\nvista.onResume();\n}\n}\n</code></pre>"},{"location":"graficos-android.html#ejercicios","title":"Ejercicios","text":"<p>Antes de empezar a crear los proyectos, debes descargarte las plantillas desde el repositorio <code>mastermoviles-multimedia-android</code> de bitbucket. En este repositorio se encuentran todas las plantillas que utilizaremos en los ejercicios Android de este m\u00f3dulo.</p>"},{"location":"graficos-android.html#graficos-3d","title":"Gr\u00e1ficos 3D","text":"<p>En las plantillas de la sesi\u00f3n tenemos una aplicaci\u00f3n <code>Graficos</code> en la que podemos ver un ejemplo completo de c\u00f3mo utilizar <code>SurfaceView</code> tanto para gr\u00e1ficos 2D con el <code>Canvas</code> como para gr\u00e1ficos 3D con OpenGL, y tambi\u00e9n de c\u00f3mo utilizar <code>GLSurfaceView</code>.</p> <ul> <li> <p>Si ejecutamos la aplicaci\u00f3n veremos un tri\u00e1ngulo rotando alrededor del eje Y. Observar el c\u00f3digo fuente, y modificarlo para que el tri\u00e1ngulo rote alrededor del eje X, en lugar de Y.</p> </li> <li> <p>Tambi\u00e9n podemos ver que hemos creado, adem\u00e1s de la clase <code>Triangulo3D</code>, la clase <code>Cubo3D</code>. Modificar el c\u00f3digo para que en lugar de mostrar el tri\u00e1ngulo se muestre el cubo.</p> </li> </ul>"},{"location":"graficos-ios.html","title":"Gr\u00e1ficos y animaciones en iOS","text":"<p>Hasta este momento hemos visto c\u00f3mo crear la interfaz de usuario de nuestra aplicaci\u00f3n utilizando una serie de componentes predefinidos de la plataforma iOS, con la posibilidad de personalizarlos. Vamos a ver ahora c\u00f3mo crear nuestros propios tipos de vistas y dibujar en ellas. Veremos tambi\u00e9n c\u00f3mo animar estas vistas para as\u00ed crear una interfaz m\u00e1s din\u00e1mica y atractiva para el usuario.</p> <p>En primer lugar repasaremos las diferentes APIs que encontramos en la plataforma para crear gr\u00e1ficos y animaciones, y las situaciones en las que conviene utilizar cada una de ellas. Tras esto, pasaremos a estudiar la forma de utilizar estas APIs para implementar funcionalidades que necesitaremos habitualmente en las aplicaciones m\u00f3viles.</p>"},{"location":"graficos-ios.html#apis-para-graficos-y-animacion","title":"APIs para gr\u00e1ficos y animaci\u00f3n","text":"<p>En iOS encontramos dos formas principales de crear aplicaciones con gr\u00e1ficos y animaci\u00f3n:</p> <ul> <li>OpenGL ES: Se trata de una API multiplataforma para gr\u00e1ficos 2D y 3D. Es una versi\u00f3n reducida de la API OpenGL, dise\u00f1ada para dispositivos limitados. Encontramos implementaciones de esta API en las diferentes plataformas m\u00f3viles: iOS, Android, e incluso en algunos dispositivos Java ME.</li> <li>APIs nativas: Se trata de APIs propias de las plataforma iOS y MacOS para mostrar gr\u00e1ficos 2D y animaciones. Estas APIs son Quartz 2D y Core Animation (CA). Muchas veces encontraremos referenciada la API Quartz 2D como Core Graphics (GC), ya que son pr\u00e1cticamente equivalentes. Realmente, Quartz 2D es una parte de Core Graphics. Core Graphics se compone de Quartz 2D, que es la API para dibujar gr\u00e1ficos 2D, y Quartz Compositor, que es el motor utilizado para componer en pantalla el contenido generado por las diferentes APIs gr\u00e1ficas.</li> </ul> <p>OpenGL ES resultar\u00e1 apropiada cuando tengamos aplicaciones con una fuerte carga gr\u00e1fica, gr\u00e1ficos 3D, y/o la necesidad de conseguir elevadas tasas de fotogramas. Este es claramente el caso de los videojuegos.</p> <p>Sin embargo, si lo que queremos es tener una aplicaci\u00f3n con una interfaz vistosa y din\u00e1mica, en la que podamos introducir tanto componentes nativos como componentes propios (como podr\u00eda ser por ejemplo una gr\u00e1fica), la opci\u00f3n m\u00e1s adecuada ser\u00e1 utilizar Quartz 2D y Core Animation. En esta sesi\u00f3n vamos a centrarnos en esta segunda opci\u00f3n. En pr\u00f3ximas asignaturas abordaremos la programaci\u00f3n de videojuegos.</p>"},{"location":"graficos-ios.html#graficos-con-quartz-2d","title":"Gr\u00e1ficos con Quartz 2D","text":"<p>Las funciones y estructuras de la librer\u00eda Quartz 2D (Core Graphics) tienen todas el prefijo <code>CG</code>. Hay que resaltar que en esta API no encontramos objetos Objective-C, sino que los datos que manejamos son registros de C (<code>struct</code>) y los manipularemos con una serie de funciones y macros de la librer\u00eda.</p> <p>En algunas ocasiones podremos tambi\u00e9n utilizar los objetos de UIKit para dibujar contenido, lo cual resultar\u00e1 notablemente m\u00e1s sencillo que utilizar los elementos de Core Graphics a bajo nivel. En muchos casos encontramos objetos de Core Graphics relacionados con objetos de UIKit (<code>CGImage</code> y <code>UIImage</code>, o <code>CGColor</code> y <code>UIColor</code>).</p>"},{"location":"graficos-ios.html#contexto-grafico","title":"Contexto gr\u00e1fico","text":"<p>Al igual que ocurr\u00eda en el caso de Java y Android, Core Graphics nos ofrece una API con la que dibujar en un contexto gr\u00e1fico. Este contexto podr\u00e1 ser una regi\u00f3n de la pantalla, pero tambi\u00e9n podr\u00eda ser una imagen en segundo plano, o un PDF por ejemplo. Esto nos permitir\u00e1 reutilizar nuestro c\u00f3digo de dibujado para generar gr\u00e1ficos en diferentes contextos.</p> <p>El tipo de contexto gr\u00e1fico que se utiliza m\u00e1s habitualmente es el contexto referente a una vista <code>UIView</code>, que nos permite dibujar en su \u00e1rea rectangular en pantalla. Esto lo haremos en el m\u00e9todo <code>drawRect:</code> de la vista. Para ello deberemos crear una subclase de <code>UIView</code> y redefinir dicho m\u00e9todo:</p> <pre><code>- (void)drawRect:(CGRect)rect {\nCGContextRef context = UIGraphicsGetCurrentContext();\n\n// Aqu\u00ed ya podemos dibujar en el contexto\n...\n}\n</code></pre> <p>Este m\u00e9todo ser\u00e1 invocado por el sistema cuando necesite pintar el contenido de la vista. Cuando eso ocurra, el contexto gr\u00e1fico actual estar\u00e1 establecido al contexto para dibujar en la vista. Podremos obtener una referencia al contexto actualmente activo con la funci\u00f3n de UIKit <code>UIGraphicsGetCurrentContext</code>. Podemos observar que el contexto gr\u00e1fico es de tipo <code>CGContextRef</code>, y que no se trata de un objeto, sino de una estructura, como todos los elementos de Core Graphics.</p> <p>Si llamamos manualmente al m\u00e9todo anterior con otro contexto activo, dibujar\u00e1 el contenido en dicho contexto. M\u00e1s adelante veremos c\u00f3mo establecer otros contextos gr\u00e1ficos de forma manual.</p> <p>Una vez tenemos el contexto, podemos establecer en \u00e9l una serie de atributos para especificar la forma en la que se va a dibujar. Estos atributos son por ejemplo el \u00e1rea de recorte, el color del relleno o del trazo, el grosor y tipo de la l\u00ednea, antialiasing, el tipo de fuente, o la transformaci\u00f3n a aplicar.</p> <p>Con estos atributos establecidos, podremos dibujar diferentes elementos en el contexto, como primitivas geom\u00e9tricas, texto, e im\u00e1genes. Esta forma de trabajar es la misma que se utiliza en Java y Android para dibujar el contenido de las vistas.</p> <p>Si queremos que se actualice en contenido de nuestra vista, por ejemplo en el caso de que hayamos cambiado algunas propiedades de los gr\u00e1ficos a mostrar, en algunos casos deberemos llamar al m\u00e9todo <code>setNeedsDisplay</code> del objeto <code>UIView</code>:</p> <pre><code>[self setNeedsDisplay];\n</code></pre> <p>Esto har\u00e1 que se invalide el contenido actual de la vista, y se tenga que volver a llamar otra vez a <code>drawRect:</code> para repintarla.</p>"},{"location":"graficos-ios.html#sistema-de-coordenadas","title":"Sistema de coordenadas","text":"<p>El sistema de coordenadas de Core Graphics puede resultar en ocasiones algo confuso, debido a que el eje y aparece invertido respecto a UIKit.</p> <p></p> <p>Si bien el sistema de coordenadas de UIKit con el que dibujamos las vistas tiene su origen en la esquina superior izquierda, siendo las y positivas hacia abajo, en Core Graphics el origen est\u00e1 en la esquina inferior izquierda y las y son positivas hacia arriba.</p> <p>Adem\u00e1s, el sistema de coordenadas est\u00e1 basado en puntos, no en p\u00edxeles. Es decir, dibujamos en un sistema de coordenadas l\u00f3gico, que es independiente del sistema de coordenadas f\u00edsico (en pixels) del dispositivo. De esta forma podremos dibujar de forma independiente a la resoluci\u00f3n real de la pantalla. Es decir, por ejemplo podremos utilizar el mismo c\u00f3digo y las mismas coordenadas para dibujar en la pantalla de un iPhone 3G, en un iPhone 4 con pantalla retina, y en un iPhone 6 plus, a pesar de que el segundo tiene el doble de resoluci\u00f3n, y el tercero el triple. El contenido dibujado se ver\u00e1 con las mismas dimensiones, con la \u00fanica diferencia de que en las pantallas con mayor resoluci\u00f3n aparecer\u00e1 con mayor nitidez.</p> <p>Para convertir las dimensiones en puntos a dimensiones en pixels, veremos que muchos componentes tienen un factor de escala. Podemos consultar el factor de escala de la pantalla en la propiedad <code>[UIScreen mainScreen].scale</code>. La resoluci\u00f3n en pixels ser\u00e1 igual a la dimensi\u00f3n en puntos multiplicada por el factor de escala. Por ejemplo, en los dispositivos b\u00e1sicos el factor de escala es <code>1.0</code>, por lo que la resoluci\u00f3n l\u00f3gica y f\u00edsica coinciden. Sin embargo, en dispositivos con pantalla retina la escala es <code>2.0</code>, por lo que su resoluci\u00f3n en pixels es exactamente el doble que las dimensiones en puntos (640 x 960 en el caso del iPhone). Para los iphone 6 plus, la escala es <code>3.0</code>.</p> <p>El mayor inconveniente del sistema de coordenadas es que el eje y se encuentre invertido. Podemos cambiar esto aplicando una transformaci\u00f3n al contexto. Podemos establecer como atributo una matriz de transformaci\u00f3n (Current Transform Matrix, CTM), que se aplicar\u00e1 a todo el contenido que dibujemos. Podemos aprovechar esta matriz de transformaci\u00f3n para cambiar el origen de coordenadas e invertir el eje y:</p> <pre><code>CGContextTranslateCTM(context, 0.0, rect.size.height);\nCGContextScaleCTM(context, 1.0, -1.0);\n</code></pre> <p>En el ejemplo anterior primero aplicamos al contexto una translaci\u00f3n para mover el origen de coordenadas a la esquina superior izquierda, y tras esto aplicamos un escalado <code>-1.0</code> al eje y para invertirlo. De esta forma podemos trabajar con el mismo sistema de coordenadas que en UIKit.</p>"},{"location":"graficos-ios.html#atributos-del-pincel","title":"Atributos del pincel","text":"<p>En el punto anterior hemos visto como podemos aplicar al contexto atributos de transformaci\u00f3n, modificando la matriz de transformaci\u00f3n actual (CTM) mediante una serie de funciones. Vamos a ver a continuaci\u00f3n otras funciones que podemos aplicar sobre el contexto para establecer otros tipos de atributos para el dibujado.</p> <p>En primer lugar, vamos a ver c\u00f3mo establecer el color con el que dibujaremos. Los colores en Core Graphics se definen mediante la clase <code>CGColor</code>, aunque podemos ayudarnos de <code>UIColor</code> para crearlos de forma m\u00e1s sencilla. Esta \u00faltima clase tiene una propiedad <code>CGColor</code> con la que podremos obtener el color en el formato Core Graphics.</p> <pre><code>UIColor *color = [UIColor redColor]\n\nCGContextSetStrokeColorWithColor(context, color.CGColor);\nCGContextSetFillColorWithColor(context, color.CGColor);\n</code></pre> <p>Podemos observar que tenemos funciones para establecer el color de relleno y el color del trazo. Seg\u00fan si dibujamos s\u00f3lo el contorno de una figura, o de si dibujamos su relleno, se aplicar\u00e1 un color u otro.</p> <p>En el caso de que vayamos a dibujar el contorno de una figura, o simplemente una polil\u00ednea, podemos establecer una serie de atributos del trazo. Por ejemplo, podemos establecer el grosor de la l\u00ednea con:</p> <pre><code>CGContextSetLineWidth(c, 0.1f);\n</code></pre> <p>Encontramos tambi\u00e9n funciones para establecer l\u00edneas punteadas, o para establecer la forma de dibujar los extremos o los puntos de uni\u00f3n de las l\u00edneas.</p>"},{"location":"graficos-ios.html#primitivas-geometricas","title":"Primitivas geom\u00e9tricas","text":"<p>Una de las primitivas geom\u00e9tricas m\u00e1s sencillas que podemos dibujar es el rect\u00e1ngulo. En primer lugar debemos definir la posici\u00f3n y dimensiones del rect\u00e1ngulo mediante un objeto de tipo <code>CGRect</code> (encontramos la macro <code>CGRectMake</code> con la que podemos inicializarlo).</p> <p>Tras crear el rect\u00e1ngulo, podemos dibujar su relleno (fill) o su contorno (stroke), o bien dibujar el contorno sobre el relleno:</p> <pre><code>CGRect rectangulo = CGRectMake(60, 40, 200, 400);\n\nCGContextStrokeRect(context, rectangulo);\nCGContextFillRect(context, rectangulo);\n</code></pre> <p>De forma similar, podemos dibujar una elipse proporcionando el rect\u00e1ngulo que la contiene. Un elemento m\u00e1s complejo, pero que nos proporciona mayor libertad es la trayectoria (path). Para dibujar una trayectoria primero debemos a\u00f1adir al contexto todos los elementos que la forman, y una vez a\u00f1adidos los dibujaremos en pantalla en una \u00fanica operaci\u00f3n. La trayectoria puede estar formada por diferentes elementos, como l\u00edneas, rect\u00e1ngulo y arcos. Los distintos elementos pueden no estar conectados entre s\u00ed. Por ejemplo, podemos crear y dibujar una trayectoria de la siguiente forma:</p> <pre><code>CGContextMoveToPoint(c, 10, 10);\nCGContextAddLineToPoint(c, 20, 30);\nCGContextAddLineToPoint(c, 30, 45);\n\nCGContextStrokePath(c);\n</code></pre> <p>El contexto recuerda el \u00faltimo punto en el que se qued\u00f3 la trayectoria, y a\u00f1ade los elementos a partir de ese punto. Podemos mover el punto actual con <code>CGContextMoveToPoint</code>, lo cual suele ser necesario siempre al comenzar a dibujar la trayectoria. Tras esto, podemos ir a\u00f1adiendo elementos con una serie de m\u00e9todos <code>CGContextAdd-</code>. En el ejemplo anterior hemos a\u00f1adido dos segmentos de l\u00ednea, aunque podr\u00edamos a\u00f1adir otros elementos como rect\u00e1ngulos o arcos. Tras a\u00f1adir todos los elementos necesarios, podemos dibujar la trayectoria definida con <code>CGContextStrokePath</code>.</p> <p>Al finalizar la trayectoria tambi\u00e9n podemos indicar que se cierre, para as\u00ed conectar el punto final con el inicial. Podemos rellenar la figura generada:</p> <pre><code>CGContextMoveToPoint(c, 10, 10);\nCGContextAddLineToPoint(c, 20, 30);\nCGContextAddLineToPoint(c, 30, 10);\nCGContextClosePath(c);\n\nCGContextFillPath(c);\n</code></pre>"},{"location":"graficos-ios.html#imagenes","title":"Im\u00e1genes","text":"<p>Podemos dibujar im\u00e1genes con Core Graphics mediante la funci\u00f3n <code>CGContextDrawImage</code>:</p> <pre><code>CGRect area = CGRectMake(0, 0, 320, 480);\nCGImageRef imagen = [UIImage imageNamed: @\"imagen.png\"].CGImage;\n\nCGContextDrawImage (contexto, area, imagen);\n</code></pre> <p>Debemos pasar como par\u00e1metro el rect\u00e1ngulo dentro del cual se debe dibujar la imagen, y la imagen como dato de tipo <code>CGImageRef</code>. Podemos obtener esta representaci\u00f3n mediante la propiedad <code>CGImage</code> de la clase <code>UIImage</code>. Encontramos tambi\u00e9n diferentes funciones en Core Graphics para crear las im\u00e1genes directamente a partir de diferentes fuentes, en lugar de tener que pasar por <code>UIImage</code>.</p> <p>Sin embargo, ser\u00e1 m\u00e1s sencillo utilizar la clase <code>UIImage</code> directamente. Esta clase tiene un m\u00e9todo que nos permite dibujar la imagen en el contexto actual de forma m\u00e1s sencilla:</p> <pre><code>CGRect area = CGRectMake(0, 0, 320, 480);\nUIImage *imagen = [UIImage imageNamed: @\"imagen.png\"];\n\n[imagen drawInRect:area];\n</code></pre>"},{"location":"graficos-ios.html#texto","title":"Texto","text":"<p>Podemos dibujar texto directamente con Core Graphics. Se puede establecer una fuente con <code>CGContextSelectFont</code> y el modo en el que se dibujar\u00e1 el texto con <code>CGContextSetTextDrawingMode</code>: trazo, relleno, relleno y trazo, o invisible (m\u00e1s adelante veremos la utilidad de dibujar el texto como invisible).</p> <pre><code>CGContextSelectFont(context, \"Helvetica-Bold\", 12, kCGEncodingMacRoman);\nCGContextSetTextDrawingMode(context, kCGTextFill);\n</code></pre> <p>Cuando dibujemos el texto dentro del contexto gr\u00e1fico de una vista, encontraremos el problema de que el texto sale invertido, por la diferencia que existe entre los sistemas de coordenadas que hemos comentado anteriormente. Por ello, en este caso deberemos aplicar una transformaci\u00f3n al sistema de coordenadas con el que se dibuja el texto. Esto lo haremos con el m\u00e9todo <code>CGContextSetTextMatrix</code>:</p> <pre><code>CGAffineTransform transform = CGAffineTransformMakeScale(1, -1);\nCGContextSetTextMatrix(context, transform);\n</code></pre> <p>Una vez hemos terminado de configurar la forma en la que se dibujar\u00e1 el texto, podemos dibujar una cadena en una posici\u00f3n del contexto con <code>CGContextShowTextAtPoint</code>, pasando la cadena como una cadena C (no como <code>NSString</code>) con cualquier tipo de codificaci\u00f3n, por ejemplo MacOSRoman:</p> <pre><code>NSString *cadena = @\"Texto de prueba\";\n\nCGContextShowTextAtPoint(context, x, y,\n[cadena cStringUsingEncoding: NSMacOSRomanStringEncoding],\n[cadena length]);\n</code></pre> <p>En algunas ocasiones nos puede interesar medir el texto, para as\u00ed poder ubicar el resto de elementos en concordancia con las dimensiones del texto. Una vez dibujada la cadena, podemos obtener la posici\u00f3n x donde ha finalizado el texto con:</p> <pre><code>CGPoint position = CGContextGetTextPosition(context);\n</code></pre> <p>Si queremos obtener las dimensiones del texto sin tener que mostrarlo, podemos dibujarlo con el modo <code>kCGTextInvisible</code>, y tras hacer esto consultar la posici\u00f3n en la que se ha quedado tras dibujarlo:</p> <pre><code>CGContextSetTextDrawingMode(context, kCGTextInvisible);\nCGContextShowTextAtPoint(context, 0, 0,\n[cadena cStringUsingEncoding: NSMacOSRomanStringEncoding],\n[cadena length]);\nCGPoint ancho = CGContextGetTextPosition(context);\n</code></pre> <p>Esta es la forma con la que podemos dibujar texto con Core Graphics. Sin embargo, resulta bastante m\u00e1s sencillo utilizar las facilidades de la clase <code>NSString</code>, aunque sea algo menos eficiente que el m\u00e9todo anterior. Con esta clase tenemos un mayor control sobre el \u00e1rea en la que se dibuja el texto y la forma de alinearlo. Adem\u00e1s, con el m\u00e9todo anterior tendremos ciertos problemas de codificaci\u00f3n si necesitamos determinados caracteres especiales, ya que estamos limitados a utilizar la codificaci\u00f3n MacOS Roman. Por ejemplo, con el m\u00e9todo anterior el s\u00edmbolo del euro no se muestra correctamente.</p> <p>Vamos a ver ahora una alternativa m\u00e1s sencilla para dibujar texto que evita los problemas con la codificaci\u00f3n. Esta alternativa consiste en utilizar el m\u00e9todo <code>drawInRect:withFont:</code> de la clase <code>NSString</code> (concretamente de su categor\u00eda <code>UIStringDrawing</code>), o cualquiera de sus variantes. Este m\u00e9todo dibujar\u00e1 el texto en el contexto gr\u00e1fico activo.</p> <pre><code>NSString *cadena = @\"Texto de prueba\";\nUIFont *font = [UIFont boldSystemFontOfSize: 12];\nCGRect area = CGRectMake(10, 10, 100, 20);\n\n[cadena drawInRect: area withFont:font\nlineBreakMode:UILineBreakModeClip\nalignment:UITextAlignmentRight];\n</code></pre> <p>El m\u00e9todo anterior, aunque c\u00f3modo de utilizar, ha sido desaprobado en las \u00faltimas versiones de iOS y en su lugar se recomienda utilizar una variante m\u00e1s gen\u00e9rica:</p> <pre><code>NSDictionary *dictionary = [[NSDictionary alloc] initWithObjectsAndKeys: font, NSFontAttributeName,\n[UIColor whiteColor], NSForegroundColorAttributeName,\nnil];\n\n[texto drawInRect: area\nwithAttributes:dictionary];\n</code></pre> <p>La propia clase <code>NSString</code> tambi\u00e9n nos proporciona informaci\u00f3n sobre las m\u00e9tricas del texto, de una forma mucho m\u00e1s sencilla que el m\u00e9todo visto anteriormente con Core Graphics, mediante el m\u00e9todo <code>sizeWithFont:</code> y sus variantes.</p> <p>Si necesitamos tener un mayor control sobre la forma de dibujar texto, y evitar los problemas de codificaci\u00f3n de Core Graphics, podemos utilizar Core Text.</p>"},{"location":"graficos-ios.html#gradientes","title":"Gradientes","text":"<p>Un elemento muy utilizado en las aplicaciones es el gradiente. Por ejemplo, muchas aplicaciones decoran el fondo de las celdas de las tablas haciendo que tengan un degradado. Para dibujar un gradiente con Core Graphics primero deberemos definirlo mediante el tipo <code>CGGradientRef</code>. Para crear este tipo deberemos proporcionar los colores que forman el gradiente. El rango de posiciones (locations) del gradiente va de <code>0</code> a <code>1</code>, y podremos especificar los colores que debe tener en los extremos y en los puntos intermedios que consideremos necesarios. El resto del rango se generar\u00e1 por interpolaci\u00f3n de los colores especificados. Por ejemplo, si en la posici\u00f3n <code>0</code> ponemos color negro, y en <code>1</code> ponemos el blanco, tendremos un degradado de negro a blanco conforme avanzamos de <code>0</code> a <code>1</code>, pasando por todos los grises intermedios.</p> <p>Adem\u00e1s, al crear el gradiente debemos especificar el espacio de colores que vamos a utilizar para establecer los colores del gradiente. El espacio de color puede ser RGB, CMYK o gris. En cualquiera de estos casos, para cada color deberemos especificar sus componentes ([R, G, B], [C, M, Y, K] o [gris]) como valores en el rango <code>[0.0, 1.0]</code>. Adem\u00e1s tendr\u00e1 un canal alpha adicional para indicar el nivel de opacidad (<code>0.0</code> es transparente, y <code>1.0</code> totalmente opaco). Por ejemplo, si establecemos el espacio de color como RGB con <code>CGColorSpaceCreateDeviceRGB</code>, para cada color deberemos establecer sus bandas <code>[R, G, B, A]</code> como vemos en el siguiente ejemplo:</p> <pre><code>size_t size = 2;\nCGFloat locations[2] = { 0.0, 1.0 };\nCGFloat components[8] = { 0.2, 0.2, 0.2, 1.0,   // Color inicial (RGBA)\n0.8, 0.8, 0.8, 1.0 }; // Color final   (RGBA)\n\nCGColorSpaceRef space = CGColorSpaceCreateDeviceRGB();\nCGGradientRef gradient = CGGradientCreateWithColorComponents(space, components, locations, size);\n</code></pre> <p>En este caso hemos especificado s\u00f3lo dos posiciones para el gradiente, la inicial (<code>0.0</code>) y la final (<code>1.0</code>). El color inicial es un gris oscuro opaco, y el final es un gris claro tambi\u00e9n opaco, en el espacio RGB. Por \u00faltimo, se crea el gradiente con <code>CGGradientCreateWithColorComponents</code>, que toma como par\u00e1metros el espacio de color utilizado, la lista de colores en el espacio especificado en el anterior par\u00e1metro, la posici\u00f3n a la que corresponde cada color, y el n\u00famero de posiciones especificadas (2 en este ejemplo: <code>0.0</code> y <code>1.0</code>).</p> <p>Tras definir el gradiente, podemos dibujarlo en pantalla con <code>CGContextDrawLinearGradient</code> o con <code>CGContextDrawRadialGradient</code>, seg\u00fan queramos dibujar el gradiente de forma lineal o radial. Por ejemplo, en el caso del gradiente lineal debemos especificar el punto inicial y final del gradiente en coordenadas del contexto. En el punto inicial se mostrar\u00e1 el color del gradiente en su posici\u00f3n <code>0.0</code>, y el lienzo del contexto se ir\u00e1 llenando con el color del degradado hasta llegar al punto final, que coindidir\u00e1 con el color del degradado en su posici\u00f3n <code>1.0</code>.</p> <pre><code>CGPoint startPoint = CGPointMake(0.0, 0.0);\nCGPoint endPoint = CGPointMake(0.0, 480.0);\n\nCGContextDrawLinearGradient(context, gradient, startPoint, endPoint, 0);\n</code></pre> <p>En este ejemplo aplicamos el degradado para que llene toda la pantalla de forma vertical, desde la posici\u00f3n y = 0.0 hasta y = 480.0. Al ser la x = 0.0 en ambos casos, el gradiente se mover\u00e1 \u00fanicamente en la vertical, pero si hubi\u00e9semos cambiando tambi\u00e9n la x habr\u00edamos tenido un gradiente oblicuo.</p>"},{"location":"graficos-ios.html#capas","title":"Capas","text":"<p>Cuando queremos dibujar un mismo elemento de forma repetida, podemos utilizar una capa para as\u00ed reutilizarlo. La ventaja de las capas es que dibujaremos dicha composici\u00f3n una \u00fanica vez, y tambi\u00e9n se almacenar\u00e1 una \u00fanica vez en la memoria de v\u00eddeo, pudiendo replicarla tantas veces como queramos en el contexto.</p> <p>Las capas son elementos de tipo <code>CGLayerRef</code>. La capa debe crearse a partir del contexto actual, pero realmente lo que dibujemos en ella no quedar\u00e1 reflejado inmediatamente en dicho contexto, sino que la capa incorpora un contexto propio. Tras crear la capa, deberemos obtener dicho contexto de capa para dibujar en ella.</p> <pre><code>CGLayerRef layer = CGLayerCreateWithContext (context, CGRectMake(0, 0, 50, 50), NULL);\nCGContextRef layerContext = CGLayerGetContext (layer);\n</code></pre> <p>A partir de este momento, para dibujar en la capa dibujaremos en <code>layerContext</code>. Podemos dibujar en \u00e9l de la misma forma que en el contexto gr\u00e1fico de la pantalla. Tras dibujar el contenido de la capa, podremos mostrarla en nuestro contexto en pantalla con:</p> <pre><code>CGContextDrawLayerAtPoint(context, CGPointZero, layer);\n</code></pre> <p>Donde el segundo par\u00e1metro indica las coordenadas de la pantalla en la que se dibujar\u00e1 la capa. Podemos repetir esta operaci\u00f3n m\u00faltiples veces con diferentes coordenadas, para as\u00ed dibujar nuestra capa replicada en diferentes posiciones.</p>"},{"location":"graficos-ios.html#generacion-de-imagenes","title":"Generaci\u00f3n de im\u00e1genes","text":"<p>Anteriormente hemos visto que la API de Core Graphics nos permite dibujar en un contexto gr\u00e1fico, pero dicho contexto no siempre tiene que ser una regi\u00f3n de la pantalla. Podemos crear distintos contextos en los que dibujar, y dibujaremos en ellos utilizando siempre la misma API. Por ejemplo, vamos a ver c\u00f3mo dibujar en una imagen. Podemos activar un contexto para dibujar en una imagen de la siguiente forma:</p> <pre><code>UIGraphicsBeginImageContextWithOptions(CGSizeMake(320,240), NO, 1.0);\n</code></pre> <p>En el primer par\u00e1metro especificamos las dimensiones de la imagen en puntos como una estructura de tipo <code>CGSize</code>, el segundo indica si la imagen es opaca (pondremos <code>NO</code> si queremos soportar transparencia), y el tercer y \u00faltimo par\u00e1metro sirve para indicar el factor de escala. Las dimensiones en pixels de la imagen se obtendr\u00e1n multiplicando las dimensiones especificadas en puntos por el factor de escala. Esto nos permitir\u00e1 soportar de forma sencilla pantallas con diferentes densidades de pixels, al trabajar con una unidad independiente de la densidad (puntos). Podemos obtener la escala de la pantalla del dispositivo con <code>[UIScreen mainScreen].scale</code>, pero si como escala especificamos <code>0.0</code>, se utilizar\u00e1 autom\u00e1ticamente la escala de la pantalla del dispositivo como escala de la imagen.</p> <p>Tras inicializar el contexto gr\u00e1fico, podremos dibujar en \u00e9l, y una vez hayamos terminado podremos obtener la imagen resultante con <code>UIGraphicsGetImageFromCurrentImageContext()</code> y cerraremos el contexto gr\u00e1fico con <code>UIGraphicsEndImageContext()</code>:</p> <pre><code>UIGraphicsBeginImageContextWithOptions(CGSizeMake(320,240), NO, 1.0);\n\n// Dibujar en el contexto\nCGContextRef context = UIGraphicsGetCurrentContext();\n...\n\nUIImage *imagen = UIGraphicsGetImageFromCurrentImageContext();\nUIGraphicsEndImageContext();\n</code></pre>"},{"location":"graficos-ios.html#generacion-de-pdfs","title":"Generaci\u00f3n de PDFs","text":"<p>El formato en el que Quartz 2D genera los gr\u00e1ficos es independiente del dispositivo y de la resoluci\u00f3n. Esto hace que estos mismos gr\u00e1ficos puedan ser guardados por ejemplo en un documento PDF, ya que usan formatos similares. Para crear un PDF con Quartz 2D simplemente deberemos crear un contexto que utilice como lienzo un documento de este tipo. Podemos escribir en un PDF en memoria, o bien directamente en disco. Para inicializar un contexto PDF utilizaremos la siguiente funci\u00f3n:</p> <pre><code>UIGraphicsBeginPDFContextToFile(@\"fichero.pdf\", CGRectZero, nil);\n</code></pre> <p>Como primer par\u00e1metro especificamos el nombre del fichero en el que guardaremos el PDF, y como segundo par\u00e1metro las dimensiones de la p\u00e1gina. Si este par\u00e1metro es un rect\u00e1ngulo de dimensi\u00f3n <code>0 x 0</code> (<code>CGRectZero</code>), como en el ejemplo anterior, tomar\u00e1 el valor por defecto <code>612 x 792</code>. El tercer par\u00e1metro es un diccionario en el que podr\u00edamos especificar distintas propiedades del PDF a crear.</p> <p>Una vez dentro del contexto del PDF, para cada p\u00e1gina que queramos incluir en el documento deberemos llamar a la funci\u00f3n <code>UIGraphicsBeginPDFPage</code>, y tras esto podremos obtener la referencia al contexto actual y dibujar en \u00e9l:</p> <pre><code>UIGraphicsBeginPDFPage();\n\n// Dibujar en el contexto\nCGContextRef context = UIGraphicsGetCurrentContext();\n...\n</code></pre> <p>Repetiremos esto tantas veces como p\u00e1ginas queramos crear en el documento. Tambi\u00e9n tenemos un m\u00e9todo alternativo para crear las p\u00e1ginas, <code>UIGraphicsBeginPDFPageWithInfo</code>, que nos permite especificar sus dimensiones y otras propiedades.</p> <p>Una vez hayamos terminado de componer las diferentes p\u00e1ginas del documento, podremos cerrar el contexto con <code>UIGraphicsEndPDFContext()</code>.</p>"},{"location":"graficos-ios.html#animaciones-con-core-animation","title":"Animaciones con Core Animation","text":"<p>Vamos a pasar a estudiar la forma en la que podemos incluir animaciones en la interfaz de nuestras aplicaciones.</p> <p>Nunca deberemos crear animaciones llamando al m\u00e9todo <code>setNeedsDisplay</code> de la vista para actualizar cada fotograma, ya que esto resulta altamente ineficiente. Si queremos animar los elementos de nuestra interfaz podemos utilizar el framework Core Animation. Con \u00e9l podemos crear de forma sencilla y eficiente animaciones vistosas para los elementos de la interfaz de las aplicaciones. Si necesitamos un mayor control sobre la animaci\u00f3n, como es el caso de los juegos, en los que tenemos que animar el contenido que nosotros pintamos con una elevada tasa de refresco, entonces deberemos utilizar OpenGL.</p> <p>Nos centraremos ahora en el caso de Core Animation. El elemento principal de este framework es la clase <code>CALayer</code>. No debemos confundir esta clase (Core Animation Layer) con <code>CGLayer</code> (Core Graphics Layer). La capa <code>CGLayer</code> nos permit\u00eda repetir un mismo elemento en nuestra composici\u00f3n, pero una vez dibujado lo que obtenemos es simplemente una composici\u00f3n 2D para mostrar en pantalla, es decir, no podemos manipular de forma independiente los distintos elementos que forman dicha composici\u00f3n.</p> <p>Sin embargo, <code>CALayer</code> de Core Animation es una capa que podremos transformar de forma din\u00e1mica independientemente del resto de capas de la pantalla. El contenido de una capa <code>CALayer</code> puede ser una composici\u00f3n creada mediante Core Graphics.</p> <p>Realmente todas las vistas (<code>UIView</code>) se construyen sobre capas <code>CALayer</code>. Podemos obtener la capa asociada a una vista con su propiedad <code>layer</code>:</p> <pre><code>CALayer *layer = self.view.layer;\n</code></pre> <p>Estas capas, adem\u00e1s de ser animadas, nos permiten crear efectos visuales de forma muy sencilla mediante sus propiedades.</p> <p>Las capas no se pueden mostrar directamente en la ventana, sino que siempre deben ir dentro de una vista (<code>UIView</code>), aunque dentro de la capa de una vista podemos incluir subcapas.</p>"},{"location":"graficos-ios.html#propiedades-de-las-capas","title":"Propiedades de las capas","text":"<p>Las capas tienen propiedades similares a las propiedades de las vistas. Por ejemplo podemos acceder a sus dimensiones con <code>bounds</code> o a la regi\u00f3n de pantalla que ocupan con <code>frame</code>. Tambi\u00e9n se le puede aplicar una transformaci\u00f3n con su propiedad <code>transform</code>. En el caso de las vistas, su posici\u00f3n se especificaba mediante la propiedad <code>center</code>, que hac\u00eda referencia a las coordenadas en las que se ubica el punto central de la vista. Sin embargo, en las capas tenemos dos propiedades para especificar la posici\u00f3n: <code>position</code> y <code>anchorPoint</code>. La primera de ellas, <code>position</code>, nos da las coordenadas del anchor point de la capa, que por defecto es el punto central, pero que no tiene por qu\u00e9 serlo.</p> <p>El <code>anchorPoint</code> se especifica en coordenadas relativas al ancho y alto de la capa, pudiendo moverse entre (0, 0) y (1, 1). El punto central corresponde al anchor point (0.5, 0.5), y este es el valor por defecto.</p> <p>Hay una propiedad adicional para la posici\u00f3n, <code>positionZ</code>, que nos da el coordenada z de la capa. Es decir, cuando tengamos varias capas solapadas, dicha propiedad nos dir\u00e1 el orden en el que se dibujar\u00e1n las capas, y por lo tanto qu\u00e9 capas quedar\u00e1n por encima de otras. Cuanto mayor sea la coordenada z, m\u00e1s cerca estar\u00e1 la capa del usuario, y tapar\u00e1 a las capas con valores de z inferiores.</p> <p>Es especialmente interesante en este caso la propiedad <code>transform</code>, ya que nos permite incluso aplicar transformaciones 3D a las capas. En el caso de las vistas, la propiedad <code>transform</code> tomaba un dato de tipo <code>CGAffineTransform</code>, con el que se pod\u00eda especificar una transformaci\u00f3n af\u00edn 2D (traslaci\u00f3n, rotaci\u00f3n, escalado o desencaje), mediante una matriz de transformaci\u00f3n 3x3. En el caso de las capas, la transformaci\u00f3n es de tipo <code>CATransform3D</code>, que se especifica mediante una matriz de transformaci\u00f3n 4x4 que nos permite realizar transformaciones en 3D. Tendremos una serie de funciones para crear distintas transformaciones de forma sencilla, como <code>CATransform3DMakeTranslation</code>, <code>CATransform3DMakeRotation</code>, y <code>CATransform3DMakeScale</code>. Tambi\u00e9n podemos aplicar transformaciones sobre una transformaci\u00f3n ya creada, pudiendo as\u00ed combinarlas y crear transformaciones complejas, con <code>CATransform3DTranslate</code>, <code>CATransform3DRotate</code>, y <code>CATransform3DScale</code>.</p> <p>Adem\u00e1s de las propiedades anteriores, la capa ofrece una gran cantidad de propiedades adicionales que nos van a permitir decorarla y tener un gran control sobre la forma en la que aparece en pantalla. A continuaci\u00f3n destacamos algunas de estas propiedades:</p> PropiedadDescripci\u00f3n `backgroundColor`Color de fondo de la capa. `cornerRadius`Con esta propiedad podemos hacer que las esquinas aparezcan redondeadas. `shadowOffset`, `shadowColor`, `shadowRadius`, `shadowOpacity`Permiten a\u00f1adir una sombra a la capa, y controlar las propiedades de dicha sombra `borderWidth`, `borderColor`Establecen el color y el grosor del borde de la capa. `doubleSided`Las capas pueden animarse para que den la vuelta. Esta propiedad nos indica si al darle la vuelta la capa debe mostrarse tambi\u00e9n por la cara de atr\u00e1s. `contents`, `contentsGravity`Nos permite especificar el contenido de la capa como una imagen de tipo `CGImageRef`. Especificando la gravedad podemos indicar     si queremos que la imagen se escale al tama\u00f1o de la capa, o que se mantenga su tama\u00f1o inicial y se alinee con alguno de los bordes. <p>Como hemos comentado anteriormente, las capas se organizan de forma jer\u00e1rquica, al igual que ocurr\u00eda con las vistas. Podemos a\u00f1adir una subcapa con el m\u00e9todo <code>addLayer:</code></p> <pre><code>CALayer *nuevaCapa = [CALayer layer];\n[self.view.layer addSublayer: nuevaCapa];\n</code></pre> <p>En la secci\u00f3n anterior vimos c\u00f3mo dibujar en una vista utilizando Core Graphics. Si queremos dibujar directamente en una capa podemos hacerlo de forma similar, pero en este caso necesitamos un delegado que implemente el m\u00e9todo <code>drawLayer:inContext:</code></p> <pre><code>- (void)drawLayer:(CALayer *)layer inContext:(CGContextRef)context {\n// Codigo Core Graphics\n...\n}\n</code></pre> <p>Deberemos establecer el delegado en la capa mediante su propiedad <code>delegate</code>:</p> <pre><code>self.layer.delegate = self;\n</code></pre> <p>Si queremos que se repinte el contenido de la capa, al igual que ocurr\u00eda en las vistas deberemos llamar a su m\u00e9todo <code>setNeedsDisplay</code>.</p>"},{"location":"graficos-ios.html#animacion-de-la-capa","title":"Animaci\u00f3n de la capa","text":"<p>La API Core Animation se centra en facilitarnos animar de forma sencilla los elementos de la interfaz. Vamos a ver c\u00f3mo realizar estas animaciones. La forma m\u00e1s sencilla de realizar una animaci\u00f3n es simplemente modificar los valores de las propiedades de la capa. Al hacer esto, la propiedad cambiar\u00e1 gradualmente hasta el valor indicado. Por ejemplo, si modificamos el valor de la propiedad <code>position</code>, veremos como la capa se mueve a la nueva posici\u00f3n. Esto es lo que se conoce como animaci\u00f3n impl\u00edcita.</p> <pre><code>layer.position=CGPointMake(100.0,100.0);\n</code></pre> <p>Las animaciones impl\u00edcitas no funcionar\u00e1n si la capa pertenece a un <code>UIView</code>. Las vistas bloquean las animaciones impl\u00edcitas, s\u00f3lo podr\u00e1n animarse mediante los m\u00e9todos de animaci\u00f3n de vistas que veremos m\u00e1s adelante, o mediante animaciones expl\u00edcitas de Core Animation como vamos a ver a continuaci\u00f3n.</p> <p>En este caso la animaci\u00f3n tendr\u00e1 una duraci\u00f3n establecida por defecto. Si queremos tener control sobre el tiempo en el que terminar\u00e1 de completarse la animaci\u00f3n, tendremos que definirlo mediante una clase que se encargue de gestionarla. De esta forma tendremos animaciones expl\u00edcitas.</p> <p>Para gestionar las animaciones expl\u00edcitas tenemos la clase <code>CABasicAnimation</code>. Con ella podemos establecer las propiedades que queremos modificar, y el valor final que deben alcanzar, y la duraci\u00f3n de las animaciones. Si necesitamos un mayor control sobre la animaci\u00f3n, podemos definir una animaci\u00f3n por fotogramas clave mediante <code>CAKeyframeAnimation</code>, que nos permitir\u00e1 establecer el valor de las propiedades en determinados instantes intermedios de la animaci\u00f3n, y el resto de instantes se calcular\u00e1n por interpolaci\u00f3n. Tambi\u00e9n tenemos <code>CATransition</code> que nos permitir\u00e1 implementar animaciones de transici\u00f3n de una capa a otra (fundido, desplazamiento, etc).</p> <p>Vamos a ver c\u00f3mo definir una animaci\u00f3n b\u00e1sica. En primer lugar debemos instanciar la clase <code>CABasicAnimation</code> proporcionando la propiedad que queremos modificar en la animaci\u00f3n como una cadena KVC:</p> <pre><code>CABasicAnimation *theAnimation=\n[CABasicAnimation animationWithKeyPath:@\"position.x\"];\n</code></pre> <p>Una vez hecho esto, deberemos especificar el valor inicial y final que tendr\u00e1 dicha propiedad:</p> <pre><code>theAnimation.fromValue=[NSNumber numberWithFloat:100.0];\ntheAnimation.toValue=[NSNumber numberWithFloat:300.0];\n</code></pre> <p>Tambi\u00e9n ser\u00e1 necesario indicar el tiempo que durar\u00e1 la animaci\u00f3n, con el atributo <code>duration</code>:</p> <pre><code>theAnimation.duration=5.0;\n</code></pre> <p>Adem\u00e1s de esta informaci\u00f3n, tambi\u00e9n podemos indicar que al finalizar la animaci\u00f3n se repita la misma animaci\u00f3n en sentido inverso, o que la animaci\u00f3n se repita varias veces:</p> <pre><code>theAnimation.repeatCount=2;\ntheAnimation.autoreverses=YES;\n</code></pre> <p>Una vez configurada la animaci\u00f3n, podemos ponerla en marcha a\u00f1adi\u00e9ndola sobre la vista:</p> <pre><code>[layer addAnimation:theAnimation forKey:@\"moverCapa\"];\n</code></pre> <p>Podemos observar que al a\u00f1adir la animaci\u00f3n le asignamos un identificador propio (<code>\"moverCapa\"</code>) en el ejemplo anterior. De esta forma podremos referenciar posteriormente dicha animaci\u00f3n. Por ejemplo, si eliminamos la animaci\u00f3n <code>\"moverCapa\"</code> de la capa anterior, la capa dejar\u00e1 de moverse de izquierda a derecha. Si no la eliminamos, se animar\u00e1 hasta que complete el n\u00famero de repeticiones que hemos indicado (si como n\u00famero de repeticiones especificamos <code>HUGE_VALF</code> se repetir\u00e1 indefinidamente).</p> <p>Esta tecnolog\u00eda nos permite realizar animaciones fluidas de forma muy sencilla. En ella se basan las animaciones que hemos visto hasta ahora para hacer transiciones entre pantallas. Podemos nosotros utilizar tambi\u00e9n estas animaciones de transici\u00f3n de forma personalizada con <code>CATransition</code>. Podemos crear una transici\u00f3n y especificar el tipo y subtipo de transici\u00f3n entre los que tenemos predefinidos:</p> <pre><code>CATransition *transition = [CATransition animation];\ntransition.duration = 0.5;\n\ntransition.type = kCATransitionMoveIn;\ntransition.subtype = kCATransitionFromLeft;\n</code></pre> <p>Podemos a\u00f1adir la transici\u00f3n a nuestra capa de la siguiente forma:</p> <pre><code>[self.view.layer addAnimation:transition forKey:nil];\n</code></pre> <p>Con esto, cada vez que mostremos u ocultemos una subcapa en <code>self.view.layer</code>, dicha subcapa aparecer\u00e1 o desaparecer\u00e1 con la animaci\u00f3n de transici\u00f3n indicada.</p> <p>Aunque Core Animation nos permite controlar estas animaciones en las capas de forma sencilla, tambi\u00e9n podemos programar muchas de estas animaciones directamente sobre la vista (<code>UIView</code>), sin necesidad de ir a programar a m\u00e1s bajo nivel. Vamos a ver a continuaci\u00f3n las animaciones que podemos definir sobre las vistas.</p>"},{"location":"graficos-ios.html#animacion-de-vistas","title":"Animaci\u00f3n de vistas","text":"<p>Las vistas implementan tambi\u00e9n numerosas facilidades para implementar animaciones sencillas y transiciones. La forma m\u00e1s sencilla de implementar una animaci\u00f3n sobre nuestra vista es utilizar el m\u00e9todo <code>animateWithDuration:animations:</code> de <code>UIView</code> (es un m\u00e9todo de clase):</p> <pre><code>[UIView animateWithDuration:0.5\nanimations:^{\nvista.frame = CGRectMake(100,100,50,50);\n}];\n</code></pre> <p>Podemos observar que adem\u00e1s de la duraci\u00f3n de la animaci\u00f3n, tenemos que especificar un bloque de c\u00f3digo en el que indicamos los valores que deben tener las propiedades de las vistas al terminar la animaci\u00f3n. Hay que remarcar que en el mismo bloque de c\u00f3digo podemos especificar distintas propiedades de distintas vistas, y todas ellas ser\u00e1n animadas simult\u00e1neamente.</p> <p>Tambi\u00e9n podemos implementar transiciones entre vistas mediante una animaci\u00f3n, con el m\u00e9todo <code>transitionFromView: toView: duration: options: completion:</code>, que tambi\u00e9n es de clase.</p> <pre><code>[UIView transitionFromView: vistaOrigen\ntoView: vistaDestino\nduration: 0.5\noptions: UIViewAnimationOptionTransitionFlipFromLeft\ncompletion: nil];\n</code></pre> <p>Llamar al m\u00e9todo anterior causa que <code>vistaOrigen</code> sea eliminada de su supervista, y que en su lugar <code>vistaDestino</code> sea a\u00f1adida a ella, todo ello mediante la animaci\u00f3n especificada en el par\u00e1metro <code>options</code>.</p>"},{"location":"graficos-ios.html#ejercicios","title":"Ejercicios","text":"<p>Antes de empezar a crear los proyectos, debes descargarte las plantillas desde el repositorio <code>mastermoviles-multimedia-ios</code> de bitbucket. En este repositorio se encuentran todas las plantillas que utilizaremos en los ejercicios iOS de este m\u00f3dulo.</p>"},{"location":"graficos-ios.html#generacion-de-graficas","title":"Generaci\u00f3n de gr\u00e1ficas","text":"<p>Vamos a crear una gr\u00e1fica en iOS para mostrar la evoluci\u00f3n de la cuota de mercado de diferentes plataformas m\u00f3viles. Para ello utilizamos una sencilla librer\u00eda: S7GraphView, que consiste en una vista que mediante CoreGraphics genera la gr\u00e1fica. Vamos a modificar dicha vista para mejorar el aspecto de la gr\u00e1fica. Podemos ver la gr\u00e1fica con su funcionamiento b\u00e1sico si abrimos la aplicaci\u00f3n <code>Grafica</code> de las plantillas. Sobre esta aplicaci\u00f3n se pide lo siguiente:</p> <p>a) Vamos a dibujar la leyenda de la gr\u00e1fica, para as\u00ed saber a qu\u00e9 plataforma corresponde cada color. Para ello trabajaremos sobre el m\u00e9todo <code>drawRect:</code> de <code>S7GraphView</code>, que es quien muestra el contenido de la gr\u00e1fica. Empezaremos modificando el final del m\u00e9todo, donde mostraremos los elementos de la leyenda superpuestos sobre el contenido que ya se ha dibujado de la gr\u00e1fica, en la esquina superior derecha. Debes buscar el comentario <code>TODO</code> referente al apartado (a). Veremos que tenemos un bucle que itera por cada una de las l\u00edneas de la gr\u00e1fica. Queremos mostrar para cada una de ellas un rect\u00e1ngulo con el color de la l\u00ednea, y junto a \u00e9l el texto indicando a qu\u00e9 plataforma corresponde dicha l\u00ednea (iOS, Android y Windows Phone). Comenzaremos incluyendo en ese punto el c\u00f3digo para dibujar un rect\u00e1ngulo con el color y dimensiones indicadas en los comentarios. El rect\u00e1ngulo deber\u00e1 tener como color de relleno el color de la l\u00ednea a la que hace referencia, y como borde color blanco.</p> <p>b) En segundo lugar, vamos a dibujar el texto junto a cada rect\u00e1ngulo de la leyenda. Esto lo haremos en el lugar donde encontramos el comentario <code>TODO</code> referente al apartado</p> <p>c). El texto tendr\u00e1 color de relleno blanco, y se dibujar\u00e1 en la posici\u00f3n indicada en los comentarios.</p> <p>d) Para terminar de dibujar la leyenda, vamos a crear un marco que la englobe. Tras el bucle <code>for</code> sabremos cu\u00e1l es la posici\u00f3n y final de la leyenda, y el texto que ha sido m\u00e1s largo (dentro del bucle se est\u00e1n comprobando las m\u00e9tricas del texto para tener esta informaci\u00f3n). Sabiendo esto, sabremos el tama\u00f1o que debe tener el rect\u00e1ngulo para que se adapte al n\u00famero de l\u00edneas que tenemos, y a la leyenda de todas ellas. En el c\u00f3digo encontramos un rect\u00e1ngulo llamado <code>recuadro</code> que nos da esa informaci\u00f3n. Vamos a dibujar un rect\u00e1ngulo blanco sin relleno con esas dimensiones.</p> <p>e) Una vez dibujada la leyenda, vamos a terminar de decorar la gr\u00e1fica dibujando un gradiente de fondo, en lugar de un color s\u00f3lido. Esto deberemos hacerlo al principio de la funci\u00f3n <code>drawRect:</code>, ya que el fondo debe dibujarse antes que el resto de elementos para que quede por debajo. Buscaremos el lugar donde tenemos un comentario de tipo <code>TODO</code> referente al apartado (e), y dibujaremos ah\u00ed un gradiente desde gris oscuro hasta gris intermedio que var\u00ede en la vertical, desde y=0 hasta y=ancho.</p> <p></p>"},{"location":"graficos-ios.html#animaciones","title":"Animaciones","text":"<p>En este ejercicio vamos a ver c\u00f3mo hacer animaciones con CoreAnimation y directamente con vistas.</p> <p>Trabajaremos con el proyecto <code>Animaciones</code>. Se pide:</p> <p>a) En la pantalla principal de la aplicaci\u00f3n podemos ver una capa con la car\u00e1tula de la pel\u00edcula \"El Resplandor\". Esta capa se inicializa en <code>viewDidLoad</code>. Configurar esa capa para decorarla tal como indica en los comentarios que encontramos en dicha funci\u00f3n.</p> <p>b) Vemos que hay cuatro botones, uno en cada esquina de la pantalla, con el texto Ven aqu\u00ed. Al pulsar cualquiera de estos botones se ejecutar\u00e1 el m\u00e9todo <code>botonVenAquiPulsado:</code>. Vamos a hacer que al pulsar cualquiera de estos botones, la capa se mueva mediante una animaci\u00f3n a la posici\u00f3n central del bot\u00f3n pulsado. Incluye el c\u00f3digo necesario en el m\u00e9todo anterior.</p> <p>c) La aplicaci\u00f3n tiene tambi\u00e9n un bot\u00f3n Car\u00e1tula que nos lleva a una vista modal que muestra en grande la car\u00e1ctula de la pel\u00edcula. Esta pantalla se implementa en el controlador <code>UACaratulaViewController</code>. En este controlador se definen dos vistas de tipo imagen: <code>vistaFrontal</code> y <code>vistaReverso</code>, con la imagen del anverso y el reverso de la car\u00e1tula de la pel\u00edcula. Sin embargo, en un primer momento s\u00f3lo se muestra la imagen frontal. Vamos a hacer que al pulsar el bot\u00f3n Girar la car\u00e1tula gire mediante una animaci\u00f3n para cambiar entre anverso y reverso. Deberemos implementar esta transici\u00f3n con las facilidades de la clase <code>UIView</code> en el m\u00e9todo <code>botonGirarPulsado:</code>.</p> <p>Detalle de implementaci\u00f3n: Podemos observar que en ese m\u00e9todo distinguimos la vista que se est\u00e1 mostrando actualmente seg\u00fan si su propiedad <code>superview</code> es <code>nil</code> o no. Cuando una vista se muestra en pantalla siempre tiene una supervista. Si no tiene supervista quiere decir que no se est\u00e1 mostrando actualmente. As\u00ed podemos hacer esta distinci\u00f3n de forma sencilla.</p> <p>d) Para terminar, vamos a implementar las funcionalidades de los botones Zoom In y Zoom Out. Con el primero de ellos veremos la portada ocupando toda la pantalla, mientras que con el segundo la reduciremos a la mitad de su tama\u00f1o. Esto deberemos hacerlo con las facilidades que nos proporciona la clase <code>UIView</code> para hacer animaciones, en los m\u00e9todos <code>botonZoomInPulsado</code> y <code>botonZoomOutPulsado</code>. En el primero de ellos haremos con el tama\u00f1o de las vistas <code>vistaFrontal</code> y <code>vistaReverso</code> (propiedad <code>bounds</code>) sea (0,0,320,416). En el segundo de ellos modificaremos estas propiedades a la mitad de tama\u00f1o: (0,0,160, 208). Ten en cuenta que podemos modificar el tama\u00f1o de ambas vistas simult\u00e1neamente.</p>"},{"location":"guia-laboratorio.html","title":"Gu\u00eda de laboratorio","text":""},{"location":"guia-laboratorio.html#inicio-del-sistema-desde-un-disco-externo-en-mac","title":"Inicio del sistema desde un disco externo en Mac","text":"<p>Con el m\u00e1ster se proporciona un disco externo con el sistema MacOS Mavericks y con todo el software que utilizaremos durante el curso preinstalado en dicho sistema operativo.</p> <p>Se trata de discos de arranque externos, que s\u00f3lo podr\u00e1n ser utilizados desde ordenadores Mac. Para iniciar el sistema desde estos discos deberemos seguir los siguientes pasos:</p> <ol> <li>Conectar el disco externo a un puerto USB</li> <li>Encender el ordenador y mantener pulsada la tecla <code>alt</code></li> <li>Cuando aparezca la pantalla de selecci\u00f3n de disco de arranque, seleccionar el disco Master M\u00f3viles</li> </ol> <p>Es importante destacar que los discos est\u00e1n preparados para funcionar en los ordenadores iMac del laboratorio. Puede que no funcionen en todos los ordenadores Mac.</p> <p>Estos discos permitir\u00e1n mantener el software actualizado y configurado, sin necesidad de depender de la instalaci\u00f3n local del laboratorio ni de tener que introducir la configuraci\u00f3n personal al comienzo de cada clase.</p>"},{"location":"guia-laboratorio.html#guia-basica-de-macos","title":"Gu\u00eda b\u00e1sica de MacOS","text":"<p>Dado que durante todo el m\u00e1ster trabajaremos sobre el sistema MacOS, vamos a repasar los fundamentos b\u00e1sicos del uso de este sistema operativo.</p>"},{"location":"guia-laboratorio.html#barra-de-menu","title":"Barra de men\u00fa","text":"<p>En MacOS la barra de men\u00fa siempre est\u00e1 fija en la parte superior de la pantalla, a diferencia de Windows que incorpora dicha barra en cada ventana.</p> <p>El primer elemento de la barra es el icono de la \"manzana\", pulsando sobre el cual desplegaremos un men\u00fa con opciones importantes como las siguientes:</p> <ul> <li>Preferencias del Sistema ...</li> <li>Forzar salida ...</li> <li>Reiniciar ...</li> <li>Apagar equipo ...</li> </ul> <p></p> <p>Con Preferencias del Sistema ... abriremos el panel que nos da acceso a todos los elementos de configuraci\u00f3n del sistema.</p> <p></p> <p>Si en alg\u00fan momento alguna aplicaci\u00f3n queda bloqueada, podemos matarla con Forzar salida ....</p>"},{"location":"guia-laboratorio.html#uso-del-raton","title":"Uso del rat\u00f3n","text":"<p>En MacOS es habitual tener un \u00fanico bot\u00f3n del rat\u00f3n, teni\u00e9ndose que pulsar la combinaci\u00f3n <code>ctrl+click</code> para obtener el efecto del click secundario.</p> <p>Si estamos acostumbrados al funcionamiento del rat\u00f3n en Windows o Linux podemos configurar el sistema de forma similar. Para hacer esto entraremos en Preferencias del sistema ..., y dentro de este panel en Rat\u00f3n. Aqu\u00ed podremos especificar que pulsando sobre el lado derecho del rat\u00f3n se produzca el efecto de click secundario.</p>"},{"location":"guia-laboratorio.html#acceso-a-ficheros","title":"Acceso a ficheros","text":"<p>Podemos navegar por el sistema de ficheros mediante la aplicaci\u00f3n Finder.</p> <p></p> <p>Dentro de esta aplicaci\u00f3n tenemos un panel a la izquierda con la ubicaciones favoritas. Podemos modificar este lista entrando en el men\u00fa Finder &gt; Preferencias ....</p> <p>Hay que remarcar que la barra de men\u00fas cambia seg\u00fan la aplicaci\u00f3n que tenga el foco en un momento dado. La opci\u00f3n Finder aparecer\u00e1 junto a la \"manzana\" cuando la aplicaci\u00f3n Finder est\u00e9 en primer plano.</p> <p>Mientras navegamos por los ficheros podemos ver una vista previa r\u00e1pida pulsando la tecla <code>espacio</code>.</p>"},{"location":"guia-laboratorio.html#compresion-de-ficheros","title":"Compresi\u00f3n de ficheros","text":"<p>Para comprimir ficheros simplemente seleccionaremos los ficheros en el escritorio o en el Finder, haremos click secundario, y seleccionaremos la opci\u00f3n Comprimir. Esto comprimir\u00e1 los ficheros seleccionados en <code>zip</code> con el compresor integrado en el sistema.</p> <p>Podremos descomprimir ficheros <code>zip</code> simplemente haciendo doble click sobre ellos.</p> <p>Para descomprimir otros tipos de ficheros (+7z+, +rar+, etc) se incluye la herramienta gratuita Stuffit Expander.</p>"},{"location":"guia-laboratorio.html#abrir-aplicaciones-con-spotlight","title":"Abrir aplicaciones con spotlight","text":"<p>La forma m\u00e1s sencilla de abrir aplicaciones en MacOS es mediante el Spotlight (el icono de la lupa de la esquina superior derecha de la pantalla).</p> <p></p> <p>Deberemos pulsar sobre este icono y empezar a escribir el nombre de la aplicaci\u00f3n. La aplicaci\u00f3n buscada aparecer\u00e1 de forma r\u00e1pida en la lista y podremos abrirla.</p>"},{"location":"guia-laboratorio.html#aplicaciones-en-el-dock","title":"Aplicaciones en el Dock","text":"<p>El Dock es la barra que vemos en la parte inferior de la pantalla. Tenemos ah\u00ed los iconos de algunas de las aplicaciones instaladas, y de todas las aplicaciones abiertas actualmente.</p> <p></p> <p>Ser\u00e1 recomendable dejar en el Dock de forma fija los iconos de las aplicaciones que m\u00e1s utilicemos. Para hacer esto abriremos la aplicaci\u00f3n, y una vez aparece su icono en el Dock, mantendremos el bot\u00f3n de rat\u00f3n pulsado sobr\u00e9 \u00e9l y seleccionaremos Opciones &gt; Mantener en el Dock. Con esto, aunque cerremos la aplicaci\u00f3n, su icono seguir\u00e1 ah\u00ed, permiti\u00e9ndonos abrirla de forma r\u00e1pida.</p> <p></p>"},{"location":"guia-laboratorio.html#aplicaciones-a-pantalla-completa","title":"Aplicaciones a pantalla completa","text":"<p>Casi todas las aplicaciones pueden ponerse a pantalla completa mediante el icono que tienen en la esquina superior derecha. Cuando tengamos varias aplicaciones a pantalla completa podemos cambiar de una a otra mediante la combinaci\u00f3n de teclas <code>ctrl + cursor izq./der.</code>.</p> <p>Cuando una aplicaci\u00f3n est\u00e1 a pantalla completa podemos cerrar la ventana pulsando la combinaci\u00f3n <code>ctrl + W</code>. Esto es equivalente a cerrar la ventaa con el bot\u00f3n rojo de su esquina superior izquierda cuando estamos en modo ventana.</p> <p>Es importante destacar que en MacOS las aplicaciones no se cierran al cerrar la ventana. Para cerrarlas tendremos que entrar en el men\u00fa con el nombre de la aplicaci\u00f3n (el que aparece junto a la \"manzana\") y pulsar sobre Salir.</p> <p>Veremos las aplicaciones que est\u00e1n abiertas marcadas con una luz en la parte inferior del Dock.</p>"},{"location":"guia-laboratorio.html#montar-discos-externos","title":"Montar discos externos","text":"<p>Al conectar un disco externo el sistema normalmente lo reconocer\u00e1 autom\u00e1ticamente y podremos verlo en el escritorio y en el panel izquierdo del Finder.</p> <p>Podremos utilizar discos con formato MacOS y con formato FAT32. Tambi\u00e9n podremos conectar discos NTFS, pero \u00e9stos ser\u00e1n de s\u00f3lo lectura. Existen aplicaciones y formas de configurar el sistema para tener tambi\u00e9n la posibilidad de escribir en este tipo de sistemas de ficheros.</p> <p>Para desmontar un disco podemos pulsar sobre el icono de expulsi\u00f3n en el Finder, o bien arrastrar el icono del disco a la papelera. Cuando hagamos esto veremos que la papelera cambia de aspecto y pasa a ser un icono de expulsi\u00f3n.</p>"},{"location":"guia-laboratorio.html#cuentas-de-usuario","title":"Cuentas de usuario","text":"<p>Podemos configurar nuestras cuentas de usuario en Preferencias del Sistema ... &gt; Cuentas de Internet.</p> <p>Por ejemplo podr\u00edamos configurar aqu\u00ed nuestra cuenta de Google. Una vez configurada la cuenta, podremos ver nuestro correo de Gmail en la aplicaci\u00f3n Mail, nuestros calendarios de Google Calendar en la aplicaci\u00f3n Calendario, nuestros contactos de Google en la aplicaci\u00f3n Contactos, etc.</p>"},{"location":"guia-laboratorio.html#instalacion-de-aplicaciones","title":"Instalaci\u00f3n de aplicaciones","text":"<p>Podemos instalar aplicaciones desde la Mac App Store. Necesitaremos configurar una cuenta de Apple para poder hacer esto.</p> <p>Tambi\u00e9n podemos instalar aplicaciones descargadas desde la web. Normalmente las aplicaciones de Mac vienen empaquetadas en ficheros <code>.dmg</code> que consisten en una imagen de disco que podemos montar en el sistema.</p> <p>La instalaci\u00f3n suele consistir \u00fanicamente en arrastrar un fichero al directorio <code>/Applications</code>.</p> <p>De la misma forma, para desinstalar estas aplicaciones simplemente tendr\u00edamos que arrastrar dicho fichero a la papelera.</p> <p>Otras aplicaciones que requieren una instalaci\u00f3n m\u00e1s compleja proporcionan un instalador.</p>"},{"location":"guia-laboratorio.html#copiar-cortar-y-pegar","title":"Copiar, cortar y pegar","text":"<p>En MacOS podemos cortar, copiar y pegar como en otros Sistemas Operativos, pero es importante tener en cuenta que en lugar de usar la tecla <code>ctrl</code> se utiliza <code>cmd</code>. Es decir, copiaremos con \u00b4cmd + C\u00b4, cortaremos con <code>cmd + X</code>, y pegaremos con <code>cmd + V</code>.</p> <p>Esto ser\u00e1 as\u00ed para pr\u00e1cticamente todos los atajos de teclado. Aquellos que en Windows y Linux utilizan en la combinaci\u00f3n la tecla <code>ctrl</code> en MacOS normalmente utilizan <code>cmd</code>.</p>"},{"location":"guia-laboratorio.html#repositorios-git","title":"Repositorios git","text":"<p>El sistema de contron de versiones git realiza una gesti\u00f3n distribuida del c\u00f3digo de nuestros proyectos. Tendremos un repositorio remoto donde se almacenar\u00e1n las versiones de los artefactos de nuestros proyecto, y este repositorio remoto estar\u00e1 replicado en un repositorio local en nuestra m\u00e1quina. De esta forma podremos realizar commits con frecuencia en nuestra m\u00e1quina sin que estos cambios afecten a otros usuarios del repositorio remoto. Cuando tras realizar una serie de cambios hayamos llegado a un estado estable de nuestro proyecto, podremos hacer push para subir todos los commits pendientes al repositorio remoto.</p> <p>Para la creaci\u00f3n de los repositorios remotos utilizaremos bitbucket. Vamos a ver en primer lugar c\u00f3mo crear este repositorio remoto, y posteriormente veremos c\u00f3mo crear una replica local.</p>"},{"location":"guia-laboratorio.html#creacion-de-un-repositorio-remoto-en-bitbucket","title":"Creaci\u00f3n de un repositorio remoto en bitbucket","text":"<p>Vamos a ver c\u00f3mo crear un repositorio privado en bitbucket (<code>bitbucket.org</code>) que vincularemos con nuestro repositorio local.</p> <ol> <li> <p>En primer lugar, deberemos crearnos una cuenta en bitbucket, si no disponemos ya de una: https://bitbucket.org</p> </li> <li> <p>Creamos desde nuestra cuenta de bitbucket un repositorio (Repositories &gt; Create repository).</p> </li> <li> <p>Deberemos darle un nombre al repositorio. Ser\u00e1 de tipo Git y como lenguaje especificaremos Objective-C o Android seg\u00fan la plataforma para la que vayamos a desarrollar. </p> </li> <li> <p>Una vez hecho esto, veremos el repositorio ya creado, en cuya ficha podremos encontrar la ruta que nos dar\u00e1   acceso a \u00e9l. </p> </li> </ol> <p>Ser\u00e1 \u00fatil copiar la direcci\u00f3n anterior para vincular con ella nuestro repositorio local al remoto. Veremos como hacer esto en el siguiente apartado.</p>"},{"location":"guia-laboratorio.html#creacion-del-repositorio-git-local","title":"Creaci\u00f3n del repositorio git local","text":"<p>Vamos a ver dos alternativas para crear una replica local del repositorio remoto:</p> <ul> <li>Clonar el repositorio remoto, lo cual inicializa un repositorio local en el que ya est\u00e1 configurado el v\u00ednculo con el remoto.</li> <li>Crear un repositorio local independiente, y vincularlo posteriormente con un repositorio remoto.</li> </ul> <p>Si utilizamos un IDE como Eclipse o Xcode estos pasos los realiza autom\u00e1ticamente el entorno.</p>"},{"location":"guia-laboratorio.html#creacion-a-partir-del-repositorio-remoto","title":"Creaci\u00f3n a partir del repositorio remoto","text":"<p>La forma m\u00e1s sencilla de crear un repositorio Git local es hacerlo directamente a partir del repositorio remoto. Si ya tenemos un repositorio remoto (vac\u00edo o con contenido) podemos clonarlo en nuestra m\u00e1quina local con:</p> <pre><code>git clone https://[usr]:bitbucket.org/[usr]/miproyecto-mastermoviles\n</code></pre> <p>Este comando podemos copiarlo directamente desde bitbucket, tal como hemos visto en el \u00faltimo paso del apartado anterior (opci\u00f3n Clone de la interfaz del repositorio).</p> <p>De esta forma se crea en nuestro ordenador el directorio <code>miproyecto-mastermoviles</code> y se descarga en \u00e9l el contenido del proyecto, en caso de no estar vac\u00edo el repositorio remoto. Adem\u00e1s, quedar\u00e1 configurado como repositorio Git local y conectado de forma autom\u00e1tica con el repositorio git remoto del que lo hemos clonado.</p>"},{"location":"guia-laboratorio.html#creacion-de-un-repositorio-local-y-vinculacion-con-el-remoto","title":"Creaci\u00f3n de un repositorio local y vinculaci\u00f3n con el remoto","text":"<p>Esta forma es algo m\u00e1s compleja que la anterior, pero ser\u00e1 \u00fatil si tenemos ya creado un repositorio Git local de antemano, o si queremos vincularlo con varios repositorios remotos.</p> <p>Para la creaci\u00f3n de un repositorio Git local seguiremos los siguientes pasos.</p> <ol> <li> <p>Crear un directorio local para el repositorio del m\u00f3dulo.</p> </li> <li> <p>Inicializar el directorio anterior como un repositorio Git. Para ello, ejecuta en dicho directorio el comando. <pre><code>$ git init\n</code></pre></p> </li> <li> <p>En bitbucket veremos la URL que identifica el repositorio, que ser\u00e1 del tipo: https://[usr]@bitbucket.org/[usr]/miproyecto-mastermoviles.git</p> </li> <li> <p>Configurar el repositorio remoto. El repositorio remoto por defecto suele tomar como nombre <code>origin</code>. Desde el directorio ra\u00edz del proyecto ejecutamos: <pre><code>$ git remote add origin https://[usr]@bitbucket.org/[usr]/miproyecto-mastermoviles.git\n</code></pre></p> </li> </ol> <p>Con esto habremos inicializado nuestro directorio como repositorio local Git y lo habremos conectado con el repositorio remoto de bitbucket.</p>"},{"location":"guia-laboratorio.html#registrar-cambios-en-el-repositorio","title":"Registrar cambios en el repositorio","text":"<p>Independientemente de cu\u00e1l de los m\u00e9todos anteriores hayamos utilizado para inicializar nuestro repositorio Git local conectado con el repositorio remoto de bitbucket, vamos a ver ahora c\u00f3mo trabajar con este repositorio.</p> <p>En primer lugar ser\u00e1 recomendable a\u00f1adir un fichero <code>.gitignore</code> al directorio del proyecto, que depender\u00e1 del tipo de proyecto y que se encargar\u00e1 de excluir del control de versiones todos aquellos artefactos que sean generados autom\u00e1ticamente (por ejemplo las clases compiladas o el paquete de la aplicaci\u00f3n). Podemos encontrar diferentes modelos de <code>.gitignore</code> en: https://github.com/github/gitignore</p> <p>Tras a\u00f1adir el <code>.gitignore</code> correcto para nuestro tipo de proyecto podremos a\u00f1adir nuevos artefactos y registrarlos en el sistema de control de versiones. Cada vez que queramos registrar cambios en el repositorio local deberemos:</p> <ol> <li> <p>Si hemos a\u00f1adido nuevos artefactos al proyecto, deberemos a\u00f1adirlos al sistema de control de versiones con: <pre><code>git add .\n</code></pre></p> </li> <li> <p>Hacer el primer commit desde el terminal o desde el IDE. <pre><code>git commit -m \"Versi\u00f3n inicial\"\n</code></pre></p> </li> <li> <p>Ahora podemos hacer commit cada vez que se haga alg\u00fan otro cambio para registrarlo en el repositorio local: <pre><code>$ git add .\n$ git commit -a -m \"Mensaje del commit\"\n$ git push origin master\n</code></pre></p> </li> <li> <p>Cada vez que el proyecto alcance un estado estable podremos hacer push para subir los cambios a bitbucket, indicando el nombre del repositorio remoto (<code>origin</code>) y la rama a la que se subir\u00e1 (<code>master</code> por defecto) <pre><code>$ git push origin master\n</code></pre></p> </li> </ol> <p>Con esto se subir\u00e1n a bitbucket todos los commits que estuviesen pendientes de enviar el repositorio remoto.</p>"},{"location":"guia-laboratorio.html#compartir-el-repositorio-con-grupos","title":"Compartir el repositorio con grupos","text":"<p>Todos los alumnos y profesores del m\u00e1ster pertenecen al equipo Master Moviles de bitbucket. Podremos crear y acceder a repositorios pertenecientes a nuestra cuenta personal o al equipo. Utilizaremos el equipo para las siguientes tareas:</p> <ul> <li>Los profesores publicaremos ejemplos y plantillas en el equipo Master Moviles, de forma que todos los alumnos pod\u00e1is acceder a ellos y replicarlos en vuestras m\u00e1quinas.</li> <li>Los alumnos pod\u00e9is compartir los proyectos que realic\u00e9is en vuestras cuentas con el equipo Master Moviles, para que as\u00ed los profesores tengamos acceso a ellos y podamos corregirlos. Esta ser\u00e1 la forma de realizar las entregas.</li> </ul> <p>Vamos ahora a ver c\u00f3mo hacer estas tareas.</p>"},{"location":"guia-laboratorio.html#compartir-un-repositorio-con-usuarios-de-bitbucket","title":"Compartir un repositorio con usuarios de bitbucket","text":"<p>Podemos compartir un repositorio bitbucket con un usuario o grupo dentro del equipo, de forma que dicho usuario o  todos los usuarios que pertenezcan al grupo puedan tener acceso al repositorio. Esto lo hacemos desde Settings &gt; Access management:</p> <p></p> <p>Los proyectos que realic\u00e9is en cada asignatura deber\u00e9is compartirlos con el/los profesor/es de la asignatura, dando permisos de lectura. Los profesores os aparecer\u00e1n en la lista de usuarios como TEAMMATES.</p>"},{"location":"guia-laboratorio.html#decarga-de-ejemplos-y-plantillas","title":"Decarga de ejemplos y plantillas","text":"<p>Los ejemplos y plantillas se dejan como repositorios de s\u00f3lo lectura en el grupo Master Moviles. Para que cada uno pueda tener una copia propia de lectura/escritura deber\u00e1 hacer un Fork en su cuenta personal de bitbucket.</p> <p>Para ello se deber\u00e1 entrar en el repositorio compartido de bitbucket y usar la opci\u00f3n Fork:</p> <p></p>"},{"location":"guia-laboratorio.html#ejercicios","title":"Ejercicios","text":""},{"location":"guia-laboratorio.html#creacion-de-una-cuenta-bitbucket","title":"Creaci\u00f3n de una cuenta bitbucket","text":"<p>Crea un cuenta personal de bitbucket para utilizar en el m\u00e1ster (si no dispones ya de una). Utiliza como nombre de usuario el nombre de usuario de Campus Virtual o uno similar, para poder identificar correctamente los ejercicios que entregues.</p>"},{"location":"guia-laboratorio.html#creacion-de-un-repositorio-git","title":"Creaci\u00f3n de un repositorio Git","text":"<p>Vamos a crear un primer repositorio de prueba en bitbucket.</p> <p>a) Crea un repositorio en tu cuenta personal con el nombre <code>presentacion-mastermoviles</code>.</p> <p>b) Obten desde la web de bitbucket el comando necesario para clonar el repositorio en tu m\u00e1quina local.</p> <p>c) Copia y pega el comando anterior en el terminal (aplicaci\u00f3n Terminal de MacOS) para clonar el repositorio en tu carpeta <code>$HOME</code>.</p> <p>d) Crea un fichero de texto en el directorio del proyecto al que llamaremos <code>README.md</code> con el siguiente texto.</p> <pre><code># Primera version del proyecto\n</code></pre> <p>e) A\u00f1ade el fichero anterior al sistema de control de versiones y haz el primer commit con mensaje Initial commit.</p> <p>f) A\u00f1ade una segunda linea al fichero anterior, para que quede como se muestra a continuaci\u00f3n:</p> <pre><code># Primera version del proyecto\n# Actualizado a la segunda version\n</code></pre> <p>g) Haz otro commit con el mensaje Texto modificado y posteriormente haz un push al servidor.</p> <p>h) Comprueba en bitbucket que el proyecto se ha subido. Explora desde la web los ficheros de fuentes y los diferentes commits que se han realizado, viendo para cada uno la versi\u00f3n correspondiente del fichero de fuentes.</p>"},{"location":"guia-laboratorio.html#descarga-de-un-repositorio-existente","title":"Descarga de un repositorio existente","text":"<p>En el equipo Master Moviles de bitbucket se ha dejado un proyecto llamado <code>presentacion-team-mastermoviles</code> que pod\u00e9is copiar a vuestras cuentas y descargar. Dado que s\u00f3lo tenemos permisos de lectura para dicho proyecto haremos lo siguiente:</p> <p>a) Hacemos un fork del proyecto a nuestra cuenta personal. De esta forma seramos propietarios de la copia y por lo tanto tendremos permiso de escritura.</p> <p>b) Hacemos un clone del proyecto en nuestra m\u00e1quina aprovechando el comando que nos proporciona bitbucket.</p> <p>c) Nos habr\u00e1 descargado un fichero. Vamos a modificar este fichero, hacer commit, y tras eso hacer push para subirlo al servidor.</p> <p>d) Comparte este proyecto y el realizado en el ejercicio anterior con el usuario <code>malozano</code> de bitbucket con permiso de lectura para que pueda ver tu soluci\u00f3n.</p>"},{"location":"guia-laboratorio.html#actualizacion-del-repositorio-local","title":"Actualizaci\u00f3n del repositorio local","text":"<p>En caso de que estemos trabajando con un mismo repositorio remoto desde diferentes m\u00e1quinas (bien por estar trabajando diferentes personas, o bien por estar la misma persona desde distintas m\u00e1quinas) cuando enviemos cambios al repositorio remoto desde una, deberemos ser capaces de actualizar estos cambios en el resto.</p> <p>Para actualizar en la m\u00e1quina local los cambios que se hayan subido el repositorio remoto deberemos hacer un pull:</p> <pre><code>$ git pull origin master\n</code></pre> <p>Esto ser\u00e1 especialmente \u00fatil si se trabaja en el proyecto o en los ejercicios desde los ordenadores del laboratorio y desde el ordenador personal. Cuando hayamos hecho cambios, antes de cerrar la sesi\u00f3n haremos commit y push, y cuando vayamos a empezar a trabajar en otra m\u00e1quina haremos un pull. De esta forma nos aseguramos de trabajar siempre con la versi\u00f3n actualizada de nuestros proyectos.</p>"},{"location":"habla-android.html","title":"S\u00edntesis y reconocimiento del habla","text":"<p>En esta sesi\u00f3n continuamos examinando las capacidades multimedia de Android presentando el sintetizador de voz Text to Speech, el cual permitir\u00e1 que una actividad reproduzca por los altavoces la lectura de un determinado texto. Se trata de un componente relativamente sencillo de utilizar que puede mejorar la accesibilidad de nuestras aplicaciones en gran medida.</p>"},{"location":"habla-android.html#sintetizador-de-voz-de-android","title":"Sintetizador de voz de Android","text":"<p>Android incorpora desde la versi\u00f3n 1.6 un motor de s\u00edntesis de voz conocido como Text to Speech. Mediante su API podremos hacer que nuestros programas \"lean\" un texto al usuario. Es necesario tener en cuenta que por motivos de espacio en disco los paquetes de lenguaje pueden no estar instalados en el dispositivo. Por lo tanto, antes de que nuestra aplicaci\u00f3n utilice Text to Speech se podr\u00eda considerar una buena pr\u00e1ctica de programaci\u00f3n el comprobar si dichos paquetes est\u00e1n instalados. Para ello podemos hacer uso de un <code>Intent</code> como el que se muestra a continuaci\u00f3n:</p> <pre><code>Intent intent = new Intent(TextToSpeech.Engine.ACTION_CHECK_TTS_DATA);\nstartActivityForResult(intent, TTS_DATA_CHECK);\n</code></pre> <p>El m\u00e9todo <code>onActivityResult()</code> recibir\u00e1 un <code>CHECK_VOICE_DATA_PASS</code> si todo est\u00e1 correctamente instalado. En caso contrario deberemos iniciar una nueva actividad por medio de un nuevo <code>Intent</code> impl\u00edcito que haga uso de la acci\u00f3n <code>ACTION_INSTALL_TTS_DATA</code> del motor Text to Speech.</p> <p>Una vez comprobemos que todo est\u00e1 instalado deberemos crear e inicializar una instancia de la clase <code>TextToSpeech</code>. Como no podemos utilizar dicha instancia hasta que est\u00e9 inicializada (la inicializaci\u00f3n se hace de forma as\u00edncrona), la mejor opci\u00f3n es pasar como par\u00e1metro al constructor un manejador <code>onInitListener</code> de tal forma que en dicho  m\u00e9todo se especifiquen las tareas a llevar a cabo por el sintetizador de voz una vez est\u00e9 inicializado.</p> <pre><code>boolean ttsIsInit = false;\nTextToSpeech tts = null;\n\ntts = new TextToSpeech(this, new OnInitListener() {\npublic void onInit(int status) {\nif (status == TextToSpeech.SUCCESS) {\nttsIsInit = true;\n// Hablar\n}\n}\n});\n</code></pre> <p>Una vez que la instancia est\u00e9 inicializada se puede utilizar el m\u00e9todo <code>speak</code> para sintetizar voz por medio del dispositivo de salida por defecto. El primer par\u00e1metro ser\u00e1 el texto a sintetizar y el segundo podr\u00e1 ser o bien <code>QUEUE_ADD</code>, que a\u00f1ade una nueva salida de voz a la cola, o bien <code>QUEUE_FLUSH</code>, que elimina todo lo que hubiera en la cola y lo sustituye por el nuevo texto.</p> <pre><code>tts.speak(\"Hello, Android\", TextToSpeech.QUEUE_ADD, null);\n</code></pre> <p>Otros m\u00e9todos de inter\u00e9s de la clase <code>TextToSpeech</code> son:</p> <ul> <li><code>setPitch</code> y <code>setSpeechRate</code> permiten modificar el tono de voz y la velocidad. Ambos m\u00e9todos aceptan un par\u00e1metro real.</li> <li><code>setLanguage</code> permite modificar la pronunciaci\u00f3n. Se le debe pasar como par\u00e1metro una instancia de la clase <code>Locale</code> para indicar el pa\u00eds y la lengua a utilizar.</li> <li>El m\u00e9todo <code>stop</code> se debe utilizar al terminar de hablar; este m\u00e9todo detiene la s\u00edntesis de voz.</li> <li>El m\u00e9todo <code>shutdown</code> permite liberar los recursos reservados por el motor de Text to Speech.</li> </ul> <p>El siguiente c\u00f3digo muestra un ejemplo en el que se comprueba si todo est\u00e1 correctamente instalado, se inicializa una nueva instancia de la clase <code>TextToSpeech</code>, y se utiliza dicha clase para decir una frase en espa\u00f1ol. Al llamar al m\u00e9todo <code>initTextToSpeech</code> se desencadenar\u00e1 todo el proceso.</p> <pre><code>private static int TTS_DATA_CHECK = 1;\nprivate TextToSpeech tts = null;\nprivate boolean ttsIsInit = false;\n\nprivate void initTextToSpeech() {\nIntent intent = new Intent(Engine.ACTION_CHECK_TTS_DATA);\nstartActivityForResult(intent, TTS_DATA_CHECK);\n}\n\nprotected void onActivityResult(int requestCode, int resultCode,\nIntent data) {\nif (requestCode == TTS_DATA_CHECK) {\nif (resultCode == Engine.CHECK_VOICE_DATA_PASS) {\ntts = new TextToSpeech(this, new OnInitListener() {\npublic void onInit(int status) {\nif (status == TextToSpeech.SUCCESS) {\nttsIsInit = true;\nLocale loc = new Locale(\"es\",\"\",\"\");\nif (tts.isLanguageAvailable(loc) &gt;=\nTextToSpeech.LANG_AVAILABLE)\ntts.setLanguage(loc);\ntts.setPitch(0.8f);\ntts.setSpeechRate(1.1f);\nspeak();\n}\n}\n});\n} else {\nIntent installVoice =\nnew Intent(Engine.ACTION_INSTALL_TTS_DATA);\nstartActivity(installIntent);\n}\n}\n}\n\nprivate void speak() {\nif (tts != null &amp;&amp; ttsIsInit) {\ntts.speak(\"Hola Android\", TextToSpeech.QUEUE_ADD, null);\n}\n}\n\n@Override\npublic void onDestroy() {\nsuper.onDestroy();\nif (tts != null) {\ntts.stop();\ntts.shutdown();\n}\n\nsuper.onDestroy();\n}\n</code></pre>"},{"location":"habla-android.html#reconocimiento-del-habla-en-android","title":"Reconocimiento del habla en Android","text":"<p>Otro sensor que podemos utilizar para introducir informaci\u00f3n en nuestras aplicaciones es el micr\u00f3fono que incorpora el dispositivo. Tanto el micr\u00f3fono como la c\u00e1mara se pueden utilizar para capturar audio y video. Una caracter\u00edstica altamente interesante de los dispositivos Android es que nos permiten realizar reconocimiento del habla de forma sencilla para introducir texto en nuestras aplicaciones.</p> <p>Para realizar este reconocimiento deberemos utilizar intents. Concretamente, crearemos un <code>Intent</code> mediante las constantes definidas en la clase <code>RecognizerIntent</code>, que es la clase principal que deberemos utilizar para utilizar esta caracter\u00edstica.</p> <p>Lo primer que deberemos hacer es crear un <code>Intent</code> para inicial el reconocimiento:</p> <pre><code>Intent intent = new Intent(\nRecognizerIntent.ACTION_RECOGNIZE_SPEECH);\n</code></pre> <p>Una vez creado, podemos a\u00f1adir una serie de par\u00e1metros para especificar la forma en la que se realizar\u00e1 el reconocimiento. Estos par\u00e1metros se introducen llamando a:</p> <pre><code>intent.putExtra(parametro, valor);\n</code></pre> <p>Los par\u00e1metros se definen como constantes de la clase <code>RecognizerIntent</code>, todas ellas tienen el prefijo <code>EXTRA_</code>. Algunos de estos par\u00e1metros son:</p> Par\u00e1metro Valor <code>EXTRA_LANGUAGE_MODEL</code> Obligatorio. Debemos especificar el tipo de lenguaje utilizado. Puede ser lenguaje orientado a realizar una b\u00fasqueda web (<code>LANGUAGE_MODEL_WEB_SEARCH</code>), o lenguaje de tipo general (<code>LANGUAGE_MODEL_FREE_FORM</code>). <code>EXTRA_LANGUAGE</code> Opcional. Se especifica para hacer el reconocimiento en un idioma diferente al idioma por defecto del dispositivo. Indicaremos el idioma mediante la etiqueta IETF correspondiente, como por ejemplo <code>\"es-ES\"</code> o <code>\"en-US\"</code> <code>EXTRA_PROMPT</code> Opcional. Nos permite indicar el texto a mostrar en la pantalla mientras se realiza el reconocimiento. Se especifica mediante una cadena de texto. <code>EXTRA_MAX_RESULTS</code> Opcional. Nos permite especificar el n\u00famero m\u00e1ximo de posibles resultados que queremos que nos devuelva. Se especifica mediante un n\u00famero entero. <p>Una vez creado el intent y especificados los par\u00e1metros, podemos lanzar el reconocimiento llamando, desde nuestra actividad, a:</p> <pre><code>startActivityForResult(intent, codigo);\n</code></pre> <p>Como c\u00f3digo deberemos especifica un entero que nos permita identificar la petici\u00f3n que estamos realizado. En la actividad deberemos definir el callback <code>onActivityResult</code>, que ser\u00e1 llamado cuando el reconocimiento haya finalizado. Aqu\u00ed deberemos comprobar en primer lugar que el c\u00f3digo de petici\u00f3n al que corresponde el callback es el que pusimos al lanzar la actividad. Una vez comprobado esto, obtendremos una lista con los resultados obtenidos de la siguiente forma:</p> <pre><code>@Override\nprotected void onActivityResult(int requestCode,\nint resultCode, Intent data) {\nif (requestCode == codigo &amp;&amp; resultCode == RESULT_OK) {\n\nArrayList&lt;String&gt; resultados =\ndata.getStringArrayListExtra(\nRecognizerIntent.EXTRA_RESULTS);\n\n// Utilizar los resultados obtenidos\n...\n}\nsuper.onActivityResult(requestCode, resultCode, data);\n}\n</code></pre>"},{"location":"habla-android.html#sintesis-de-voz-en-ios","title":"S\u00edntesis de voz en iOS","text":"<p>La s\u00edntesis de voz aparece a partir de iOS 7. Hasta entonces la \u00fanica forma de implementar s\u00edntesis de voz era utilizar la caracter\u00edstica de accesibilidad Voice Over, lo cual forzaba a tener que tener activada esta caracter\u00edstica.</p> <p>A partir e iOS 7 aparece la clase <code>AVSpeechSynthesizer</code> que nos permite sintetizar voz en cualquier aplicaci\u00f3n.</p> <p>En primer lugar deberemos establecer el texto a vocalizar mediante un objeto de tipo <code>AVSpeechUtterance</code>:</p> <pre><code>AVSpeechUtterance *utterance = [AVSpeechUtterance speechUtteranceWithString: @\"Hola mundo\"];\n</code></pre> <p>Este objeto nos permite tambi\u00e9n especificar otras propiedades de la locuci\u00f3n, como el pitch o la voz a utilizar.</p> <p>Una vez definido y configurado el utterance, podremos reproducirlo con:</p> <pre><code>AVSpeechSynthesizer *synthesizer = [[AVSpeechSynthesizer alloc] init];\n\n[synthesizer speakUtterance: utterance];\n</code></pre> <p>A trav\u00e9s de este objeto podremos pausar o reanudar la locuci\u00f3n, o conocer si actualmente est\u00e1 reproduci\u00e9ndose.</p>"},{"location":"habla-android.html#reconocimiento-del-habla-en-ios","title":"Reconocimiento del habla en iOS","text":"<p>Los dispositivos iOS cuentan con el asistente Siri que utiliza reconocimiento de voz para realizar diferentes operaciones. Estas operaciones se realizan a nivel del Sistema Operativo, y nos permiten utilizar diferentes servicios que proporciona la plataforma, como por ejemplo hacer una llamada, leer los mensajes, o consultar el tiempo que hace.</p> <p>A partir de iOS 10 aparece SiriKit, que nos permite integrar las funcionalidades que ofrecen nuestras aplicaciones en este entorno. De esta forma, trav\u00e9s del asistente Siri podremos utilizar comandos que lanzar\u00e1n determinadas funcionalidades de nuestra aplicaci\u00f3n.</p> <p>Si queremos tener un mayor control sobre el reconocimiento de voz dentro de nuestra aplicaci\u00f3n, podemos encontrar APIs de terceros que nos proporcionan dicha funcionalidad, como por ejemplo las siguientes:</p> <ul> <li>SpeechKit</li> </ul> <p>http://developer.nuance.com/</p> <ul> <li>MindMeld</li> </ul> <p>https://expectlabs.com/docs/sdks/ios/gettingStarted</p> <ul> <li>OpenEars</li> </ul> <p>http://www.politepix.com/openears/</p>"},{"location":"habla-android.html#ejercicios","title":"Ejercicios","text":""},{"location":"habla-android.html#sintesis-de-voz-con-text-to-speech","title":"S\u00edntesis de voz con Text to Speech","text":"<p>En este primer ejercicio vamos a utilizar el motor Text to Speech para crear una aplicaci\u00f3n que lea el texto contenido en un <code>EditText</code> de la actividad principal. Para ello el primer paso ser\u00e1 descargar de las plantillas la aplicaci\u00f3n SintesisVoz. La aplicaci\u00f3n contiene una \u00fanica actividad. La idea es que al pulsar el bot\u00f3n Leer se lea el texto en el cuadro de edici\u00f3n. Existen dos botones de radio para escoger la pronunciaci\u00f3n (ingl\u00e9s o espa\u00f1ol).</p> <p>Deberemos seguir los siguientes pasos:</p> <ul> <li>Inserta el c\u00f3digo necesario en el m\u00e9todo <code>initTextToSpeech</code> para que se lance un <code>Intent</code> impl\u00edcito para comprobar si el motor Text to Speech est\u00e1 instalado en el sistema:</li> </ul> <pre><code>Intent intent = new Intent(Engine.ACTION_CHECK_TTS_DATA);\nstartActivityForResult(intent, TTS_DATA_CHECK);\n</code></pre> <ul> <li>En el manejador <code>onActivityResult</code> incorporamos el c\u00f3digo necesario para inicializar el motor Text to Speech en el caso en el que est\u00e9 instalado, o para instalarlo en el caso en el que no lo estuviera.</li> </ul> <pre><code>if (requestCode == TTS_DATA_CHECK) {\nif (resultCode == Engine.CHECK_VOICE_DATA_PASS) {\ntts = new TextToSpeech(this, new OnInitListener() {\npublic void onInit(int status) {\nif (status == TextToSpeech.SUCCESS) {\nttsIsInit = true;\nLocale loc = new Locale(\"es\",\"\",\"\");\nif (tts.isLanguageAvailable(loc)\n&gt;= TextToSpeech.LANG_AVAILABLE)\ntts.setLanguage(loc);\ntts.setPitch(0.8f);\ntts.setSpeechRate(1.1f);\n}\n}\n});\n} else {\nIntent installVoice = new Intent(Engine.ACTION_INSTALL_TTS_DATA);\nstartActivity(installVoice);\n}\n}\n</code></pre> <p>En el c\u00f3digo anterior <code>tts</code> es un objeto de la clase <code>TextToSpeech</code> que ya est\u00e1 definido en la plantilla. La variable booleana <code>ttsIsInit</code> tendr\u00e1 valor <code>true</code> en el caso en el que el motor de s\u00edntesis de voz se haya inicializado correctamente. La utilizaremos m\u00e1s adelante para comprobar si se puede leer o no un texto. Mediante el objeto <code>loc</code> inicializamos el idioma a espa\u00f1ol, ya que es el bot\u00f3n de radio seleccionado por defecto al iniciar la actividad.</p> <ul> <li>A\u00f1ade el c\u00f3digo necesario en el m\u00e9todo <code>onDestroy</code> para liberar los recursos asociados a la instancia de Text to Speech cuando la actividad vaya a ser destruida:</li> </ul> <pre><code>if (tts != null) {\ntts.stop();\ntts.shutdown();\n}\n</code></pre> <ul> <li>El manejador del click del bot\u00f3n Leer simplemente llama al m\u00e9todo <code>speak</code>, que ser\u00e1 el encargado de utilizar el objeto <code>TextToSpeech</code> para leer el texto en la vista <code>EditText</code>. Introduce el c\u00f3digo necesario para hacer esto; no olvides de comprobar si el motor Text to Speech est\u00e1 inicializado por medio de la variable booleana <code>ttsIsInit</code>.</li> <li>Por \u00faltimo a\u00f1ade el c\u00f3digo necesario a los manejadores del click de los botones de radio para que se cambie el idioma a espa\u00f1ol o ingl\u00e9s seg\u00fan corresponda. Observa c\u00f3mo se usa la clase <code>Locale</code> en <code>onActivityResult</code> para hacer exactamente lo mismo.</li> </ul>"},{"location":"mirroring.html","title":"Reproducci\u00f3n en dispositivos externos","text":"<p>Existen diferentes sistemas que nos permiten reproducir los contenidos multimedia de nuestros dispositivos en dispositivos externos, tales como pantallas de TV u ordenadores. Encontramos diferentes alternativas, como Chromecast, Miracast y Airplay.</p> <p>Airplay es un protocolo propietario de Apple que en principio se dise\u00f1\u00f3 para transmitir contenidos multimedia desde iPhone o iPad al dispositivo Apple TV, aunque actualmente puede utilizarse para enviar contenidos entre diferentes tipos de dispositivos conectados a la misma red.</p> <p>Chromecast es un dispositivo fabricado por Google que se conecta al puerto HDMI del televisor y nos permite reproducir en \u00e9l contenidos multimedia enviados desde distintos tipos de dispositivos m\u00f3viles (Android e iOS).</p> <p>Encontramos tambi\u00e9n otros protocolos para esta misma funcionalidad, como por ejemplo Miracast, que est\u00e1 presente en determinados televisores. Tambi\u00e9n podemos encontrar dispositivos HDMI que soportan todos los protocolos anteriores.</p> <p>En cualquiera de estos casos, estas tecnolog\u00edas nos permitir\u00e1n visualizar en otras pantallas las fotos o v\u00eddeos almacenados en el m\u00f3vil, reproducir audio, o incluso mostrar lo mismo que se est\u00e9 visualizando en la pantalla del m\u00f3vil (mirroring).</p> <p>Existen dos formas b\u00e1sicas de operar:</p> <ul> <li>Reproducci\u00f3n remota: El dispositivo externo funciona de forma independiente. Nosotros simplemente le indicamos lo que debe reproducir desde el m\u00f3vil, y \u00e9l se encarga de gestionar el acceso y la reproducci\u00f3n del medio. As\u00ed puede funcionar por ejemplo la aplicaci\u00f3n You Tube con pantallas externas configuradas. Env\u00eda al dispositivo externo la indicaci\u00f3n del v\u00eddeo que queremos reproducir, y el dispositivo se encarga de conectarse y reproducir el v\u00eddeo.</li> <li>Retransmisi\u00f3n de medios: El dispositivo m\u00f3vil se encarga de retransmitir v\u00eda streaming los contenidos al dispositivo remoto secundario. Este dispositivo simplemente reproducir\u00e1 lo que est\u00e9 recibiendo desde el m\u00f3vil, sin autonom\u00eda propia.</li> </ul> <p>Vamos a ver a continuaci\u00f3n c\u00f3mo preparar nuestras aplicaciones para que soporten estas tecnolog\u00edas. Hablaremos de enrutado de medios para referirnos al camino de salida que tomar\u00e1n para su reproducci\u00f3n (altavoz o pantalla del m\u00f3vil, auriculares, pantalla externa, etc).</p>"},{"location":"mirroring.html#enrutado-de-medios-en-android","title":"Enrutado de medios en Android","text":"<p>En Android podremos definir la forma en la que realizar el enrutado de medios mediante la clase <code>MediaRouter</code>. Las aplicaciones enrutar\u00e1n los medios a trav\u00e9s de un objeto <code>MediaRouter</code>, y \u00e9ste se conectar\u00e1 con un <code>MediaRouteProvider</code> (que puede representar un dispositivos Chromecast, Miracast, etc).</p>"},{"location":"mirroring.html#seleccion-de-ruta","title":"Selecci\u00f3n de ruta","text":"<p>Para utilizar esta API deberemos utilizar la librer\u00eda de compatibilidad v7. Las aplicaciones que utilicen enrutado de medios deber\u00e1n incluir en el Action Bar un bot\u00f3n para seleccionar la ruta de salida. Para ello incluiremos el siguiente item en el men\u00fa, indicando que deber\u00e1 aparecer siempre en la barra como acci\u00f3n:</p> <pre><code>&lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt;\n&lt;menu xmlns:android=\"http://schemas.android.com/apk/res/android\"\nxmlns:app=\"http://schemas.android.com/apk/res-auto\"\n&gt;\n\n&lt;item android:id=\"@+id/media_route_menu_item\"\nandroid:title=\"@string/media_route_menu_title\"\napp:actionProviderClass=\"android.support.v7.app.MediaRouteActionProvider\"\napp:showAsAction=\"always\"\n/&gt;\n&lt;/menu&gt;\n</code></pre> <p>Este bot\u00f3n puede tambi\u00e9n ser a\u00f1adido de forma alternativa mediante un objeto <code>MediaRouteButton</code>.</p> <p>Una vez definido el bot\u00f3n, deberemos hacer que al pulsar sobre \u00e9l se muestre un cuadro selector de ruta. Para ello deberemos crear un objeto <code>MediaRouteSelector</code> en el que deberemos indicar el tipo de dispositivos con los que podemos conectar:</p> <pre><code>mSelector = new MediaRouteSelector.Builder()\n.addControlCategory(MediaControlIntent.CATEGORY_LIVE_AUDIO)\n.addControlCategory(MediaControlIntent.CATEGORY_LIVE_VIDEO)\n.addControlCategory(MediaControlIntent.CATEGORY_REMOTE_PLAYBACK)\n.build();\n</code></pre> <p>Esto podemos hacerlo por ejemplo en el m\u00e9todo <code>onCreate</code>.</p> <p>Las posibles categor\u00edas de dispositivos externos son:</p> <ul> <li><code>CATEGORY_LIVE_AUDIO</code>: Sistemas de sonido que reciben el audio v\u00eda streaming</li> <li><code>CATEGORY_LIVE_VIDEO</code>: Sistemas de v\u00eddeo que reciben el contenido v\u00eda streaming</li> <li><code>CATEGORY_REMOTE_PLAYBACK</code>: Dispositivos que pueden realizar la reproducci\u00f3n de forma aut\u00f3noma.</li> </ul> <p>Por \u00faltimo, deberemos a\u00f1adir este selector al bot\u00f3n del men\u00fa:</p> <pre><code>MediaRouteSelector mSelector;\n\n@Override\npublic boolean onCreateOptionsMenu(Menu menu) {\nsuper.onCreateOptionsMenu(menu);\n\ngetMenuInflater().inflate(R.menu.sample_media_router_menu, menu);\n\nMenuItem mediaRouteMenuItem = menu.findItem(R.id.media_route_menu_item);\nMediaRouteActionProvider mediaRouteActionProvider =\n(MediaRouteActionProvider)MenuItemCompat.getActionProvider(\nmediaRouteMenuItem);\nmediaRouteActionProvider.setRouteSelector(mSelector);\n\nreturn true;\n}\n</code></pre> <p>Para que el bot\u00f3n de selecci\u00f3n de ruta se muestre deberemos tener un dispositivo compatible conectado en nuestra red.</p>"},{"location":"mirroring.html#conexion-con-la-ruta-seleccionada","title":"Conexi\u00f3n con la ruta seleccionada","text":"<p>Para conectar con una ruta en primer lugar debemos crear un objeto <code>MediaRouter</code> que es un objeto singleton que debe ser retenido por nuestra actividad:</p> <pre><code>private MediaRouter mMediaRouter;\n\n@Override\nprotected void onCreate(Bundle savedInstanceState) {\nsuper.onCreate(savedInstanceState);\nsetContentView(R.layout.activity_main);\n\nmMediaRouter = MediaRouter.getInstance(this);\n}\n</code></pre> <p>Necesitaremos proporcionar a este objeto un callback de tipo <code>MediaRouter.Callback</code> para responder ante los eventos de enrutamiento. Al menos deberemos definir los siguientes m\u00e9todos en el callback:</p> <pre><code>private final MediaRouter.Callback mMediaRouterCallback =\nnew MediaRouter.Callback() {\n\n@Override\npublic void onRouteSelected(MediaRouter router, RouteInfo route) {\n// Se establece la conexi\u00f3n a un dispositivo de salida\n}\n\n@Override\npublic void onRouteUnselected(MediaRouter router, RouteInfo route) {\n// Se ha desconectado de un dispositivo de salida\n}\n\n@Override\npublic void onRoutePresentationDisplayChanged(\nMediaRouter router, RouteInfo route) {\n// Hay un cambio de display en la nueva salida\n// (por ejemplo distinto tama\u00f1o de pantalla)\n}\n}\n</code></pre> <p>Podemos a\u00f1adir el callback en <code>onStart</code> y eliminarlo en <code>onStop</code>:</p> <pre><code>@Override\npublic void onStart() {\nmMediaRouter.addCallback(mSelector, mMediaRouterCallback,\nMediaRouter.CALLBACK_FLAG_REQUEST_DISCOVERY);\nsuper.onStart();\n}\n\n@Override\npublic void onStop() {\nmMediaRouter.removeCallback(mMediaRouterCallback);\nsuper.onStop();\n}\n</code></pre>"},{"location":"mirroring.html#reproduccion-remota","title":"Reproducci\u00f3n remota","text":"<p>Si queremos utilizar el enfoque de reproducci\u00f3n remota, en primer lugar deberemos comprobar que el dispositivo externo soporta este modo de funcionamiento con:</p> <pre><code>route.supportsControlCategory(\nMediaControlIntent.CATEGORY_REMOTE_PLAYBACK)\n</code></pre> <p>Esto lo podemos hacer desde cualquiera de los m\u00e9todos del callback. En caso de no soportarlo deberemos utilizar en modo de retransmisi\u00f3n desde el dispositivo.</p> <p>Una vez comprobado que tenemos soporte, podemos enviar comandos al dispositivo externo para controlar la reproducci\u00f3n (pausar, reanudar, parar, cambiar el volumen, etc).</p> <p>Podemos enviar el comando para que el dispositivo remoto inicie la reproducci\u00f3n de un v\u00eddeo en internet de la siguiente forma:</p> <pre><code>RouteInfo mRoute;\nRemotePlaybackClient mRemotePlaybackClient;\n\nprivate void reproducirRemoto(RouteInfo route) {\n\n// Si hab\u00eda una ruta anterior, antes la liberamos\nif (mRoute != null &amp;&amp; mRemotePlaybackClient != null) {\nmRemotePlaybackClient.release();\nmRemotePlaybackClient = null;\n}\n\nmRoute = route;\nmRemotePlaybackClient = new RemotePlaybackClient(this, mRoute);\n\n// Env\u00eda el comando de reproducci\u00f3n\nmRemotePlaybackClient.play(Uri.parse(\n\"http://jtech.ua.es/dadm/video.mp4\"),\n\"video/mp4\", null, 0, null, new ItemActionCallback() {\n\n@Override\npublic void onResult(Bundle data, String sessionId,\nMediaSessionStatus sessionStatus,\nString itemId, MediaItemStatus itemStatus) {\n}\n\n@Override\npublic void onError(String error, int code, Bundle data) {\n}\n});\n}\n}\n</code></pre> <p>Podemos utilizar el objeto <code>RemotePlaybackClient</code> para enviar diferentes comandos al dispositivo externo:</p> <ul> <li><code>play()</code></li> <li><code>pause()</code></li> <li><code>resume()</code></li> <li><code>seek()</code></li> <li><code>release()</code></li> </ul>"},{"location":"mirroring.html#emision-a-un-dispositivo-secundario","title":"Emisi\u00f3n a un dispositivo secundario","text":"<p>Si el modo de reproducci\u00f3n remota no est\u00e1 disponible, podemos emitir directamente el medio desde nuestro dispositivo m\u00f3vil. De esta forma el m\u00f3vil se encarga de la reproducci\u00f3n, y env\u00eda el contenido al dispositivo externo.</p> <p>Para ello necesitamos un objeto de tipo <code>Presentation</code>, que define un segundo display donde mostraremos todo lo que queramos enviar al dispositivo remoto. Este objeto tiene su propio contexto y podemos incluir en el un layout, como si de una actividad se tratase. Podemos por ejemplo a\u00f1adir en \u00e9l un <code>SurfaceView</code>:</p> <pre><code>public class PantallaSecundaria extends Presentation {\npublic PantallaSecundaria(Context outerContext, Display display) {\nsuper(outerContext, display);\n}\n\n@Override\nprotected void onCreate(Bundle savedInstanceState) {\nsuper.onCreate(savedInstanceState);\nResources resources = getContext().getResources();\n\nsetContentView(R.layout.pantalla_secundaria);\n\nmSurfaceView = (GLSurfaceView)findViewById(R.id.surface_view);\n...\n}\n}\n</code></pre> <p>Podemos mostrar la presentaci\u00f3n en la ruta establecida con:</p> <pre><code>private void emitirContenido(RouteInfo route) {\n\n// Obtiene el display de la ruta seleccionada\nDisplay selectedDisplay = null;\nif (route != null) {\nselectedDisplay = route.getPresentationDisplay();\n}\n\n// Cierra la presentacion actual si existe y tiene un display distinto al seleccionado\nif (mPresentation != null &amp;&amp; mPresentation.getDisplay() != selectedDisplay) {\nmPresentation.dismiss();\nmPresentation = null;\n}\n\n// Muestra la presentaci\u00f3n en el display seleccionado, si no lo estabamos haciendo ya\nif (mPresentation == null &amp;&amp; selectedDisplay != null) {\nmPresentation = new PantallaSecundaria(this, selectedDisplay);\nmPresentation.setOnDismissListener(\nnew DialogInterface.OnDismissListener() {\n@Override\npublic void onDismiss(DialogInterface dialog) {\nif (dialog == mPresentation) {\nmPresentation = null;\n}\n}\n});\n\ntry {\nmPresentation.show();\n} catch (WindowManager.InvalidDisplayException ex) {\nmPresentation = null;\n}\n}\n}\n</code></pre> <p>Podemos utilizar el callback del <code>MediaRouter</code> para comprobar el modo de emisi\u00f3n soportado e iniciar la reproducci\u00f3n por el m\u00e9todo adecuado:</p> <pre><code>private final MediaRouter.Callback mMediaRouterCallback =\nnew MediaRouter.Callback() {\n\n@Override\npublic void onRouteSelected(MediaRouter router, RouteInfo route) {\nif (route.supportsControlCategory(\nMediaControlIntent.CATEGORY_REMOTE_PLAYBACK)){\nreproducirRemoto(route);\n} else {\nemitirContenido(route);\n}\n}\n\n@Override\npublic void onRouteUnselected(MediaRouter router, RouteInfo route) {\n}\n\n@Override\npublic void onRoutePresentationDisplayChanged(\nMediaRouter router, RouteInfo route) {\n}\n}\n</code></pre>"},{"location":"mirroring.html#airplay-en-ios","title":"AirPlay en iOS","text":"<p>En iOS el contenido reproducido por AVFoundation tiene por defecto soporte para AirPlay, y se enruta autom\u00e1ticamente al dispositivo externo seleccionado actualmente.</p> <p>Para hacer posible la reproducci\u00f3n por AirPlay deberemos tener activado el modo Audio, AirPlay and Picture in Picture en Background Modes, desde la pesta\u00f1a Capabilities de nuestro target, como vimos anteriormente. </p> <p>Esto ser\u00e1 suficiente cuando lo que necesitemos sea emitir v\u00eddeo o audio al dispositivo externo, pero tambi\u00e9n podremos utilizar AirPlay para mostrar contenido propio en una segunda pantalla. Veremos a continuaci\u00f3n con m\u00e1s detalle estas dos formas de trabajar con AirPlay.</p>"},{"location":"mirroring.html#seleccion-de-enrutamiento-con-airplay","title":"Selecci\u00f3n de enrutamiento con AirPlay","text":"<p>Podemos enviar mediante AirPlay audio o v\u00eddeo almacenado localmente en el dispositivo, o al que accedemos de forma remota mediante HTTP Live Streaming o descarga progresiva, siendo HTTP Live Streaming el sistema preferido. En cualquier caso, siempre se debe acceder al contenido mediante HTTP. El audio y v\u00eddeo debe estar codificado en formatos compatibles con AirPlay: v\u00eddeo H.264 y audio AAC o MP3.</p> <p>AirPlay est\u00e1 por defecto activado cuando reproducimos medios con MoviePlayer o con AVFoundation. Para poder seleccionar el dispositivo de salida del audio o el v\u00eddeo podemos introducir un bot\u00f3n predefinido que nos mostrar\u00e1 la lista de posibles dispositivos. Esto lo haremos creando un objeto de tipo <code>MPVolumeView</code> y a\u00f1adi\u00e9ndolo a la vista donde queremos mostrarlo:</p> <pre><code>MPVolumeView *volumeView = [ [MPVolumeView alloc] init];\n[view addSubview: volumeView];\n</code></pre> <p>Este componente mostrar\u00e1 un control de volumen y adem\u00e1s el bot\u00f3n para seleccionar la salida. Si s\u00f3lo queremos mostrar el bot\u00f3n podemos ocultar la barra de volumen de la siguiente forma:</p> <pre><code>MPVolumeView *volumeView = [ [MPVolumeView alloc] init];\n[volumeView setShowsVolumeSlider:NO];\n[volumeView sizeToFit];\n[view addSubview: volumeView];\n</code></pre> <p>Tambi\u00e9n es posible a\u00f1adir el bot\u00f3n anterior como bot\u00f3n de la barra de navegaci\u00f3n o de herramientas, de la siguiente forma:</p> <pre><code>MPVolumeView *volumeView = [ [MPVolumeView alloc] init];\n[volumeView setShowsVolumeSlider:NO];\n[volumeView sizeToFit];\n\nUIBarButtonItem *barButton = [[UIBarButtonItem alloc] initWithCustomView:volumeView];\nself.navigationItem.rightBarButtonItem = barButton;\n</code></pre>"},{"location":"mirroring.html#airplay-y-reproduccion-de-video","title":"AirPlay y reproducci\u00f3n de v\u00eddeo","text":"<p>La emisi\u00f3n de v\u00eddeo por AirPlay estar\u00e1 activa por defecto tanto cuando utilicemos <code>AVPlayer</code> como <code>MPMoviePlayerController</code>. Podemos desactivarlo o volverlo a activar con las siguientes propiedades:</p> <pre><code>AVPlayer *player;\nMPMoviePlayerController moviePlayerController;\n\n...\n\nmoviePlayerController.allowsAirPlay = NO;\nplayer.allowsAirPlayVideo = NO;\n</code></pre>"},{"location":"mirroring.html#pantalla-secundaria","title":"Pantalla secundaria","text":"<p>Vamos a ver ahora c\u00f3mo manejar pantallas secundarias. Cuando tengamos nuestro dispositivo conectado a una pantalla secundaria por AirPlay, podremos o bien mostrar lo mismo que estamos mostrando en el dispositivo (mirroring), o contenido distinto. Por defecto tendremos el mirroring activado cuando nos conectemos por AirPlay a una pantalla secundaria. Vamos a estudiar c\u00f3mo enviar un contenido diferente.</p> <p>Para que el contenido en la pantalla secundaria sea distinto al de la pantalla del m\u00f3vil deberemos crearnos un objeto de tipo <code>UIWindow</code> para ella:</p> <pre><code>@property (nonatomic, strong) UIWindow *ventanaSecundaria;\n</code></pre> <p>Comprobaremos si hay m\u00e1s de una pantalla, y en tal caso crearemos nuestra nueva ventana (<code>UIWindow</code>) a partir de ella:</p> <pre><code>if ([[UIScreen screens] count] &gt; 1)\n{\nUIScreen *pantallaSecundaria = [[UIScreen screens] objectAtIndex:1];\n\nself.ventanaSecundaria = [[UIWindow alloc] initWithFrame:pantallaSecundaria.bounds];\nself.ventanaSecundaria.screen = pantallaSecundaria;\n\nself.ventanaSecundaria.hidden = NO;\n}\n</code></pre> <p>Es importante asociar la nueva ventana a la pantalla secundaria antes de mostrarla</p> <p>Con esto ya se mostrar\u00e1 el contenido que a\u00f1adamos a dicha ventana en nuestro dispositivo externo. Podremos volver a activar el mirroring simplemente eliminando la ventana secundaria de la pantalla secundaria.</p>"},{"location":"mirroring.html#eventos-de-la-pantalla-secundaria","title":"Eventos de la pantalla secundaria","text":"<p>Es importante estar al tanto de los eventos de conexi\u00f3n y desconexi\u00f3n de pantallas secundarias. Esto podemos hacerlo escuchando las siguientes notificaciones:</p> <pre><code>NSNotificationCenter *center = [NSNotificationCenter defaultCenter];\n[center addObserver:self selector:@selector(pantallaConectada:)\nname:UIScreenDidConnectNotification object:nil];\n[center addObserver:self selector:@selector(pantallaDesconectada:)\nname:UIScreenDidDisconnectNotification object:nil];\n}\n</code></pre> <p>Podemos crear la ventana secundaria cuando se conecte una segunda pantalla, y eliminarla cuando se desconecte:</p> <pre><code>- (void)pantallaConectada:(NSNotification*)aNotification\n{\nUIScreen *pantallaSecundaria = [aNotification object];\n\nif (!self.ventanaSecundaria)\n{\nself.ventanaSecundaria = [[UIWindow alloc] initWithFrame:pantallaSecundaria.bounds];\nself.ventanaSecundaria.screen = pantallaSecundaria;\n}\n}\n\n- (void)pantallaDesconectada:(NSNotification*)aNotification\n{\nif (self.ventanaSecundaria)\n{\nself.ventanaSecundaria.hidden = YES;\nself.ventanaSecundaria = nil;\n}\n}\n</code></pre>"},{"location":"mirroring.html#aplicaciones-para-televisores","title":"Aplicaciones para televisores","text":"<p>Adem\u00e1s de poder emitir medios desde los dispositivos m\u00f3viles a dispositivos externos, tambi\u00e9n podemos crear aplicaciones que se ejecuten de forma directa en la TV. Para ello contamos con dispositivos como Apple TV, con el sistema operativo tvOS, o los diferentes dispositivos que implementan Android TV. Estos sistemas a\u00f1aden a las API que ya conocemos de iOS y Android respectivamente funcionalidades adicionales dise\u00f1adas de forma espec\u00edfica para pantallas de televisor. </p> <p>Vamos a ver a continuaci\u00f3n una introducci\u00f3n a estos sistemas tvOS y Android TV.</p>"},{"location":"mirroring.html#apple-tv-y-tvos","title":"Apple TV y tvOS","text":"<p>El sistema operativo tvOS deriva de iOS, pero se trata de un operativo diferente. Veremos que el desarrollo ser\u00e1 muy similar al que ya conocemos de iOS, podemos desarrollar una aplicaci\u00f3n tvOS con Xcode, e incluso una misma aplicaci\u00f3n puede destinarse a iOS y tvOS creando dos targets distintos, y as\u00ed reutilizar todas las clases que sean comunes. Una aplicaci\u00f3n iOS se podr\u00e1 normalmente ofrecer en tvOS sin tener que hacer cambios, simplemente a\u00f1adiendo el target correspondiente. La principal diferencia entre estos dos sistemas es que tvOS a\u00f1ade algunos nuevos frameworks destinados a crear aplicaciones para TV.</p> <p>Los principales frameworks adicionales que encontramos en tvOS son:</p> <ul> <li>TVMLKit: Permite utilizar un lenguaje de marcado propio (TVML) y Javascript para crear la interfaz de las aplicaciones para TV. Este framework nos proporciona el puente entre el c\u00f3digo nativo de la aplicaci\u00f3n y el c\u00f3digo Javascript de la interfaz.</li> <li>TVMLJS: Describe las APIs Javascript utilizadas en la interfaz TVML.</li> <li>TVServices: Permite a\u00f1adir una extensi\u00f3n a la app para que cuando la seleccionemos en el men\u00fa de aplicaciones, nos aparezca en pantalla una previsualizaci\u00f3n de informaci\u00f3n destacada de la aplicaci\u00f3n (por ejemplo contenidos recomendados). </li> </ul> <p>Algunas de las principales cuestiones a tener en cuenta cuando dise\u00f1emos una aplicaci\u00f3n tvOS son:</p> <ul> <li>La interfaz ya no es t\u00e1ctil. Normalmente manejaremos el dispositivo con el mando a distancia (Siri Remote) o con un mando de videojuegos. </li> <li>No es un dispositivo de uso personal. Los dispositivos m\u00f3viles normalmente son personales, pero la TV es familiar. Deberemos tener esto en cuenta, por ejemplo creando diferentes perfiles como hacen por ejemplo aplicaciones como Netflix.</li> <li>El almacenamiento es limitado. Las aplicaciones pueden ocupar un m\u00e1ximo de 200Mb, y pueden almacenar un m\u00e1ximo de 500Kb de datos persistentes. Para poder tener aplicaciones m\u00e1s grandes, podemos utilizar recursos bajo demanda en la App Store, es decir, paquetes de la aplicaci\u00f3n que se descargar\u00e1n cuando se necesiten (por ejemplo, niveles adicionales de un juego). Para el almacenamiento de informaci\u00f3n, se puede almacenar en el directorio de cach\u00e9 (se puede borrar de una ejecuci\u00f3n a la siguiente), o en iCloud.</li> </ul>"},{"location":"mirroring.html#android-tv","title":"Android TV","text":"<p>Adem\u00e1s de aplicaciones para dispositivos vestibles, Android tambi\u00e9n nos permite crear aplicaciones para dispositivos de TV.</p> <p>Las aplicaciones Android para televisores se crean utilizando la API 21 (Android 5.0). Se pueden utilizar adem\u00e1s de forma opcional librer\u00edas de soporte para TV, como la librer\u00eda v17 (leanback), que depende de las librer\u00edas v4 y v7, y nos proporciona elementos y estilos orientados a este tipo de pantallas.</p> <p>Una actividad se marca como destinada a televisores mediante la categor\u00eda <code>LEANBACK_LAUNCHER</code>:</p> <pre><code>&lt;activity\nandroid:name=\"es.ua.eps.MainActivity\"\nandroid:label=\"@string/app_name\" &gt;\n\n&lt;intent-filter&gt;\n&lt;action android:name=\"android.intent.action.MAIN\" /&gt;\n&lt;category android:name=\"android.intent.category.LEANBACK_LAUNCHER\" /&gt;\n&lt;/intent-filter&gt;\n&lt;/activity&gt;\n</code></pre> <p>Los televisores carecen de gran parte del hardware con el que contamos en dispositivos m\u00f3viles. Si queremos hacer una aplicaci\u00f3n que sirva tanto para tel\u00e9fonos m\u00f3viles como para televisores, deberemos declarar que dichas caracter\u00edsticas no son requeridas (pantalla t\u00e1ctil, c\u00e1mara, tel\u00e9fono, etc) para que de esta manera pueda instalarse en ambos tipos de dispositivos:</p> <pre><code>&lt;uses-feature android:name=\"android.hardware.touchscreen\"\nandroid:required=\"false\" /&gt;\n&lt;uses-feature android:name=\"android.hardware.telephony\"\nandroid:required=\"false\"/&gt;\n&lt;uses-feature android:name=\"android.hardware.camera\"\nandroid:required=\"false\"/&gt;\n&lt;uses-feature android:name=\"android.hardware.nfc\"\nandroid:required=\"false\"/&gt;\n&lt;uses-feature android:name=\"android.hardware.gps\"\nandroid:required=\"false\"/&gt;\n&lt;uses-feature android:name=\"android.hardware.microphone\"\nandroid:required=\"false\"/&gt;\n</code></pre> <p>Deberemos comprobar en c\u00f3digo si dichas caracter\u00edsticas est\u00e1n disponibles antes de intentar utilizarlas, para as\u00ed conseguir aplicaciones compatibles con distintos tipos de dispositivos.</p>"},{"location":"mirroring.html#navegacion","title":"Navegaci\u00f3n","text":"<p>La aplicaci\u00f3n para TV deber\u00e1 manejarse mediante un pad direccional (un mando de videojuegos o un mando a distancia). Para ello es importante que en los botones proporcionemos diferentes aspectos seg\u00fan si est\u00e1 seleccionado, pulsado, o no pulsado, y que en el layout especifiquemos de forma correcta el orden en el que recibir\u00e1n el foco los distintos elementos de la interfaz..</p>"},{"location":"mirroring.html#estilo","title":"Estilo","text":"<p>El aspecto est\u00e1ndar recomendado para las aplicaciones de TV es leanback, perteneciente a la librer\u00eda v17:</p> <pre><code>&lt;activity\nandroid:name=\"es.ua.eps.MiTVActividad\"\nandroid:label=\"@string/app_name\"\nandroid:theme=\"@style/Theme.Leanback\"&gt;\n</code></pre> <p>En cualquier caso, ser\u00e1 importante que el tema que utilicemos no tenga barra de t\u00edtulo. Si queremos utilizar otro estilo como tema, deberemos siempre utilizar uno que no tenga barra de t\u00edtulo ni action bar:</p> <pre><code>&lt;activity\nandroid:name=\"es.ua.eps.MiTVActividad\"\nandroid:label=\"@string/app_name\"\nandroid:theme=\"@android:style/Theme.NoTitleBar\"&gt;\n</code></pre>"},{"location":"mirroring.html#contenido","title":"Contenido","text":"<p>Los principales usos de las aplicaciones de TV son los videojuegos y la reproducci\u00f3n de medios. Dentro de la librer\u00eda v17 encontramos diferentes componentes orientados a las aplicaciones de TV, como por ejemplo browsers para el cat\u00e1logo de medios y fichas para mostrar los metadatos de los medios en reproducci\u00f3n.</p> <p>Para el browser de medios se puede utilizar el tipo de fragmento <code>BrowseFragment</code>, que nos mostrar\u00e1 la lista de contenidos, y para ver los detalles de cada contenido tenemos <code>DetailsFragment</code>.</p> <p>Podemos tambi\u00e9n mostrar una ficha con los datos de la reproducci\u00f3n actual mediente el objeto <code>MediaSession</code>. Con su m\u00e9todo <code>setMetadata()</code> podremos introducir los metadatos del medio en reproducci\u00f3n y as\u00ed aparecer\u00e1n en la ficha de reproducci\u00f3n actual.</p>"},{"location":"procesamiento-android.html","title":"Emisi\u00f3n de medios","text":"<p>Vamos a estudiar en esta sesi\u00f3n la forma en la que podemos emitir v\u00eddeo desde los dispositivos m\u00f3viles a trav\u00e9s de Internet o a otros dispositivos.</p>"},{"location":"procesamiento-android.html#publicacion-en-wowza","title":"Publicaci\u00f3n en Wowza","text":"<p>El servidor Wowza, adem\u00e1s de publicar contenido bajo demanda (VOD), nos permite realizar emisiones en directo. Para ello deberemos enviar desde el cliente un stream RTSP o RTMP, y el servidor de encargar\u00e1 de difundirlo al igual que en el caso del VOD (con HLS para dispositivos iOS, y con RTSP para Android).</p> <p>En la secci\u00f3n Incoming Publishers de la aplicaci\u00f3n live de Wowza podemos ver los datos que deberemos utilizar en la aplicaci\u00f3n de emisi\u00f3n para publicar v\u00eddeo en directo en Wowza:</p> <p></p> <p>De esta pantalla necesitaremos la siguiente informaci\u00f3n para poder publicar v\u00eddeo en el servidor:</p> <ul> <li>IP del servidor</li> <li>Puerto del servicio de streaming</li> <li>Nombre de la aplicaci\u00f3n de streaming (<code>live</code> por defecto)</li> <li>Usuario y password del usuario publisher (deberemos recordar los datos del usuario que introdujimos al configurar Wowza).</li> </ul> <p>A continuaci\u00f3n veremos c\u00f3mo podemos publicar en esta aplicaci\u00f3n utilizando una aplicaci\u00f3n Android.</p>"},{"location":"procesamiento-android.html#tecnologias-para-la-emision-de-video","title":"Tecnolog\u00edas para la emisi\u00f3n de v\u00eddeo","text":"<p>Aunque el soporte para la emisi\u00f3n de medios de momento no forma parte de los SDKs de las principales plataformas m\u00f3viles, podemos encontrar diferentes librer\u00edas y tecnolog\u00edas que lo proporcionan. Vamos a revisar algunas de ellas:</p> <ul> <li>libstreaming: Librer\u00eda open source para Android que nos permite emitir v\u00eddeo v\u00eda RTSP. <p>https://github.com/fyhertz/libstreaming</p> </li> <li>Media Library for iOS: Librer\u00eda open source para iOS que nos permite emitir v\u00eddeo v\u00eda RTMP. <p>https://github.com/slavavdovichenko/MediaLibDemos3x</p> </li> <li>Adobe AIR: Esta tecnolog\u00eda nos proporciona un SDK con el que podemos crear aplicaciones multiplataforma. Dentro de dicho SDK contamos con clases para la reproducci\u00f3n y captura de v\u00eddeo, y adem\u00e1s para su emisi\u00f3n de forma sencilla mediante RTMP. <p>http://www.adobe.com/devnet/air.html</p> </li> <li>Kickflip: Plataforma para la emisi\u00f3n de v\u00eddeo desde dispositivos iOS y Android. Nos proporciona un SDK open source sencillo de utilizar que emite el v\u00eddeo a trav\u00e9s de sus propios servidores y mediante sus propios protocolos. <p>https://kickflip.io</p> </li> </ul>"},{"location":"procesamiento-android.html#emision-desde-android-con-libstreaming","title":"Emisi\u00f3n desde Android con libstreaming","text":"<p>Podemos utilizar la librer\u00eda libstreaming para emitir v\u00eddeo RTSP desde un dispositivo Android:</p> <p>https://github.com/fyhertz/libstreaming</p>"},{"location":"procesamiento-android.html#instalacion-y-configuracion-de-libstreaming","title":"Instalaci\u00f3n y configuraci\u00f3n de libstreaming","text":"<p>Seguiremos los siguientes pasos para importar la librer\u00eda en Eclipse:</p> <ul> <li> <p>Descargar la librer\u00eda libstreaming.</p> </li> <li> <p>Seleccionar File &gt; New &gt; Import module\u2026.</p> </li> <li> <p>Seleccionar el directorio donde se encuentra libstreaming.</p> </li> <li> <p>Deja todas las opciones por defecto del asitente.</p> </li> <li> <p>Pulsar Finish.</p> </li> </ul> <p>Tras ejecutar el asistente es posible que haya que modificar el fichero <code>build.gradle</code> de <code>libstreaming</code> para que utilice una versi\u00f3n de las build tools (<code>buildToolsVersion</code>) que tengamos instalada. Tras hacer la correcci\u00f3n, se deber\u00e1 volver a sincronizar con gradle.</p> <p>Una vez tengamos la librer\u00eda como m\u00f3dulo del proyecto, tendremos que a\u00f1adirla como dependencia del m\u00f3dulo de nuestra aplicaci\u00f3n para poder emitir desde ella v\u00eda streaming. Para ello seguiremos los siguientes pasos:</p> <ul> <li>Seleccionamos File &gt; Project Structure....</li> <li>Seleccionamos el m\u00f3dulo de nuestra aplicaci\u00f3n (por ejemplo <code>app</code>) y vamos a la pesta\u00f1a Dependencies.</li> <li>A\u00f1adimos una dependencia de m\u00f3dulo (Module Dependency) y seleccionamos <code>:libstreaming</code>.</li> </ul> <p></p> <p>En el <code>build.gradle</code> del m\u00f3dulo de nuestra aplicaci\u00f3n veremos la dependencia de la siguiente forma:</p> <pre><code>dependencies {\ncompile fileTree(dir: 'libs', include: ['*.jar'])\ntestCompile 'junit:junit:4.12'\ncompile 'com.android.support:appcompat-v7:23.1.0'\ncompile 'com.android.support:design:23.1.0'\ncompile project(':libstreaming')\n}\n</code></pre> <p>Con esto ya podremos utilizar la librer\u00eda libstreaming desde nuestro proyecto. Al empaquetar la aplicaci\u00f3n, la librer\u00eda ser\u00e1 incluida con ella.</p> <p>Vamos a ver a continuaci\u00f3n c\u00f3mo utilizar esta librer\u00eda en el c\u00f3digo de nuestra aplicaci\u00f3n.</p>"},{"location":"procesamiento-android.html#uso-de-la-libreria-libstreaming","title":"Uso de la librer\u00eda libstreaming","text":"<p>En primer lugar, necesitaremos solicitar los siguientes permisos para poder utilizar la librer\u00eda:</p> <pre><code>&lt;uses-permission android:name=\"android.permission.INTERNET\" /&gt;\n&lt;uses-permission android:name=\"android.permission.WRITE_EXTERNAL_STORAGE\" /&gt;\n&lt;uses-permission android:name=\"android.permission.RECORD_AUDIO\" /&gt;\n&lt;uses-permission android:name=\"android.permission.CAMERA\" /&gt;\n</code></pre> <p>En la interfaz necesitaremos contar con una vista de tipo <code>SurfaceView</code> para poder realizar el preview de la c\u00e1mara. Suponemos que en el layout de la actividad hemos incluido esta vista con un identificador <code>R.id.surfaceView</code>.</p> <p>En el <code>onCreate</code> de nuestra actividad obtenemos el <code>SurfaceView</code> y el holder de dicha superficie:</p> <pre><code>mSurfaceView = (SurfaceView)findViewById(R.id.surfaceView);\n\nmHolder = mSurfaceView.getHolder();\nmHolder.addCallback(this);\n</code></pre> <p>La actividad deber\u00e1 implementar la interfaz <code>SurfaceHolder.Callback</code>.</p> <p>\u00a1CUIDADO! La vista <code>SurfaceView</code> que debemos utilizar no es la de Android, sino una versi\u00f3n alternativa suministrada por libstreaming, que se encuentra en el paquete <code>net.majorkernelpanic.streaming.gl</code>. Por lo tanto, para incluir esta vista en el layout deberemos indicar su nombre completo: <pre><code>&lt;net.majorkernelpanic.streaming.gl.SurfaceView\n...\n/&gt;\n</code></pre></p> <p>Tras la obtenci\u00f3n de la superficie, creamos una sesi\u00f3n de captura que la utilice para mostrar el preview. La sesi\u00f3n ser\u00e1 un objeto de tipo <code>Session</code> (pertenece a la librer\u00eda libstreaming):</p> <pre><code>mSession = SessionBuilder.getInstance()\n.setCallback(this)\n.setSurfaceView(mSurfaceView)\n.setPreviewOrientation(0)\n.setContext(getApplicationContext())\n.setAudioEncoder(SessionBuilder.AUDIO_AAC)\n.setAudioQuality(new AudioQuality(8000, 16000))\n.setVideoEncoder(SessionBuilder.VIDEO_H264)\n.setVideoQuality(new VideoQuality(640,480,30,600000))\n.build();\n</code></pre> <p>En la sesi\u00f3n configuramos los codecs y la calidad del audio y del v\u00eddeo. Deberemos implementar en la actividad la interfaz <code>Session.Callback</code> para recibir los eventos de la sesi\u00f3n.</p> <p>Por \u00faltimo, crearemos un cliente RTSP que se encargar\u00e1 de enviar el stream de v\u00eddeo capturado en la sesi\u00f3n a Wowza. El cliente los crearemos con un objeto de la clase <code>RtspClient</code>, pas\u00e1ndole como par\u00e1metro los datos de acceso al servidor:</p> <pre><code>mClient = new RtspClient();\nmClient.setSession(mSession);\nmClient.setCallback(this);\n\nmClient.setCredentials(\"publisher\", \"mastermoviles\");\nmClient.setServerAddress(\"192.168.1.44\", 1935);\nmClient.setStreamPath(\"/live/canal1\");\n</code></pre> <p>Es importante poner en la ruta el nombre de la aplicaci\u00f3n configurada en Wowza, y tras \u00e9l el nombre del stream. Por ejemplo, si tenemos la aplicaci\u00f3n <code>live</code> en Wowza, podr\u00edamos crear el siguiente stream:</p> <pre><code>/live/canal1\n</code></pre> <p>Deberemos implementar en la actividad la interfaz <code>RtspClient.Callback</code> para tener constancia de los eventos de la conexi\u00f3n con el servidor de streaming.</p> <p>Una vez terminado de configurar, podemos poner en marcha la captura y el env\u00edo de datos al servidor con:</p> <pre><code>mClient.startStream();\n</code></pre> <p>En <code>onDestroy</code> deber\u00edamos destruir todos los objetos que hemos construido en <code>onCreate</code>:</p> <pre><code>@Override\nprotected void onDestroy() {\nsuper.onDestroy();\nmClient.release();\nmSession.release();\nmSurfaceView.getHolder().removeCallback(this);\n}\n</code></pre> <p>Nos falta \u00fanicamente definir los m\u00e9todos de las distintas interfaces que hemos tenido que implementar.</p>"},{"location":"procesamiento-android.html#metodos-de-surfaceholdercallback","title":"M\u00e9todos de <code>SurfaceHolder.Callback</code>","text":"<p>Cuando la superficie se crea podemos poner en marcha el preview de la sesi\u00f3n de captura, y al destruirse podemos detenerlo:</p> <pre><code>@Override\npublic void surfaceChanged(SurfaceHolder holder, int format, int width,\nint height) {\n}\n\n\n@Override\npublic void surfaceCreated(SurfaceHolder holder) {\nmSession.startPreview();\n}\n\n\n@Override\npublic void surfaceDestroyed(SurfaceHolder holder) {\nmSession.stop();\n}\n</code></pre>"},{"location":"procesamiento-android.html#metodos-de-sessioncallback","title":"M\u00e9todos de <code>Session.Callback</code>","text":"<p>Nos obliga a definir los siguientes m\u00e9todos:</p> <pre><code>@Override\npublic void onBitrateUpdate(long bitrate) {\n}\n\n@Override\npublic void onSessionError(int reason, int streamType, Exception e) {\n}\n\n@Override\npublic void onPreviewStarted() {\n}\n\n@Override\npublic void onSessionConfigured() {\n}\n\n@Override\npublic void onSessionStarted() {\n}\n\n@Override\npublic void onSessionStopped() {\n}\n</code></pre> <p>Con ellos podemos tener constancia de todos los cambios ocurridos en la sesi\u00f3n de captura. No es necesario introducir contenido en ellos para utilizar el streaming.</p>"},{"location":"procesamiento-android.html#metodos-de-rtspclientcallback","title":"M\u00e9todos de <code>RtspClient.Callback</code>","text":"<p>Nos obliga a definir el m\u00e9todo <code>onRtspUpdate</code>, que se invoca cuando hay alg\u00fan evento en la comunicaci\u00f3n por RTSP:</p> <pre><code>@Override\npublic void onRtspUpdate(int message, Exception exception) {\n}\n</code></pre> <p>No es necesario introducir ning\u00fan contenido en este m\u00e9todo, pero puede ser de utilidad introducir alg\u00fan mensaje de log para depurar el funcionamiento de la conexi\u00f3n con el servidor (por ejemplo comprobar si los credenciales son correctos).</p> <p>Si al poner en marcha la emisi\u00f3n con libstreaming tenemos un <code>IOException</code> (con mensaje Try again), puede que sea necesario aumentar el timeout en la clase <code>net.majorkernelpanic.streaming.MediaStream</code> <pre><code>mReceiver.setSoTimeout(3000);\n</code></pre></p>"},{"location":"procesamiento-de-imagen-en-ios-opencv.html","title":"Procesamiento de im\u00e1genes en iOS: OpenCV","text":"<p>En esta secci\u00f3n vamos a ver como llevar a cabo procesamiento de im\u00e1genes en iOS utilizando la librer\u00eda de visi\u00f3n por computador OpenCV.</p>"},{"location":"procesamiento-de-imagen-en-ios-opencv.html#opencv","title":"OpenCV","text":"<p>OpenCV es una librer\u00eda de c\u00f3digo abierto para visi\u00f3n por computador. El principal objetivo de esta librer\u00eda es ofrecer algoritmos de visi\u00f3n artificial que funcionen en tiempo real. Originalmente OpenCV fue desarrollada por el departamento de investigaci\u00f3n de Intel, pero m\u00e1s tarde recibi\u00f3 soporte de la empresa Willow Garage y actualmente es mantenido por la empresa Itseez. Se trata de una librer\u00eda multi-plataforma (Windows, Linux, MacOSX, iOS, Android, ... )y se puede utilizar libremente al ser desarrollada bajo una licencia de c\u00f3digo abierto BSD. </p> <p>\u00c1reas de aplicaci\u00f3n librer\u00eda OpenCV: * Egomotion * Sistema de detecci\u00f3n/reconocimiento facial * Reconocimiento de gestos * Segmentaci\u00f3n * Calibraci\u00f3n de c\u00e1maras * Tracking * Algoritmos est\u00e9reo * Structure from Motion (SfM) * Retoque de im\u00e1genes (Balance de blancos, HDR, constrate, etc) * ...</p> <p>Adem\u00e1s, OpenCV cuenta con una librer\u00eda de aprendizaje autom\u00e1tico que contiene m\u00e9todos tradicionales de clasificaci\u00f3n y regresi\u00f3n: * Arboles de decisi\u00f3n * Redes neuronales artificiales * k-vecinos m\u00e1s cercanos * Random forest * Support vector machine (SVM) * ...</p>"},{"location":"procesamiento-de-imagen-en-ios-opencv.html#opencv-en-ios","title":"OpenCV en iOS","text":"<p>OpenCV es una librer\u00eda escrita en C/C++, por lo que utilizando Objective-C no hay mucho problema para su integraci\u00f3n en iOS. Sin embargo, con Swift la cosa es ligeramente m\u00e1s complicada, y necesitaremos utilizar una cabecera puente en nuestro proyecto para acceder a funciones de OpenCV. Por ello tendremos que escribir un poco de Objective-C++ en nuestros proyectos.</p> <p></p>"},{"location":"procesamiento-de-imagen-en-ios-opencv.html#enlazando-la-libreria-opencv-en-nuestro-proyecto","title":"Enlazando la librer\u00eda OpenCV en nuestro proyecto","text":"<p>Para utilizar OpenCV en iOS tendremos que descargarnos la versi\u00f3n correspondiente para esta plataforma de la web oficial. En concreto vamos a utilizar la versi\u00f3n 2.4.13 para iOS. Enlace descarga.</p> <p>El siguiente paso ser\u00e1 crear un nuevo proyecto en Xcode y a\u00f1adir las librer\u00edas necesarias para usar OpenCV en nuestro proyecto. Creamos un proyecto nuevo (Single View) y simplemente arrastramos a la ventana donde se muestran los ficheros del proyecto el fichero <code>opencv2.framework</code>, al hacer esto nos aparecer\u00e1 un di\u00e1logo para importarlo en el proyecto. Marcamos la opci\u00f3n copiar si es necesario.</p> <p></p> <p>Con este paso ya tendremos OpenCV a\u00f1adido a nuestro proyecto, pero para poder utilizarlo tendremos que crear una cabecera puente (Bridging header), que nos permita utilizar c\u00f3digo Objetive-C++ desde c\u00f3digo Swift. Para crear la cabecera puente, vamos a File &gt; New &gt; File (\u2318N) y en iOS &gt; Source seleccionamos <code>Cocoa Touch Class</code>. Lo llamaremos OpenCVWrapper, en este fichero escribiremos el c\u00f3digo que servir\u00e1 de envoltorio para utilizar funciones OpenCV en Objective-C++. Al crear este fichero Xcode nos sugerir\u00e1 crear la cabecera puente: <code>nombreproyecto-Bridging-header.h</code>. Todas las librer\u00edas que incluyamos desde este fichero header, las clases y m\u00e9todos definidas en las mismas ser\u00e1n accesibles desde nuestro c\u00f3digo en Swift.</p> <p>Con estos pasos habremos creados tres nuevos ficheros en nuestro proyecto: <code>OpenCVWrapper.h</code>, <code>OpenCVWrapper.m</code> y <code>nombre-proyecto-Bridging-Header.h</code>. Como hemos comentado, la cabecera puente ser\u00e1 nuestra interfaz para usar funciones de OpenCV, por lo tanto tenemos que a\u00f1adir el envoltorio que hemos creado: </p> <pre><code>#import \"OpenCVWrapper.h\"\n</code></pre>"},{"location":"procesamiento-de-imagen-en-ios-opencv.html#primer-ejemplo-conversion-imagen-a-escala-de-grises","title":"Primer ejemplo: Conversi\u00f3n imagen a escala de grises","text":"<p>Con esto ya estamos listos para empezar a utilizar OpenCV en nuestro proyecto, por ello vamos a crear una sencilla aplicaci\u00f3n que cargue una imagen y convierta la imagen a escala de grises utilizando la funci\u00f3n predefinida en OpenCV `cv::cvtColor(imageMat, gray, CV_RGBA2GRAY);``</p> <p>A\u00f1adimos un ImageView (cargar imagen) y un bot\u00f3n a la interfaz de la apliaci\u00f3n que creamos anteriormente. Al pulsar el bot\u00f3n convertiremos la im\u00e1gen a escala de grises. Para ello, en la clase OpenCVWrapper que creamos anteriormente para utilizar la funcionalidad de la librer\u00eda escribimos nuestra propia funci\u00f3n para convertir a escala de grises. La funci\u00f3n tomar\u00e1 como entrada una imagen en formato UIImage, la convertir\u00e1 a formato OpenCV, cv::Mat, y finalmente se convertir\u00e1 de vuelta al formato UIImage para su visualizai\u00f3n. Para convertir im\u00e1genes entre UIImage y el formato de OpenCV, utilizaremos los m\u00e9todos definidos en las cabeceras:  * <code>#import \"opencv2/highgui/ios.h\"</code>  * <code>#import &lt;opencv2/highgui/cap_ios.h&gt;</code></p> <p>En estas cabeceras se encuentran definidos m\u00e9todos para interoperar con tipos de datos propios de iOS.</p> <p>Convertir una imagen a formato cv::Mat:</p> <p><code>UIImageToMat(image,imageMat);</code></p> <p>Convertir formato cv::Mat a UIImage</p> <p><code>MatToUIImage(gray);</code></p> <p>Finalmente crearemos una funci\u00f3n <code>convertImageToGrayScale</code>en la clase <code>OpenCVWrapper</code>. Esta clase nos servir\u00e1 de interfaz con OpenCV y la utilizaremos desde nuestro c\u00f3digo en Swift</p> <p>( OpenCVWrapper.mm ) <pre><code>#import \"OpenCVWrapper.h\"\n\n@implementation OpenCVWrapper\n\n+(UIImage *) convertImageToGrayscale: (UIImage *)image\n{\n cv::Mat imageMat;\n UIImageToMat(image,imageMat);\n if(imageMat.channels() == 1){ return image; }\n cv::Mat gray;\n // Convert the image to grayscale\n cv::cvtColor(imageMat, gray, CV_RGBA2GRAY);\n return MatToUIImage(gray);\n}\n@end\n</code></pre></p> <p>( OpenCVWrapper.h ) <pre><code>#import &lt;Foundation/Foundation.h&gt;\n\n#import &lt;UIKit/UIKit.h&gt;\n\n// Need this ifdef, so the C++ header won't confuse Swift\n#ifdef __cplusplus\n #import &lt;opencv2/opencv.hpp&gt;\n #import \"opencv2/highgui/ios.h\"\n #import &lt;opencv2/highgui/cap_ios.h&gt;\n#endif\n\n@interface OpenCVWrapper : NSObject\n\n+(UIImage *) convertImageToGrayscale: (UIImage *)Image;\n\n@end\n</code></pre></p> <p>Finalmente, desde nuestro c\u00f3digo Swift llamaremos a la funci\u00f3n <code>convertImageToGrayscale</code> al accionar el bot\u00f3n 'convertir':</p> <pre><code>@IBAction func botonConvertirGrises(_ sender: UIButton) \n{\n    imagen.image = OpenCVWrapper.convertImage(toGrayscale: imagen!.image)\n}\n</code></pre> <p></p>"},{"location":"procesamiento-de-imagen-en-ios-opencv.html#otros-ejemplos","title":"Otros ejemplos","text":"<p>A continuaci\u00f3n vamos a ver como aplicar a nuestra imagen otros filtros t\u00edpicos en procesamiento de imagen. Por ejemplo, un filtro de emborronamiento, del ingl\u00e9s, blur. De igual manera que hicimos anteriormente, creamos una nuevo m\u00e9todo en nuestra interfaz <code>OpenCVWrapper</code>. En este caso usaremos una de las funciones de filtrado de OpenCV, se trata de un m\u00e9todo para emborronar la imagen: </p> <p><code>+(UIImage *) blurImage: (UIImage *)Image;</code></p> <p>El nuevo m\u00e9todo quedar\u00eda de la siguiente forma:</p> <pre><code>+(UIImage *) blurImage: (UIImage *)image\n{\n     cv::Mat imageMat;\n     UIImageToMat(image,imageMat);\n     cv::Mat blurImage;\n     cv::GaussianBlur( imageMat, blurImage, cv::Size( 3, 3 ), 0, 0 );\n     return MatToUIImage(blurImage);\n}\n</code></pre> <p>A continuaci\u00f3n vamos a ver otro filtro un poco m\u00e1s complejo, en concreto un filtro detector de bordes, algoritmo de Canny. Este filtro se encuentra implementado en OpenCV y por lo tanto podemos usarlo para procesar nuestras im\u00e1genes. </p> <pre><code> cv::Canny(gray, edges, 0, 50, 3);\n</code></pre> <p>Los argumentos que recibe este m\u00e9todo son los siguientes:</p> <ul> <li>Imagen origen en escala de grises</li> <li>Imagen salida con los bordes detectados</li> <li>lowThreshold: umbral inferior para la detecci\u00f3n de bordes</li> <li>highThreshold: umbral superior para la detecci\u00f3n de bordes</li> <li>kernel_size: tama\u00f1o de la convoluci\u00f3n 2D en las dimensiones X e Y. Por defecto 3.</li> </ul> <p></p> <p>En OpenCV encontramos muchos otras funciones de procesamiento de im\u00e1gen y visi\u00f3n por computador que podemos utilizar en nuestra aplicaci\u00f3n, toda la documentaci\u00f3n sobre la librer\u00eda y m\u00e9todos disponibles se puede encontrar online.</p>"},{"location":"procesamiento-de-imagen-en-ios-opencv.html#detector-de-caras","title":"Detector de caras","text":"<p>A continuaci\u00f3n, vamos a ver como utilizar OpenCV para poner en funcionamiento un detector de caras en nuestra aplicaci\u00f3n m\u00f3vil. OpenCV implementa clasificadores en cascada para detectar m\u00faltiples objetos. En nuestro caso vamos a cargar un modelo para el clasificador que esta entrenado para detectar caras. En concreto usaremos el clasificador basado en las caracter\u00edsticas de Haar. Podeis encontrar m\u00e1s informaci\u00f3n sobre la implementaci\u00f3n de este m\u00e9todo en el siguiente enlace. El trabajo original se basa en el siguiente art\u00edculo cient\u00edfico:</p> <pre><code>[Viola01] Paul Viola and Michael J. Jones. Rapid Object Detection using a Boosted Cascade of Simple Features. IEEE CVPR, 2001. \n</code></pre> <p>Para utilizar este m\u00e9todo en nuestro proyecto vamos a crear un nuevo wrapper que nos sirva de interfaz para el objeto <code>cv::CascadeClassifier</code>. Este objeto OpenCV nos permitir\u00e1 cargar un modelo entrenado para la detecci\u00f3n de caras as\u00ed como la detecci\u00f3n en s\u00ed. Crearemos un m\u00e9todo para inicializar el clasificador con el modelo detector de caras.</p> <pre><code>NSString* cascadePath = [[NSBundle mainBundle] pathForResource:@\"haarcascade_frontalface_alt2\" ofType:@\"xml\"];\nfaceDetector.load([cascadePath UTF8String]);\n</code></pre> <p>Podremos descargas este modelo y otros previamente entrenados desde el repositorio oficial de la librer\u00eda  OpenCV. Encontramos otros modelos para detectar ojos, personas, tronco superior, etc\u00e9tera. </p> <p>Crearemos otro m\u00e9todo dentro de nuestro wrapper para llevar a cabo la detecci\u00f3n de caras dada una imagen de entrada.</p> <pre><code>-(UIImage *) detectFaces: (UIImage *)image\n</code></pre> <p>Este m\u00e9todo contiene toda la l\u00f3gica necesaria para detectar caras en una imagen utilizando la funci\u00f3n </p> <pre><code>faceDetector.detectMultiScale(gray, faces, 1.1, 2, 0|CV_HAAR_SCALE_IMAGE, cv::Size(30, 30));\n</code></pre> <p>Esta funci\u00f3n recibe los siguientes par\u00e1metros:</p> <ul> <li>Imagen de entrada (escala de grises)</li> <li>Factor de escala: especifica cuanto se escala la imagen en cada iteraci\u00f3n</li> <li>N\u00famero m\u00ednimo de vecinos: especifica cuantos vecinos (detecciones) como m\u00ednimo debe tener una detecci\u00f3n para considerarse como v\u00e1lida.</li> <li><code>CV_HAAR_SCALE_IMAGE</code>: especifica que se escale la imagen y no el detector. Esto mejora el rendimiento del detector.</li> <li>Tama\u00f1o m\u00ednimo: tama\u00f1o m\u00ednimo posible de una cara a detectar en la im\u00e1gen.</li> </ul> <p>El m\u00e9todo <code>detectFaces</code> contendr\u00eda el siguiente c\u00f3digo (Se ha eliminado el c\u00f3digo para interoperar con UIImage y convertir a escala de grises por claridad):</p> <pre><code>// Detect faces\nstd::vector&lt;cv::Rect&gt; faces;\nfaceDetector.detectMultiScale(gray, faces, 1.1, 2, 0|CV_HAAR_SCALE_IMAGE, cv::Size(30, 30));\n\n// Draw all detected faces\nfor(unsigned int i = 0; i &lt; faces.size(); i++)\n{\n    const cv::Rect&amp; face = faces[i];\n    // Get top-left and bottom-right corner points\n    cv::Point tl(face.x, face.y);\n    cv::Point br = tl + cv::Point(face.width, face.height);\n    // Draw rectangle around the face\n    cv::Scalar magenta = cv::Scalar(255, 0, 255);\n    cv::rectangle(cvImage, tl, br, magenta, 4, 8, 0);\n}\n</code></pre> <p>Como podemos apreciar arriba, una vez detectadas las caras, utilizando funciones OpenCV dibujamos gr\u00e1ficos sobre la imagen que posteriormente visualizaremos en el ImageView de nuestra app m\u00f3vil.</p> <p></p> <p>Aunque iOS tambi\u00e9n tiene su propio detector de caras como hemos visto anteriormente, el clasificador de OpenCV nos ofrece m\u00e1s posibilidades, permiti\u00e9ndonos detectar m\u00e1s objetos por tantos siendo m\u00e1s flexible. Si no existiese un clasificador pre-entrenado para el tipo de objeto que queremos detectar, siempre podemos entrenar nuestro propio clasificador nuevos propio conjunto de im\u00e1genes previamente anotadas. Esta tarea puede requerir de tiempo y esfuerzo en t\u00e9rminos de investigaci\u00f3n, aunque es factible.</p> <p>Por \u00faltimo, en OpenCV podemos tambi\u00e9n ajustar el rendimiento del clasificador, ya que el clasificador en cascada es una t\u00e9cnica costa computacionalmente. Ajustando algunos de los par\u00e1metros del m\u00e9todo <code>detectMultiScale</code>podemos mejorar el rendimiento, aunque por otro lado la precisi\u00f3n del m\u00e9todo podr\u00eda reducirse. Adem\u00e1s de ajustar los par\u00e1metros, OpenCV implementa otro tipo de caracter\u00edsticas, denominadas Local Binary Patterns (LBP), que se puede utilizar para el clasificador en cascada. Este tipo de caracter\u00edsticas son m\u00e1s eficientes de computar ya que estan basadas principalmente en c\u00f3mputos que se pueden llevar a cabo con aritm\u00e9tica entera, siendo por lo tanto m\u00e1s eficientes y funcionando 2-3 veces m\u00e1s r\u00e1pido que las caracter\u00edsticas de Haar (punto flotante). Otra opci\u00f3n para mejorar el rendimiento podr\u00eda ser no procesar todos las im\u00e1genes de nuestro flujo de v\u00eddeo y llevar a cabo alg\u00fana t\u00e9cnica de seguimiento entre detecci\u00f3nes (Optical flow).</p>"},{"location":"procesamiento-de-imagen-en-ios-opencv.html#captura-de-video","title":"Captura de v\u00eddeo","text":"<p>OpenCV tambi\u00e9n implementa sus clases propias para la captura de v\u00eddeo, de forma que nos permite abstraernos a la hora de utilizar la c\u00e1mara de nuestro dispositivo m\u00f3vil y utilizar el recurso que tengamos disponible. La clase OpenCV para obtener un flujo de v\u00eddeo en nuestra app es <code>CvVideoCamera</code>. Utilizando esta clase podremos facilmente obtener v\u00eddeo de la c\u00e1mara de nuestro dispositivo y adem\u00e1s procesarlo usando las distintas funcionalidades que OpenCV nos ofrece.</p> <p>Para poder utilizar esta clase en nuestro proyecto iOS, tendremos que enlazar las siguientes librer\u00edas en el proyecto:</p> <p></p> <p>Adem\u00e1s, a\u00f1adiremos la siguiente cabecera en el wrapper de OpenCV: `#import `` <p>Para la utilizaci\u00f3n de <code>CvVideoCamera</code>en Swift, escribiremos tambi\u00e9n un wrapper como hicimos anteriormente para las funciones de procesamiento de imagen y detecci\u00f3n de caras.</p> <p>CvVideoCaptureWrapper.mm <pre><code>// Class extension to adopt the delegate protocol\n@interface CvVideoCameraWrapper () &lt;CvVideoCameraDelegate&gt;\n{\n\n}\n@end\n\n@implementation CvVideoCameraWrapper\n{\n ViewController * viewController;\n UIImageView * imageView;\n CvVideoCamera * videoCamera;\n}\n\n// Otros m\u00e9todos para inicializar y procesar im\u00e1genes del v\u00eddeo\n...\n</code></pre></p> <p>CvVideoCaptureWrapper.h <pre><code>// This is a forward declaration; we cannot include *-Swift.h in a header.\n@class ViewController;\n@interface CvVideoCameraWrapper : NSObject\n    -(id)initWithController:(ViewController*)c andImageView (UIImageView*)iv;\n@end\n</code></pre></p> <p>Finalmente, imlementaremos dos m\u00e9todos en nuestra clase <code>CvVideoCameraWrapper</code>, un m\u00e9todo para inicializar la c\u00e1mara con las opciones que deseemos, y otro m\u00e9todo para llevar a cabo procesamiento del flujo de v\u00eddeo de la c\u00e1mara.</p> <p>M\u00e9todo inicializaci\u00f3n c\u00e1mara <pre><code>-(id)initWithController:(ViewController*)c andImageView:(UIImageView*)iv \n{\n    viewController = c;\n    imageView = iv;\n    videoCamera = [[CvVideoCamera alloc] initWithParentView:imageView];\n\n    // ... set up the camera\n    videoCamera.delegate = self;\n    videoCamera.defaultAVCaptureDevicePosition = AVCaptureDevicePositionBack;\n    videoCamera.defaultAVCaptureSessionPreset = AVCaptureSessionPreset640x480;\n    videoCamera.defaultAVCaptureVideoOrientation = AVCaptureVideoOrientationLandscapeLeft;\n    videoCamera.defaultFPS = 30;\n    videoCamera.rotateVideo = !videoCamera.rotateVideo;\n    [videoCamera start];\n    return self;\n}\n</code></pre></p> <p>En el m\u00e9todo anterior hemos definido las propiedades para el flujo de v\u00eddeo a obtener de la c\u00e1mara: resoluci\u00f3n, c\u00e1mara (frontal o trasera), configuraci\u00f3n (por defecto), im\u00e1genes por segundo, rotar el v\u00eddeo, etc\u00e9tera. Adem\u00e1s, en este m\u00e9todo asignamos objetos como el imageView donde vamos a renderizar las im\u00e1genes de la c\u00e1mara y el controlador que se har\u00e1 cargo de esta vista.</p> <p>Finalmente, implementaremos el m\u00e9todo <code>processImage</code>, el cual se encargar\u00e1 de procesar cada im\u00e1gen del flujo de v\u00eddeo. Podemos aplicar filtros y t\u00e9cnicas como las vistas anteriormente: conversi\u00f3n escala de grises, detecci\u00f3n de bordes, suavizado, detecci\u00f3n de caras, etc\u00e9tera.</p> <pre><code>- (void)processImage:(cv::Mat&amp;)image\n{\n    // Procesamiento imagen con OpenCV: ejemplo, conversi\u00f3n escala de grises\n    cv::cvtColor(image, image, CV_RGBA2GRAY);\n}\n</code></pre>"},{"location":"procesamiento-de-imagen-en-ios-opencv.html#ejercicios","title":"Ejercicios","text":"<p>1.- En primer lugar, crea un nuevo proyecto en Xcode y enlaza la librer\u00eda OpenCV. Crea las clases envoltorio, as\u00ed como la librer\u00eda puente para poder utilizar OpenCV en tu c\u00f3digo Swift.</p> <p>(a) Dise\u00f1a una interfaz de usuario que contenga inicialmente un <code>ImageView</code> y un bot\u00f3n para convertir una imagen que cargaremos en el <code>imageView</code> a escala de grises.</p> <p>(b) Implementa en el envoltorio para utilizar OpenCV un m\u00e9todo que nos permita convertir una imagen a escala de grises. Al pulsar el bot\u00f3n creado anteriormente se convertir\u00e1 la imagen a escala de grises.</p> <p>(c) Implementa un m\u00e9todo para la detecci\u00f3n de bordes en la imagen que hemos cargado. Para mejorar la detecci\u00f3n de l\u00edneas es muy com\u00fan utilizar un filtro de suavizado (emborronamiento) de la imagen antes de llevar a cabo la detecci\u00f3n usando el filtro de Canny. Implementa estas dos operaciones en el envoltorio y a\u00f1ade un componente <code>slider</code>que nos permite cambiar los umbrales que utilizamos en el m\u00e9todo de Canny. Al mover el slider actualizaremos la imagen con los nuevos resultados.</p> <p>(d) A\u00f1ade un nuevo bot\u00f3n a la interfaz que nos permita detectar caras en la imagen. Utilizaremos un clasificador en cascada como el visto anteriormente. Implementa un envoltorio en Objective-C++ para utilizar esta funcionalidad el <code>ViewController</code> (Swift). Dibuja un rect\u00e1ngulo sobre cada una de las caras detectadas, utiliza las funciones de dibujado de formas b\u00e1sicas de OpenCV (<code>cv::rectangle(cvImage, tl, br, magenta, 4, 8, 0);</code>).</p> <p>2.- Utiliza la clase <code>CvVideoCamera</code> de OpenCV para obtener un flujo de v\u00eddeo en nuestra aplicaci\u00f3n iOS y procesar las im\u00e1genes obtenidas.</p> <p>(a) Lleva a cabo un procesamiento de extracci\u00f3n de bordes sobre el flujo de v\u00eddeo obtenido en tiempo real de la c\u00e1mara. </p> <p>(b) A\u00f1ade un slider a la interfaz de usuario para modificar los par\u00e1metros del detector de bordes de Canny y otro slider para cambiar el nivel de suavizado de la imagen. Observa como estos par\u00e1metros alteran la imagen resultado.</p> <p>Para el ejercicio 2 utiliza la plantilla que hemos dejado en el repositorio de la asignatura: <code>plantillas-multimedia-graficos-ios-swift</code>. Nombre proyecto: <code>CapturaVideoOpenCV</code>.</p> <p>Recuerda que el ejercicio 2 debe desplegarse sobre un dispositivo real, debido a que el emulador no soporta el uso de la c\u00e1mara.</p>"},{"location":"proyecto.html","title":"Proyecto multimedia","text":""},{"location":"proyecto.html#plataforma-de-television-online","title":"Plataforma de televisi\u00f3n online","text":"<p>Como proyecto de la asignatura vamos a implementar una plataforma de televisi\u00f3n online que constar\u00e1 de las siguientes aplicaciones:</p> <ul> <li>Aplicaci\u00f3n iOS para el acceso a radio, videoclub, y emisiones en directo.</li> <li>Aplicaci\u00f3n Android para retransmitir v\u00eddeo en directo a los canales</li> </ul>"},{"location":"proyecto.html#aplicacion-ios","title":"Aplicaci\u00f3n iOS","text":"<p>En la aplicaci\u00f3n iOS veremos 4 secciones diferentes:</p> <ul> <li>Radio: Mostrar\u00e1 una lista de contenidos de audio (m\u00fasica, programas de radio, etc) que podremos reproducir en el dispositivo. Seleccionando uno de ellos se reproducir\u00e1 el audio en el dispositivo. Estos contenidos estar\u00e1n almacenados de forma local en el dispositivo.</li> <li>Videoclub: Mostrar\u00e1 una lista de contenidos de v\u00eddeo (series, pel\u00edculas, programas, etc) bajo demanda. Seleccionando un elemento se reproducir\u00e1 el v\u00eddeo en el dispositivo. A estos v\u00eddeos se acceder\u00e1 a trav\u00e9s de un servidor que nos proporcione v\u00eddeo bajo demando v\u00eda streaming.</li> <li>Directo: Mostrar\u00e1 una lista de canales de televisi\u00f3n. Al seleccionar uno de ellos veremos sus emisiones en directo. Estas emisiones se realizar\u00e1n desde un dispositivo Android.</li> <li>Acerca de: Debe mostrar una pantalla con el nombre de la aplicaci\u00f3n y el nombre del autor, y una gr\u00e1fica de tarta generada de forma din\u00e1mica en la que se muestre la proporci\u00f3n de v\u00eddeo en directo, video bajo demanda, y audio que ofrece nuestra plataforma.</li> </ul> <p>\u00bfQu\u00e9 tipo de navegaci\u00f3n consideras m\u00e1s adecuada en iOS para definir las secciones anteriores?</p>"},{"location":"proyecto.html#aplicacion-android","title":"Aplicaci\u00f3n Android","text":"<p>Tendremos tambi\u00e9n una aplicaci\u00f3n Android que nos permitir\u00e1 hacer emisiones en directo. Tendremos una serie de canales de televisi\u00f3n predefinidos, y desde la pantalla principal podremos:</p> <ul> <li>Mostrar la pantalla \"Acerca de\" con el nombre de la aplicaci\u00f3n y del autor.</li> <li>Configurar la emisi\u00f3n. En la pantalla de configuraci\u00f3n deberemos poder modificar la IP del servidor de streaming, el canal en el que vamos a emitir, y la calidad del v\u00eddeo a emitir.</li> <li>Comenzar la emisi\u00f3n. Lanzar\u00e1 una pantalla en la que se enviar\u00e1 v\u00eddeo capturado desde la c\u00e1mara al servidor de streaming, mostrando una previsualizaci\u00f3n del v\u00eddeo emitido.</li> </ul>"},{"location":"proyecto.html#video-bajo-demanda-en-wowza-sesion-de-20-10-2015","title":"Video bajo demanda en Wowza (sesi\u00f3n de 20-10-2015)","text":"<p>Vamos a publicar v\u00eddeo bajo demanda en el servidor Wowza, para as\u00ed poderlo reproducir en dispositivos m\u00f3viles v\u00eda streaming.</p> <ul> <li>Copia los v\u00eddeos que quieras ofrecer como contenido bajo demanda en la carpeta de contenido VOD de Wowza.</li> <li>Comprueba en el Test Player de Wowza que los v\u00eddeos se ven correctamente.</li> <li>Copia las direcciones para reproducir los v\u00eddeos en iOS y en Android. Podemos probarlo introduciendo esta direcci\u00f3n en el cuadro de direcci\u00f3n del navegador del m\u00f3vil. Seg\u00fan si vamos a reproducir en emuladores o dispositivos reales el procedimiento cambiar\u00e1.</li> </ul>"},{"location":"proyecto.html#prueba-en-emuladores","title":"Prueba en emuladores","text":"<p>La prueba en emuladores/simuladores depender\u00e1 de si utilizamos el simulador iOS o el emulador Android, ya que el primero es una aplicaci\u00f3n m\u00e1s que se ejecuta en nuestra m\u00e1quina local, mientras que el segundo se ejecuta dentro de una m\u00e1quina virtual:</p> <ul> <li>En el simulador de iOS podemos utilizar la direcci\u00f3n que nos proporciona Wowza directamente.</li> <li>En el emulador de Android deberemos utilizar la IP del host en el que se ejecuta la m\u00e1quina virtual que emula el dispositivo. Esta IP es <code>10.0.2.2</code> (equivale a la direcci\u00f3n de loopback en la m\u00e1quina host). Sustituiremos la IP de la direcci\u00f3n que nos ha proporcionado Wowza por dicha IP.</li> </ul>"},{"location":"proyecto.html#prueba-en-dispositivos-reales","title":"Prueba en dispositivos reales","text":"<p>Para probar en dispositivos reales debemos hacer que nuestros dispositivos est\u00e9n en la misma red que el servidor Wowza. Para ello deberemos activar en el Mac la opci\u00f3n de compartir Internet, siguiendo los siguientes pasos:</p> <ul> <li>Entramos en Manzana &gt; Preferencia del Sistema &gt; Compartir</li> <li>Con la casilla Compartir Internet desmarcada seleccionamos la opci\u00f3n Wi-Fi.</li> <li>Entramos en Opciones de Wi-Fi \u2026.</li> <li>Ponemos como SSID nuestro login de Campus Virtual (para evitar conflictos entre redes), y elegimos un password propio.</li> <li>Conectamos el m\u00f3vil a la WiFi del Mac con la que se comparte Internet. Ahora los dispositivos estar\u00e1n conectados a la misma red.</li> </ul> <p></p> <p>Ahora podremos probar el enlace en el m\u00f3vil directamente, ya que al estar en la misma red debe poder ver la direcci\u00f3n donde est\u00e1 escuchando Wowza.</p>"},{"location":"proyecto.html#creacion-de-la-aplicacion-ios-sesion-de-22-10-2015","title":"Creaci\u00f3n de la aplicaci\u00f3n iOS (sesi\u00f3n de 22-10-2015)","text":"<p>Vamos a crear la aplicaci\u00f3n iOS para nuestra plataforma de televisi\u00f3n online.</p>"},{"location":"proyecto.html#creacion-de-la-aplicacion","title":"Creaci\u00f3n de la aplicaci\u00f3n","text":"<p>Recordemos que la aplicaci\u00f3n debe tener las siguientes secciones:</p> <ul> <li>Radio</li> <li>Videoclub</li> <li>Directo</li> <li>Acerca de</li> </ul> <p>Para ello creamos una aplicaci\u00f3n basada en Tab Bar. Esto nos crear\u00e1 una aplicaci\u00f3n con dos controladores: First View Controller y Second View Controller.</p> <p>M\u00e1s adelante definiremos una pesta\u00f1a para cada secci\u00f3n de la aplicaci\u00f3n en el Tab Bar. De momento aprovecharemos las que nos ha creado la plantilla.</p>"},{"location":"proyecto.html#tipos-de-datos","title":"Tipos de datos","text":"<p>Creamos una clase <code>UAMedio</code> que nos sirva para encapsular los datos de cada medio, de los que debemos tener  * t\u00edtulo (<code>NSString</code>)  * artista (<code>NSString</code>)  * album (<code>NSString</code>)  * portada (<code>UIImage</code>)  * url (<code>NSURL</code>)</p> <p>Crearemos una clase <code>UAFuenteDatos</code> que haga de fuente de datos, y nos proporcione acceso a la lista de medios de cada tipo de forma centralizada. Podemos utilizar el patr\u00f3n singleton para dicha clase.</p> <p>Para implementar el patr\u00f3n singleton en Objective-C podemos utilizar un m\u00e9todo de clase como el siguiente:</p> <pre><code>+ (UAFuenteDatos*) sharedFuenteDatos {\nstatic UAFuenteDatos *datos = nil;\nif(nil == datos) {\ndatos = [[UAFuenteDatos alloc] init];\n}\nreturn datos;\n</code></pre> <p>La clase <code>UAFuenteDatos</code> debe proporcionarnos 3 listados: * Lista de audios * Lista de videos bajo demanda * Lista de emisiones en directo</p> <p>Todos ellos ser\u00e1n arrays de objetos de tipo <code>UAMedio</code>. </p> <p>Las listas de audio y v\u00eddeo ser\u00e1n fijas, podemos crearlas directamente desde c\u00f3digo. En otras asignaturas veremos c\u00f3mo obtenerlas de una base de datos o de servicios en la red.</p>"},{"location":"proyecto.html#acerca-de","title":"Acerca de","text":"<p>Crearemos una pantalla \"Acerca de...\" para la aplicaci\u00f3n iOS en la que veremos el nombre de la aplicaci\u00f3n, el nombre del autor, y una vista propia creada con Core Graphics que mostrar\u00e1 un gr\u00e1fico de tipo tarta que indicar\u00e1 la proporci\u00f3n de cada tipo de medio (directo, VOD y audio) disponible en la plataforma (a partir de los datos que proporciona <code>UAFuenteDatos</code>).</p> <p>Para crear esta pantalla aprovecharemos el controlador First View Controller. Podemos cambiar su nombre para que pase a ser \"Acerca de\". </p> <p>Como posibles mejores se propone:</p> <ul> <li>A\u00f1adir alguna animaci\u00f3n utilizando Core Animation.</li> <li>A\u00f1adir alguna gr\u00e1fica alternativa. </li> </ul>"},{"location":"proyecto.html#estructura-de-la-aplicacion","title":"Estructura de la aplicaci\u00f3n","text":"<p>Vamos a crear el resto de la estructura de la aplicaci\u00f3n, con una pesta\u00f1a para cada secci\u00f3n. Para ello:</p> <ol> <li> <p>Eliminamos Second View Controller del storyboard</p> </li> <li> <p>Arrastramos tres Table View Controller sobre el storyboard</p> </li> <li> <p>Podemos darles a estos controladores un nombre identificativo (Radio, Videoclub, Directo)</p> </li> <li> <p>Conectamos cada controlador con el Tab Bar Controller con un segue de tipo Relationship, para que aparezcan como pesta\u00f1as del mismo (seleccionamos el Table View Controller, y desde la pesta\u00f1a Connections del inspector conectamos el segue relationship con el controlador Tab Bar.</p> </li> <li> <p>Introducimos cada Table View Controller dentro de un Navigation Controller, seleccion\u00e1ndolo en el storyboard y pulsando Editor &gt; Embed In &gt; Navigation Controller.</p> </li> <li> <p>En cada controlador establecemos el nombre e icono para su pesta\u00f1a.</p> <p>Podemos encontrar packs de iconos de estilo line gratuitos para la aplicaci\u00f3n iOS: http://www.iconbeast.com http://www.flaticon.com/packs/line-icon-set</p> </li> <li> <p>Creamos una clase que herede de <code>UITableViewController</code> para cada controlador, y las conectamos desde el inspector de identidades estos controladores.</p> </li> <li> <p>En cada controlador creamos una propiedad que almacene una lista de medios. Inicializaremos dicha lista en el m\u00e9todo <code>viewDidLoad</code>, a partir de las listas de objetos <code>UAMedio</code> que nos proporciona <code>UAFuenteDatos</code>. Poblaremos las tablas con los datos obtenidos de estas listas de medios en cada una de las pantallas (\"Radio\", \"Videoclub\" y \"Directo\").</p> </li> </ol>"},{"location":"proyecto.html#reproduccion-de-medios-en-ios-sesion-de-27-10-2015","title":"Reproducci\u00f3n de medios en iOS (sesi\u00f3n de 27-10-2015)","text":""},{"location":"proyecto.html#reproduccion-de-audio","title":"Reproducci\u00f3n de audio","text":"<p>Al pulsar sobre un elemento de la lista de audios deberemos comenzar su reproducci\u00f3n. Para ello:</p> <ul> <li>En la clase del controlador de la pantalla de \"Radio\", crearemos una propiedad  de tipo <code>AVAudioPlayer</code> que utilizaremos como reproductor de audio.</li> <li> <p>En el evento de pulsaci\u00f3n de un elemento de la tabla <code>tableView:didSelectRowAtIndexPath</code> comenzaremos la reproducci\u00f3n del audio seleccionado.</p> <p>Los audios los almacenaremos en el dispositivo. Pod\u00e9is descargar m\u00fasica MP3 gratuita desde diferentes repositorios: http://freemusicarchive.org</p> </li> <li> <p>El audio debe continuar reproduci\u00e9ndose aunque se bloquee la pantalla, se silencie el tel\u00e9fono, o se pase a otra aplicaci\u00f3n. Introduce la configuraci\u00f3n necesaria para conseguir este comportamiento.</p> </li> </ul> <p>Como posibles mejoras se propone:</p> <ul> <li>Introduce informaci\u00f3n del audio que se est\u00e1 reproduciendo en <code>MPNowPlayingInfoCenter</code> para que as\u00ed se muestre informaci\u00f3n en la pantalla de bloqueo.</li> <li>Haz que la aplicaci\u00f3n responda ante el control remoto del audio, permitiendo pausar o reanudar la reproducci\u00f3n actual desde la pantalla de bloqueo o desde el bot\u00f3n de los auriculares.</li> <li>Haz que el desconectar los auriculares se detenga autom\u00e1ticamente la reproducci\u00f3n del audio.</li> </ul>"},{"location":"proyecto.html#reproduccion-de-vod","title":"Reproducci\u00f3n de VOD","text":"<p>Accederemos a los v\u00eddeos mediante Wowza (debemos tener publicados v\u00eddeos en VOD). M\u00e1s adelante publicaremos fuentes de emisi\u00f3n en directo.</p> <p>Al seleccionar un v\u00eddeo de la lista mostraremos el controlador del reproductor asign\u00e1ndole la URL correspondiente.</p> <p>Como mejora se proponer crear un controlador de reproducci\u00f3n de v\u00eddeo propio, que en caso de estar en orientaci\u00f3n vertical muestre un fondo decorativo para la parte de la pantalla que queda en negro.</p> <p>IMPORTANTE: A partir de iOS 9 s\u00f3lo se permite acceder a los v\u00eddeos mediante HTTPS, mientras que Wowza por defecto funciona sobre HTTP. Como soluci\u00f3n temporal, se puede a\u00f1adir la siguiente configuraci\u00f3n a <code>Info.plist</code> para permitir v\u00eddeos por HTTP, aunque lo recomendable es disponer de los contenidos mediante HTTPS: <pre><code>&lt;key&gt;NSAppTransportSecurity&lt;/key&gt;\n&lt;dict&gt;\n&lt;key&gt;NSAllowsArbitraryLoads&lt;/key&gt;\n&lt;true/&gt;\n&lt;/dict&gt;\n</code></pre></p>"},{"location":"proyecto.html#emision-de-video-en-directo-desde-android-sesion-de-3-11-2015","title":"Emisi\u00f3n de v\u00eddeo en directo desde Android (sesi\u00f3n de 3-11-2015)","text":"<p>Vamos a desarrollar una aplicaci\u00f3n Android que utilice la librer\u00eda libstreaming para enviar v\u00eddeo por RTSP desde un m\u00f3vil Android a Wowza. Consideramos que podremos tener una lista predeterminada de canales en el servidor, todos ellos bajo la aplicaci\u00f3n <code>live</code>, por ejemplo:</p> <pre><code>/live/canalA.stream\n/live/canalB.stream\n/live/canalC.stream\n/live/canalD.stream\n</code></pre> <p>En la pantalla principal de la aplicaci\u00f3n Android tendremos un men\u00fa con las siguientes opciones:</p> <ul> <li>Configurar la emisi\u00f3n</li> <li>Comenzar la emisi\u00f3n</li> <li>Acerca de</li> </ul>"},{"location":"proyecto.html#configurar-la-emision","title":"Configurar la emisi\u00f3n","text":"<p>Nos permitir\u00e1:</p> <ul> <li>Establecer el canal en el que emitir entre la lista de canales predeterminados que vayamos a considerar para nuestra aplicaci\u00f3n.</li> <li>Establecer la calidad del v\u00eddeo a emitir, entre una serie de posibles presets que deberemos definir.</li> </ul>"},{"location":"proyecto.html#comenzar-la-emision","title":"Comenzar la emisi\u00f3n","text":"<p>Lanzar\u00e1 una pantalla que establecer\u00e1 la conexi\u00f3n con la URL del servidor Wowza (la URL depender\u00e1 del canal que hayamos seleccionado), y una vez haya conectado mostraremos una previsualizaci\u00f3n del v\u00eddeo que se est\u00e9 emitiendo. Deber\u00e1 mostrar tambi\u00e9n un bot\u00f3n para detener la emisi\u00f3n, con el que adem\u00e1s volveremos al men\u00fa principal.</p>"},{"location":"proyecto.html#acerca-de_1","title":"Acerca de","text":"<p>Mostrar\u00e1 el nombre de la aplicaci\u00f3n y del autor de la misma. Debe permitir volver al men\u00fa principal.</p>"},{"location":"proyecto.html#recepcion-en-ios","title":"Recepci\u00f3n en iOS","text":"<p>Deberemos actualizar la aplicaci\u00f3n iOS para que en la secci\u00f3n de directos muestre la lista de posibles canales que hayamos predefinido. Al pulsar sobre alguno de ellos mostrar\u00e1 un reproductor de v\u00eddeo que conectar\u00e1 a la URL adecuada para mostrar el directo mediante HTTP Live Streaming.</p>"},{"location":"proyecto.html#entrega-del-proyecto","title":"Entrega del proyecto","text":"<p>El proyecto deber\u00e1 entregarse antes del lunes 16 de noviembre a las 23:55, a trav\u00e9s de la plataforma Moodle. Deber\u00e1 compartirse tambi\u00e9n el repositorio de bitbucket en el que se encuentre con el usuario <code>entregas-mastermoviles</code>.</p> <p>El proyecto se puntuar\u00e1 de 0 a 10, con el siguiente reparto:</p> <ul> <li>7 puntos: Funcionamiento correcto de los requisitos b\u00e1sicos del proyecto y seguimiento del trabajo.</li> <li>3 puntos: Implementaci\u00f3n de mejoras adicionales propuestas, mejoras adicionales propias, y acabado est\u00e9tico.</li> </ul>"},{"location":"reproduccion-android.html","title":"Reproducci\u00f3n de medios en Android","text":"<p>La capacidad de reproducir contenido multimedia es una caracter\u00edstica presente en la pr\u00e1ctica totalidad de las terminales telef\u00f3nicas existentes en el mercado hoy en d\u00eda. Muchos usuarios prefieren utilizar las capacidades multimedia de su tel\u00e9fono, en lugar de tener que depender de otro dispositivo adicional para ello.</p> <p>En esta sesi\u00f3n vamos a aprender a a\u00f1adir contenido multimedia en nuestras aplicaciones. En concreto, veremos c\u00f3mo reproducir audio o video en una actividad.</p> <p>La reproducci\u00f3n de contenido multimedia se lleva a cabo por medio de la clase <code>MediaPlayer</code>. Dicha clase nos permite la reproducci\u00f3n de archivos multimedia almacenados como recursos de la aplicaci\u00f3n, en ficheros locales, en proveedores de contenido, o servidos por medio de streaming a partir de una URL. En todos los casos, como desarrolladores, la clase <code>MediaPlayer</code> nos permitir\u00e1 abstraernos del formato as\u00ed como del origen del fichero a reproducir.</p>"},{"location":"reproduccion-android.html#reproduccion-de-audio","title":"Reproducci\u00f3n de audio","text":""},{"location":"reproduccion-android.html#tipos-de-fuentes-de-audio","title":"Tipos de fuentes de audio","text":"<p>Podemos reproducir audio proveniente de diferentes fuentes:</p> <ul> <li>Un recurso de la aplicaci\u00f3n</li> <li>Un fichero en el dispositivo local</li> <li>Una URL remota</li> </ul> <p>Incluir un fichero de audio en los recursos de la aplicaci\u00f3n para poder ser reproducido durante su ejecuci\u00f3n es muy sencillo. Simplemente creamos una carpeta <code>raw</code> dentro de la carpeta <code>res</code>, y almacenamos en ella sin comprimir el fichero o ficheros que deseamos reproducir. A partir de ese momento el fichero se identificar\u00e1 dentro del c\u00f3digo como <code>R.raw.nombre_fichero</code> (obs\u00e9rvese que no es necesario especificar la extensi\u00f3n del fichero).</p> <p>Para reproducir un fichero de audio almacenado en el m\u00f3vil o en un servidor remoto simplemente deberemos especificar su URL (local o remota).</p> <p>Importante: En caso de necesitar reproducir un medio alojado en una URL remota, ser\u00e1 necesario que la aplicaci\u00f3n solicite el permiso <code>INTERNET</code> en el fichero <code>AndroidManifest.xml</code>: <pre><code>&lt;uses-permission android:name=\"android.permission.INTERNET\" /&gt;\n</code></pre></p>"},{"location":"reproduccion-android.html#inicializacion-del-reproductor-de-medios","title":"Inicializaci\u00f3n del reproductor de medios","text":"<p>Para reproducir un fichero de audio tendremos que seguir una secuencia de pasos. En primer lugar deberemos crear una instancia de la clase <code>MediaPlayer</code> e indicar qu\u00e9 fichero ser\u00e1 el que se reproducir\u00e1. Tenemos dos opciones para hacer esto.</p> <p>La primera opci\u00f3n para inicializar la reproducci\u00f3n multimedia es por medio del m\u00e9todo <code>setDataSource</code>, el cual asigna una fuente multimedia a una instancia ya existente de la clase <code>MediaPlayer</code>.</p> <pre><code>MediaPlayer mediaPlayer = new MediaPlayer();\nmediaPlayer.setDataSource(\"/sdcard/test.mp3\");\nmediaPlayer.prepare();\n</code></pre> <p>Al instanciar la clase <code>MediaPlayer</code> se encontrar\u00e1 en estado idle. En este estado lo primero que debemos hacer es indicar el fichero a reproducir. Una vez hecho esto pasa a estado inicializado. En este estado ya sabe qu\u00e9 fichero ha de reproducir, pero todav\u00eda no se ha preparado para ello (inicializar bufferes, etc), por lo que no podr\u00e1 comenzar la reproducci\u00f3n. Para prepararlo deberemos llamar al m\u00e9todo <code>prepare()</code>, con lo que tendremos el reproductor listo para empezar a reproducir el audio.</p> <p>La segunda opci\u00f3n consiste en crear una instancia de la clase <code>MediaPlayer</code> por medio del m\u00e9todo <code>create</code>. En este caso se deber\u00e1 pasar como par\u00e1metro, adem\u00e1s del contexto de la aplicaci\u00f3n, el identificador del recurso, como se puede ver en el siguiente ejemplo:</p> <pre><code>Context appContext = getApplicationContext();\n\n// Recurso de la aplicaci\u00f3n\nMediaPlayer resourcePlayer =\nMediaPlayer.create(appContext, R.raw.my_audio);\n// Fichero local (en la tarjeta de memoria)\nMediaPlayer filePlayer =\nMediaPlayer.create(appContext, Uri.parse(\"file:///sdcard/localfile.mp3\"));\n// URL\nMediaPlayer urlPlayer =\nMediaPlayer.create(appContext, Uri.parse(\"http://site.com/audio/audio.mp3\"));\n// Proveedor de contenido\nMediaPlayer contentPlayer =\nMediaPlayer.create(appContext, Settings.System.DEFAULT_RINGTONE_URI);\n</code></pre> <p>En este caso el m\u00e9todo <code>create()</code> se encarga de asignar la fuente de audio y adem\u00e1s pasar el reproductor a estado preparado. Por lo tanto, en este caso no ser\u00e1 necesario llamar a <code>prepare()</code>, sino que podremos reproducir el medio directamente. Aunque es m\u00e1s sencillo que la primera opci\u00f3n, tambi\u00e9n resulta menos flexible.</p>"},{"location":"reproduccion-android.html#inicializacion-asincrona","title":"Inicializaci\u00f3n as\u00edncrona","text":"<p>La llamada a <code>prepare()</code> puede resultar bastante costosa y producir un retardo considerable, especialmente cuando accedemos a medios externos a la aplicaci\u00f3n. Por este motivo debemos evitar llamarla desde el hilo de eventos, para evitar bloquear este hilo.</p> <p>Podemos crear un hilo secundario y realizar la preparaci\u00f3n del medio desde \u00e9l, pero tambi\u00e9n contamos con una variante del m\u00e9todo anterior que nos facilitar\u00e1 realizar la preparaci\u00f3n de forma as\u00edncrona, fuera del hilo de eventos. Esta variante es <code>prepareAsync()</code>.</p> <p>Podemos utilizar un listener de tipo <code>MediaPlayer.OnPreparedListener</code> para que se nos notifique con el m\u00e9todo <code>onPrepared</code> cu\u00e1ndo est\u00e1 preparado el reproductor, y as\u00ed poder comenzar la reproducci\u00f3n. Este listener se deber\u00e1 registrar en el <code>MediaPlayer</code> con el m\u00e9todo <code>setOnPreparedListener()</code>:</p> <pre><code>public class MiActividad extends Activity implements MediaPlayer.OnPreparedListener {\nMediaPlayer mMediaPlayer = null;\n\npublic void reproducir() {\nmMediaPlayer = new MediaPlayer();\nmMediaPlayer.setOnPreparedListener(this);\nmMediaPlayer.prepareAsync();\n}\n\npublic void onPrepared(MediaPlayer player) {\nmMediaPlayer.start();\n}\n}\n</code></pre>"},{"location":"reproduccion-android.html#metodos-del-reproductor-de-medios","title":"M\u00e9todos del reproductor de medios","text":"<p>Una vez que la instancia de la clase <code>MediaPlayer</code> ha sido inicializada, podemos comenzar la reproducci\u00f3n mediante el m\u00e9todo <code>start</code>. Tambi\u00e9n es posible utilizar los m\u00e9todos <code>stop</code> y <code>pause</code> para detener y pausar la reproducci\u00f3n. Si se detuvo la reproducci\u00f3n de audio mediante el m\u00e9todo <code>stop</code> ser\u00e1 imprescindible invocar el m\u00e9todo <code>prepare</code> antes de poder reproducirlo de nuevo mediante una llamada a <code>start</code>. Por otra parte, si se detuvo la reproducci\u00f3n por medio de <code>pause</code>, tan s\u00f3lo ser\u00e1 necesario hacer una llamada a <code>start</code> para continuar en el punto donde \u00e9sta se dej\u00f3.</p> <p>Otros m\u00e9todos de la clase <code>MediaPlayer</code> que podr\u00edamos considerar interesante utilizar son los siguientes:</p> <ul> <li><code>setLooping</code> nos permite especificar si el clip de audio deber\u00e1 volver a reproducirse cada vez que finalice.</li> </ul> <pre><code>if (!mediaPlayer.isLooping())\nmediaPlayer.setLooping(true);\n</code></pre> <ul> <li><code>setScreenOnWhilePlaying</code> nos permitir\u00e1 conseguir que la pantalla se encuentre activada siempre durante la reproducci\u00f3n. Tiene m\u00e1s sentido en el caso de la reproducci\u00f3n de video, que ser\u00e1 tratada en la siguiente secci\u00f3n.</li> </ul> <pre><code>mediaPlayer.setScreenOnWhilePlaying(true);\n</code></pre> <ul> <li><code>setVolume</code> modifica el volumen. Recibe dos par\u00e1metros que deber\u00e1n ser dos n\u00fameros reales entre 0 y 1, indicando el volumen del canal izquierdo y del canal derecho, respectivamente. El valor 0 indica silencio total mientras que el valor 1 indica m\u00e1ximo volumen.</li> </ul> <pre><code>mediaPlayer.setVolume(1f, 0.5f);\n</code></pre> <ul> <li><code>seekTo</code> permite avanzar o retroceder a un determinado punto del archivo de audio. Podemos obtener la duraci\u00f3n total del clip de audio con el m\u00e9todo <code>getDuration</code>, mientras que <code>getCurrentPosition</code> nos dar\u00e1 la posici\u00f3n actual. En el siguiente c\u00f3digo se puede ver un ejemplo de uso de estos tres \u00faltimos m\u00e9todos.</li> </ul> <pre><code>mediaPlayer.start();\n\nint pos = mediaPlayer.getCurrentPosition();\nint duration = mediaPlayer.getDuration();\n\nmediaPlayer.seekTo(pos + (duration-pos)/10);\n</code></pre>"},{"location":"reproduccion-android.html#liberacion-del-reproductor-de-medios","title":"Liberaci\u00f3n del reproductor de medios","text":"<p>Una acci\u00f3n muy importante que deberemos llevar a cabo una vez haya finalizado definitivamente la reproducci\u00f3n (porque se vaya a salir de la aplicaci\u00f3n o porque se vaya a cerrar la actividad donde se reproduce el audio) es destruir la instancia de la clase <code>MediaPlayer</code> y liberar su memoria. Para ello deberemos hacer uso del m\u00e9todo <code>release</code>.</p> <pre><code>if(mediaPlayer!=null) {\nmediaPlayer.release();\nmediaPlayer = null;\n}\n</code></pre> <p>Por ejemplo, si nuestra actividad crea un reproductor en <code>start()</code>, deberemos asegurarnos de destruirlo siempre en <code>stop()</code> para evitar que pueda haber m\u00e1s de uno simult\u00e1neamente.</p>"},{"location":"reproduccion-android.html#streams-de-audio-y-control-de-volumen","title":"Streams de audio y control de volumen","text":"<p>Android reproduce el audio en diferentes streams seg\u00fan su naturaleza: m\u00fasica, alarmas, notificaciones, llamadas, etc. Cada uno de estos streams tiene su propio nivel de volumen, de forma que el volumen del tono de llamada puede ser distinto al volumen del reproductor de m\u00fasica. Cuando el usuario manipule los botones de control de volumen del dispositivo, estar\u00e1 modificando el volumen del stream que est\u00e9 sonando actualmente.</p> <p>A la hora de reproducir sonido con el <code>MediaPlayer</code> podemos especificar el stream dentro del cual lo queremos reproducir con <code>setAudioStreamType</code>. Siempre llamaremos a este m\u00e9todo en el estado idle, antes de haber especificado la fuente de audio:</p> <pre><code>MediaPlayer mediaPlayer = new MediaPlayer();\nmediaPlayer.setAudioStreamType(AudioManager.STREAM_MUSIC);\nmediaPlayer.setDataSource(getApplicationContext(), \"sdcard/test.mp3\");\nmediaPlayer.prepare();\nmediaPlayer.start();\n</code></pre> <p>Es importante destacar que no podremos especificar el stream de audio si lo creamos con el atajo <code>create</code>.</p> <p>Como hemos comentado, cuando usamos los botones de control de volumen del dispositivos afectamos al stream que est\u00e9 sonando actualmente. Si no estuviese sonando ning\u00fan audio, entonces por defecto afectar\u00eda al stream del tono de llamada.</p> <p>Si estamos dentro de una aplicaci\u00f3n como por ejemplo un videojuego, aunque no est\u00e9 sonando nada normalmente nos interesar\u00e1 que al manipular el control de volumen dentro del videojuego se modifique el volumen de m\u00fasica del juego, y no el del todo de llamada. Para indicar a qu\u00e9 stream debe afectar el volumen utilizaremos el m\u00e9todo <code>setVolumenControlStream()</code>:</p> <pre><code>setVolumeControlStream(AudioManager.STREAM_MUSIC);\n</code></pre> <p>Cuando la actividad o fragmento en el que hayamos llamado al m\u00e9todo anterior est\u00e9 visible, el control de volumen afectar\u00e1 al stream de m\u00fasica, y no al por defecto.</p>"},{"location":"reproduccion-android.html#reproduccion-de-audio-en-segundo-plano","title":"Reproducci\u00f3n de audio en segundo plano","text":"<p>Puede intersarnos dejar nuestro reproductor funcionando en segundo plano aunque cerremos la actividad, por ejemplo para implementar un reproductor de m\u00fasica o de podcasts. Para hacer esto deberemos iniciar el reproductor desde un servicio, en lugar de una actividad:</p> <pre><code>public class MiServicio extends Service implements MediaPlayer.OnPreparedListener {\nprivate static final String ACTION_PLAY = \"es.ua.jtech.action.PLAY\";\nprivate static final String EXTRA_URI = \"es.ua.jtech.extras.URI\";\nMediaPlayer mMediaPlayer = null;\n\npublic int onStartCommand(Intent intent, int flags, int startId) {\nif (intent.getAction().equals(ACTION_PLAY)) {\nmMediaPlayer = new MediaPlayer();\nmMediaPlayer.setDataSource(intent.getExtra(EXTRA_URI));\nmMediaPlayer.setOnPreparedListener(this);\nmMediaPlayer.prepareAsync();\n}\n}\n\npublic void onPrepared(MediaPlayer player) {\nplayer.start();\n}\n}\n</code></pre> <p>Cuando el m\u00f3vil no se est\u00e9 utilizando normalmente la pantalla y la CPU se apagar\u00e1n autom\u00e1ticamente para ahorrar bater\u00eda. Si esto ocurre la m\u00fasica se detendr\u00e1. Para evitarlo necesitamos adquirir lo que se conoce como un wake lock. Esto nos permitir\u00e1 mantener el m\u00f3vil activo aunque el usuario no lo est\u00e9 utilizando.</p> <p>Para poder obtener un wake lock lo primero que deberemos es solicitar el permiso correspondiente:</p> <pre><code>&lt;uses-permission android:name=\"android.permission.WAKE_LOCK\" /&gt;\n</code></pre> <p>Una vez contemos con el permiso, podemos indicar al reproductor de medios que necesitamos un wake lock parcial (se puede apagar la pantalla pero no la CPU):</p> <pre><code>mMediaPlayer.setWakeMode(getApplicationContext(), PowerManager.PARTIAL_WAKE_LOCK);\n</code></pre> <p>Si estamos accediendo a un fichero remoto deberemos indicar tambi\u00e9n que no se apague la WiFi, mediante un wifi lock:</p> <pre><code>WifiLock wifiLock = ((WifiManager) getSystemService(Context.WIFI_SERVICE))\n.createWifiLock(WifiManager.WIFI_MODE_FULL, \"wifilock\");\n\nwifiLock.acquire();\n</code></pre> <p>Al parar o pausar la reproducci\u00f3n podemos liberar este lock para as\u00ed ahorrar bater\u00eda:</p> <pre><code>wifiLock.release();\n</code></pre> <p>Aunque la reproducci\u00f3n se produzca en segundo plano, normalmente el usuario querr\u00e1 estar al tanto de la reproducci\u00f3n y poder deternerla. Por ello es conveniente que el servicio funcione en modo foreground. Para ello deberemos crear una notificaci\u00f3n de tipo ongoing (evento en curso) que se mostrar\u00e1 en la barra de notificaciones mientras el servicio est\u00e9 activo. Llamando a <code>startForeground()</code> desde nuestro servicio haremos que dicha notificaci\u00f3n se muestre mientras el servicio est\u00e9 activo:</p> <pre><code>// Crea un PendingIntent para iniciar la actividad ReproductorActivity\nPendingIntent pi = PendingIntent.getActivity(getApplicationContext(), 0,\nnew Intent(getApplicationContext(), ReproductorActivity.class),\nPendingIntent.FLAG_UPDATE_CURRENT);\n\n// Creamos una notificaci\u00f3n que lance el PendingIntentAnterior\nNotification notification = new Notification.Builder(mContext)\n.setContentTitle(\"Reproductor de musica\")\n.setContentText(\"Reproduciendo \" + nombreCancion)\n.setSmallIcon(R.drawable.icono_play)\n.setTicker(\"Reproduccion en curso\")\n.setOngoing(true)\n.addAction(R.drawable.icono_play, \"Reproductor de musica\", pi)\n.build();\n\nstartForeground(NOTIFICATION_ID, notification);\n</code></pre> <p>En el c\u00f3digo anterior podemos ver que hemos a\u00f1adido a la notificaci\u00f3n un <code>PendingIntent</code> para que al pulsar sobre ella se abra la actividad del reproductor, y as\u00ed el usuario podr\u00eda pausar, detener, o cambiar la m\u00fasica que est\u00e9 sonando.</p> <p>Cuando queramos que nuestro servicio deje de funcionar en modo foreground llamaremos a:</p> <pre><code>stopForeground(true);\n</code></pre> <p>Tambi\u00e9n podr\u00edamos incluir en la propia notificaci\u00f3n controles para poder parar o reanudar el audio. Para ello podr\u00edamos utilizar un objeto de tipo <code>RemoteViews</code> y cargar en \u00e9l el layout que queramos que tenga la notificaci\u00f3n. Al construir la notificaci\u00f3n podremos establecer la vista remota a aplicar con <code>setContent</code>. </p> <pre><code>RemoteViews remoteViews = new RemoteViews(getPackageName(), R.layout.notificacion_audio);\n\nintent = new Intent(ACTION_PLAY);       pendingIntent = PendingIntent.getService(getApplicationContext(),\nREQUEST_CODE_PLAY, intent,\nPendingIntent.FLAG_UPDATE_CURRENT);\n\nremoteViews.setOnClickPendingIntent(R.id.boton_play,\npendingIntent);\n\nintent = new Intent(ACTION_PAUSE);       pendingIntent = PendingIntent.getService(getApplicationContext(),\nREQUEST_CODE_PAUSE, intent,\nPendingIntent.FLAG_UPDATE_CURRENT);\n\nremoteViews.setOnClickPendingIntent(R.id.boton_pause,\npendingIntent);\n\nNotification notification = new NotificationCompat.Builder(getApplicationContext())\n.setSmallIcon(R.drawable.ic_launcher).setOngoing(true)\n.setWhen(System.currentTimeMillis())                .setContent(remoteViews)\n.build();\n\nstartForeground(NOTIFICATION_ID, notification);     </code></pre> <p>Al pulsar los botones de la notificaci\u00f3n se lanzar\u00e1 un intent con una acci\u00f3n <code>ACTION_PLAY</code> o <code>ACTION_PAUSE</code>, definidas ambas por nuestra aplicaci\u00f3n. Haremos que nuestro servicio se ejecute cuando se lancen dichas acciones, a\u00f1adi\u00e9ndolas a un<code>&lt;intent-filter&gt;</code>:</p> <pre><code>&lt;service\nandroid:name=\"es.ua.jtech.MiServicio\"\nandroid:exported=\"false\" &gt;\n&lt;intent-filter&gt;\n&lt;action android:name=\"eps.ua.es.ACTION_PLAY\" /&gt;\n&lt;action android:name=\"eps.ua.es.ACTION_PAUSE\" /&gt;\n&lt;category android:name=\"android.intent.category.DEFAULT\" /&gt;\n&lt;/intent-filter&gt;\n&lt;/service&gt;\n</code></pre> <p>Deberemos actualizar el c\u00f3digo del servicio para que soporte tambi\u00e9n la acci\u00f3n <code>ACTION_PAUSE</code>:</p> <pre><code>public class MiServicio extends Service implements MediaPlayer.OnPreparedListener {\nprivate static final String ACTION_PLAY = \"es.ua.jtech.action.PLAY\";\nprivate static final String ACTION_PAUSE = \"es.ua.jtech.action.PAUSE\";\nprivate static final String EXTRA_URI = \"es.ua.jtech.extras.URI\";\nMediaPlayer mMediaPlayer = null;\n\npublic int onStartCommand(Intent intent, int flags, int startId) {\nif (intent.getAction().equals(ACTION_PLAY)) {\nif(mMediaPlayer == null) {\nmMediaPlayer = new MediaPlayer();\nmMediaPlayer.setDataSource(intent.getExtra(EXTRA_URI));\nmMediaPlayer.setOnPreparedListener(this);\nmMediaPlayer.prepareAsync();\n} else {\nmMediaPlayer.play();\n}\n} else if (intent.getAction().equals(ACTION_PAUSE)) {\nif(mMediaPlayer != null) {\nmMediaPlayer.pause();\n}\n}\n}\n\npublic void onPrepared(MediaPlayer player) {\nplayer.start();\n}\n}\n</code></pre> <p>A partir de Android 4.0 tambi\u00e9n podemos utilizar la clase <code>RemoteControlClient</code> para implementar estos controles, y a partir de Android 5.0 este mecanismo es reemplazado por la clase <code>MediaSession</code>. Podemos obtener m\u00e1s informaci\u00f3n sobre el nuevo mecanismo en:</p> <p>http://developer.android.com/guide/topics/ui/notifiers/notifications.html#controllingMedia</p>"},{"location":"reproduccion-android.html#gestion-del-foco-del-audio","title":"Gesti\u00f3n del foco del audio","text":"<p>La posibilidad de reproducir audio en un servicio en segundo plano nos trae tambi\u00e9n la problem\u00e1tica de poder tener varios sonidos reproduci\u00e9ndose simult\u00e1neamente. Por ejemplo, podemos estar reproduciendo m\u00fasia al mismo tiempo que suena la alerta por la llegada de un mensaje, lo cual puede suponer que no oigamos el aviso de mensaje.</p> <p>Para evitar este problema deberemos coordinar la reproducci\u00f3n de los diferentes audios. La forma que tiene Android de resolverlo es mediante un sistema en el que cada aplicaci\u00f3n podr\u00e1 solitar el foco para reproducir audio. Por ejemplo, si estamos reproduciendo m\u00fasica y otra aplicaci\u00f3n solicita el foco, podemos deterner la m\u00fasica moment\u00e1neamente o bajar su volumen (t\u00e9cnica conocida como ducking) mientras la otra aplicaci\u00f3n reproduce el sonido, hasta que nos devuelva el foco.</p> <p>En primer lugar deberemos solicitar el foco para reproducir audio mediante el servicio <code>AudioManager</code>:</p> <pre><code>AudioManager audioManager = (AudioManager) getSystemService(Context.AUDIO_SERVICE);\nint result = audioManager.requestAudioFocus(this,\nAudioManager.STREAM_MUSIC,\nAudioManager.AUDIOFOCUS_GAIN);\n\nif (result != AudioManager.AUDIOFOCUS_REQUEST_GRANTED) {\n// No se ha concedido el foco\n}\n</code></pre> <p>En este caso anterior vemos que hemos solicitado el foco para el stream de reproducci\u00f3n de m\u00fasica. Adem\u00e1s, al solicitar el foco debemos proporcional un listener de tipo <code>AudioManager.OnAudioFocusChangeListener</code> que tendr\u00e1  siguiente estructura:</p> <pre><code>class MiServicio extends Service\nimplements AudioManager.OnAudioFocusChangeListener {\n\npublic void onAudioFocusChange(int focusChange) {\nswitch (focusChange) {\ncase AudioManager.AUDIOFOCUS_GAIN:\nif (mMediaPlayer == null) {\ninitMediaPlayer();\n} else if (!mMediaPlayer.isPlaying()) {\nmMediaPlayer.start();\n}\nmMediaPlayer.setVolume(1.0f, 1.0f);\nbreak;\n\ncase AudioManager.AUDIOFOCUS_LOSS:\nif (mMediaPlayer.isPlaying()) {\nmMediaPlayer.stop();\n}\nmMediaPlayer.release();\nmMediaPlayer = null;\nbreak;\n\ncase AudioManager.AUDIOFOCUS_LOSS_TRANSIENT:\nif (mMediaPlayer.isPlaying()) {\nmMediaPlayer.pause();\n}\nbreak;\n\ncase AudioManager.AUDIOFOCUS_LOSS_TRANSIENT_CAN_DUCK:\nif (mMediaPlayer.isPlaying()) {\nmMediaPlayer.setVolume(0.1f, 0.1f);\n}\nbreak;\n}\n}\n}\n</code></pre> <p>Vemos en este ejemplo que hay varias mas de perder el foco:</p> <ul> <li><code>AUDIOFOCUS_LOSS</code>: Se pierde de forma definitiva. Detenemos y liberamos el reproductor.</li> <li><code>AUDIOFOCUS_LOSS_TRANSIENT</code>: Se pierde de forma temporal. Pausamos el reproductor.</li> <li><code>AUDIOFOCUS_LOSS_TRANSIENT_CAN_DUCK</code>: Se pierde de forma temporal y se permite que simplemente reduzcamos el volumen del audio actual (ducking).</li> </ul> <p>Cuando ganemos el foco de nuevo (<code>AUDIOFOCUS_GAIN</code>) deberemos inicializar el reproductor si no estaba inicializado todav\u00eda, reanudar la reproducci\u00f3n si se hab\u00eda pausado, y restaurar el volumen original.</p> <p>La caracter\u00edstica audio focus s\u00f3lo est\u00e1 disponible a partir de Android 2.2 (API 8). Si queremos tener compatibiliad con versiones anteriores deberemos comprobar la versi\u00f3n de Android en c\u00f3digo y s\u00f3lo utilizar audio focus si es mayor o igual que la 8.</p>"},{"location":"reproduccion-android.html#desconexion-de-auriculares","title":"Desconexi\u00f3n de auriculares","text":"<p>Estamos acostumbrados a ver que en las aplicaciones m\u00f3viles para reproducir m\u00fasica, al desconectar los auriculares la reproducci\u00f3n se detiene autom\u00e1ticamente para evitar que el ruido repentino pudiera causar molestias.</p> <p>Este comportamiento no es autom\u00e1tico, sino que lo deberemos programar nosotros. Para ello deberemos capturar el intent <code>AUDIO_BECOMING_NOISY</code> mediante un broadcast receiver:</p> <pre><code>&lt;receiver android:name=\".MusicIntentReceiver\"&gt;\n&lt;intent-filter&gt;\n&lt;action android:name=\"android.media.AUDIO_BECOMING_NOISY\" /&gt;\n&lt;/intent-filter&gt;\n&lt;/receiver&gt;\n</code></pre> <p>Dentro del broadcast receiver podremos por ejemplo realizar la acci\u00f3n de detener la reproducci\u00f3n actual:</p> <pre><code>public class MusicIntentReceiver extends android.content.BroadcastReceiver {\n@Override\npublic void onReceive(Context ctx, Intent intent) {\nif (intent.getAction().equals(\nandroid.media.AudioManager.ACTION_AUDIO_BECOMING_NOISY)) {\nstopMusic();\n}\n}\n}\n</code></pre>"},{"location":"reproduccion-android.html#reproduccion-de-video","title":"Reproducci\u00f3n de v\u00eddeo","text":""},{"location":"reproduccion-android.html#reproducir-video-mediante-videoview","title":"Reproducir v\u00eddeo mediante VideoView","text":"<p>La reproducci\u00f3n de v\u00eddeo es muy similar a la reproducci\u00f3n de audio, salvo dos particularidades. En primer lugar, no es posible reproducir un clip de v\u00eddeo almacenado como parte de los recursos de la aplicaci\u00f3n. En este caso no existe ning\u00fan m\u00e9todo para reproducir un v\u00eddeo a partir de un <code>id</code> de recurso. Lo que si que podemos hacer es representar un recurso <code>raw</code> de tipo v\u00eddeo mediante una URL. Esta URL tendr\u00e1 la siguiente forma (hay que especificar como host el paquete de nuestra aplicaci\u00f3n, y como ruta el id del recurso):</p> <pre><code>Uri.parse(\"android.resource://es.ua.jtech/\" + R.raw.video)\n</code></pre> <p>En segundo lugar, el v\u00eddeo necesitar\u00e1 de una superficie para poder reproducirse. Esta superficie se corresponder\u00e1 con una vista dentro del layout de la actividad.</p> <p>Existen varias alternativas para la reproducci\u00f3n de v\u00eddeo, teniendo en cuenta lo que acabamos de comentar. La m\u00e1s sencilla es hacer uso de una vista de tipo <code>VideoView</code>, que encapsula tanto la creaci\u00f3n de una superficie en la que reproducir el v\u00eddeo como el control del mismo mediante una instancia de la clase <code>MediaPlayer</code>. Este m\u00e9todo ser\u00e1 el que veamos en primer lugar.</p> <p>El primer paso consistir\u00e1 en a\u00f1adir la vista <code>VideoView</code> a la interfaz gr\u00e1fica de la aplicaci\u00f3n. Para ello a\u00f1adimos el elemento en el archivo de layout correspondiente:</p> <pre><code>&lt;VideoView android:id=\"@+id/superficie\"\nandroid:layout_height=\"fill_parent\"\nandroid:layout_width=\"fill_parent\"&gt;\n&lt;/VideoView&gt;\n</code></pre> <p>Dentro del c\u00f3digo Java podremos acceder a dicho elemento de la manera habitual, es decir, mediante el m\u00e9todo <code>findViewById</code>. Una vez hecho esto, asignaremos una fuente que se corresponder\u00e1 con el contenido multimedia a reproducir. El <code>VideoView</code> se encargar\u00e1 de la inicializaci\u00f3n del objeto <code>MediaPlayer</code>. Para asignar un video a reproducir podemos utilizar cualquiera de estos dos m\u00e9todos:</p> <pre><code>videoView1.setVideoUri(\"http://www.mysite.com/videos/myvideo.3gp\");\nvideoView2.setVideoPath(\"/sdcard/test2.3gp\");\n</code></pre> <p>Una vez inicializada la vista se puede controlar la reproducci\u00f3n con los m\u00e9todos <code>start</code>, <code>stopPlayback</code>, <code>pause</code> y <code>seekTo</code>. La clase <code>VideoView</code> tambi\u00e9n incorpora el m\u00e9todo <code>setKeepScreenOn(boolean)</code>con la que se podr\u00e1 controlar el comportamiento de la iluminaci\u00f3n de la pantalla durante la reproducci\u00f3n del clip de v\u00eddeo. Si se pasa como par\u00e1metro el valor <code>true</code> \u00e9sta permanecer\u00e1 constantemente iluminada.</p> <p>El siguiente c\u00f3digo muestra un ejemplo de asignaci\u00f3n de un v\u00eddeo a una vista <code>VideoView</code> y de su posterior reproducci\u00f3n. Dicho c\u00f3digo puede ser utilizado a modo de esqueleto en nuestra propia aplicaci\u00f3n. Tambi\u00e9n podemos ver un ejemplo de uso de <code>seekTo</code>, en este caso para avanzar hasta la posici\u00f3n intermedia del video.</p> <pre><code>VideoView videoView = (VideoView)findViewById(R.id.superficie);\nvideoView.setKeepScreenOn(true);\nvideoView.setVideoPath(\"/sdcard/ejemplo.3gp\");\n\nif (videoView.canSeekForward())\nvideoView.seekTo(videoView.getDuration()/2);\n\nvideoView.start();\n\n// Hacer algo durante la reproducci\u00f3n\n\nvideoView.stopPlayback();\n</code></pre>"},{"location":"reproduccion-android.html#reproducir-video-con-mediaplayer","title":"Reproducir v\u00eddeo con MediaPlayer","text":"<p>La segunda alternativa para la reproducci\u00f3n de v\u00eddeo consiste en la creaci\u00f3n de una superficie en la que dicho v\u00eddeo se reproducir\u00e1 y en el uso directo de la clase <code>MediaPlayer</code>. La superficie deber\u00e1 ser asignada manualmente a la instancia de la clase <code>MediaPlayer</code>. En caso contrario el v\u00eddeo no se mostrar\u00e1. Adem\u00e1s, la clase <code>MediaPlayer</code> requiere que la superficie sea un objeto de tipo <code>SurfaceHolder</code>.</p> <p>Un ejemplo de objeto <code>SurfaceHolder</code> podr\u00eda ser la vista <code>SurfaceView</code>, que podremos a\u00f1adir al XML del layout correspondiente:</p> <pre><code>&lt;SurfaceView\nandroid:id=\"@+id/superficie\"\nandroid:layout_width=\"200dip\"\nandroid:layout_height=\"200dip\"\nandroid:layout_gravity=\"center\"&gt;\n&lt;/SurfaceView&gt;\n</code></pre> <p>El siguiente paso ser\u00e1 la inicializaci\u00f3n el objeto <code>SurfaceView</code> y la asignaci\u00f3n del mismo a la instancia de la clase <code>MediaPlayer</code> encargada de reproducir el v\u00eddeo. El siguiente c\u00f3digo muestra c\u00f3mo hacer esto. Obs\u00e9rvese que es necesario que la actividad implemente la interfaz <code>SurfaceHolder.Callback</code>. Esto es as\u00ed porque los objetos de la clase <code>SurfaceHolder</code> se crean de manera as\u00edncrona, por lo que debemos a\u00f1adir un mecanismo que permita esperar a que dicho objeto haya sido creado antes de poder reproducir el v\u00eddeo.</p> <pre><code>public class MiActividad extends Activity implements SurfaceHolder.Callback\n{\nprivate MediaPlayer mediaPlayer;\n\n@Override\npublic void onCreate(Bundle savedInstanceState) {\nsuper.onCreate(savedInstanceState);\nsetContentView(R.layout.main);\nmediaPlayer = new MediaPlayer();\nSurfaceView superficie = (SurfaceView)findViewById(R.id.superficie);\n// Obteniendo el objeto SurfaceHolder a partir del SurfaceView\nSurfaceHolder holder = superficie.getHolder();\nholder.addCallback(this);\nholder.setType(SurfaceHolder.SURFACE_TYPE_PUSH_BUFFERS);\n}\n\n// Este manejador se invoca tras crearse la superficie, momento\n// en el que podremos trabajar con ella\npublic void surfaceCreated(SurfaceHolder holder) {\ntry {\nmediaPlayer.setDisplay(holder);\n} catch (IllegalArgumentException e) {\nLog.d(\"MEDIA_PLAYER\", e.getMessage());\n} catch (IllegalStateException e) {\nLog.d(\"MEDIA_PLAYER\", e.getMessage());\n}\n}\n\n// Y este manejador se invoca cuando se destruye la superficie,\n// momento que podemos aprovechar para liberar los recursos asociados\n// al objeto MediaPlayer\npublic void surfaceDestroyed(SurfaceHolder holder) {\nmediaPlayer.release();\n}\n\npublic void surfaceChanged(SurfaceHolder holder, int format, int width, int height) { }\n}\n}\n</code></pre> <p>Una vez que hemos asociado la superficie al objeto de la clase <code>MediaPlayer</code> debemos asignar a dicho objeto el clip de v\u00eddeo a reproducir. Ya que habremos creado el objeto <code>MediaPlayer</code> previamente, la \u00fanica posibilidad que tendremos ser\u00e1 utilizar el m\u00e9todo <code>setDataSource</code>, como se muestra en el siguiente ejemplo. Recuerda que cuando se utiliza dicho m\u00e9todo es necesario llamar tambi\u00e9n al m\u00e9todo <code>prepare</code>.</p> <pre><code>public void surfaceCreated(SurfaceHolder holder) {\ntry {\nmediaPlayer.setDisplay(holder);\nmediaPlayer.setDataSource(\"/mnt/sdcard/DCIM/video.mp4\");\nmediaPlayer.prepare();\nmediaPlayer.start();\n} catch (IllegalArgumentException e) {\nLog.d(\"MEDIA_PLAYER\", e.getMessage());\n} catch (IllegalStateException e) {\nLog.d(\"MEDIA_PLAYER\", e.getMessage());\n} catch (IOException e) {\nLog.d(\"MEDIA_PLAYER\", e.getMessage());\n}\n}\n</code></pre>"},{"location":"reproduccion-android.html#ejercicios","title":"Ejercicios","text":""},{"location":"reproduccion-android.html#reproducir-de-un-clip-de-audio","title":"Reproducir de un clip de audio","text":"<p>Se te proporciona en las plantillas de la sesi\u00f3n la aplicaci\u00f3n Audio. Dicha aplicaci\u00f3n contiene un clip de audio almacenado en los recursos, cuyo nombre es zelda_nes.mp3. La aplicaci\u00f3n contiene una \u00fanica actividad con una serie de botones, a los que a\u00f1adiremos funcionalidad para poder controlar la reproducci\u00f3n del clip de audio.</p> <p>Se te pide hacer lo siguiente:</p> <ul> <li> <p>A\u00f1ade el c\u00f3digo necesario en el constructor para crear una instancia de la clase <code>MediaPlayer</code> (donde el objeto <code>mp</code> habr\u00e1 sido declarado como un atributo de la clase): <pre><code>mp = MediaPlayer.create(this, R.raw.zelda_nes);\n</code></pre></p> </li> <li> <p>Modifica el manejador del bot\u00f3n Reproducir para que cada vez que se pulse \u00e9ste se deshabilite, se habiliten los botones Pausa y Detener, y empiece a reproducirse el clip de audio mediante la invocaci\u00f3n del m\u00e9todo <code>start</code> del objeto <code>MediaPlayer</code>.</p> <p>Para habilitar o deshabilitar botones usaremos el m\u00e9todo <code>setEnabled</code>, que recibe como par\u00e1metro un booleano.</p> </li> <li> <p>Modifica el manejador del bot\u00f3n Detener para que cada vez que se pulse se deshabilite dicho bot\u00f3n y el bot\u00f3n Pausa, se habilite el bot\u00f3n Reproducir, y se detenga la reproducci\u00f3n del audio mediante el m\u00e9todo <code>stop</code>. Invoca tambi\u00e9n el m\u00e9todo <code>prepare</code> del objeto <code>MediaPlayer</code> para dejarlo todo preparado por si se desea reproducir de nuevo el audio.</p> </li> <li> <p>Modifica el manejador del bot\u00f3n Pausa. Si el audio estaba en reproducci\u00f3n el texto del bot\u00f3n pasar\u00e1 a ser Reanudar y se pausar\u00e1 la reproducci\u00f3n por medio del m\u00e9todo pause. Si ya estaba en pausa el texto del bot\u00f3n volver\u00e1 a ser Pausa y se reanudar\u00e1 la reproducci\u00f3n del audio. No olvides cambiar la etiqueta del bot\u00f3n a Pausa si se pulsa el bot\u00f3n Detener.</p> </li> <li> <p>Observa que cuando detienes la reproducci\u00f3n con <code>stop</code> y la reanudas con <code>start</code> el archivo de audio contin\u00faa reproduci\u00e9ndose en el punto donde se detuvo. \u00bfQu\u00e9 tendr\u00edas que hacer para que al pulsar el bot\u00f3n <code>Reproducir</code> la reproducci\u00f3n comenzara siempre por el principio?</p> </li> <li> <p>Libera los recursos asociados al objeto <code>MediaPlayer</code> en el m\u00e9todo <code>onDestroy</code>. No olvides invocar al m\u00e9todo <code>onDestroy</code> de la superclase.</p> </li> </ul>"},{"location":"reproduccion-android.html#evento-de-finalizacion-de-la-reproduccion","title":"Evento de finalizaci\u00f3n de la reproducci\u00f3n","text":"<p>El objeto <code>MediaPlayer</code> no pasa autom\u00e1ticamente al estado de detenido una vez que se reproduce completamente el clip de audio, sino que es necesario detener la reproducci\u00f3n a mano. Por lo tanto, en este ejercicio vamos a a\u00f1adir un manejador para el evento que se dispara cuando el clip de audio finaliza. A\u00f1ade el siguiente c\u00f3digo en el m\u00e9todo <code>onCreate</code>, tras la inicializaci\u00f3n del objeto <code>MediaPlayer</code>:</p> <pre><code>mp.setOnCompletionListener(new OnCompletionListener() {\npublic void onCompletion(MediaPlayer mp) {\n// TODO Rellena con tu c\u00f3digo\n\n}\n});\n</code></pre> <p>Lo que tiene que hacer este manejador es exactamente lo mismo que se deber\u00eda hacer en el caso de haber pulsado el bot\u00f3n de <code>Detener</code>, es decir, habilitar el bot\u00f3n Reproducir, deshabilitar el resto, e invocar al m\u00e9todo <code>stop</code>.</p>"},{"location":"reproduccion-android.html#reproducir-un-clip-de-video-usando-videoview","title":"Reproducir un clip de v\u00eddeo usando VideoView","text":"<p>Para este ejercicio se te proporciona en las plantillas el proyecto Video, que contiene una \u00fanica actividad para controlar la reproducci\u00f3n de un clip de video, que se incluye en los recursos del proyecto (fichero <code>/res/raw/tetris.3gp</code>). En este caso tendremos un bot\u00f3n etiquetado como Reproducir, un bot\u00f3n etiquetado como Detener y una vista de tipo TextView que usaremos para mostrar la duraci\u00f3n del v\u00eddeo, el cual se mostrar\u00e1 en una vista de tipo <code>VideoView</code>.</p> <p>Los pasos que debes seguir son los siguientes:</p> <ul> <li>Modifica el manejador del bot\u00f3n Reproducir para que cuando \u00e9ste se pulse se deshabilite y se habilite el bot\u00f3n Detener.</li> <li>Modifica el manejador del bot\u00f3n Detener para que cuando \u00e9ste se pulse se deshabilite y se habilite el bot\u00f3n Reproducir.</li> <li> <p>Prepara el v\u00eddeo a reproducir al pulsar el bot\u00f3n Reproducir con la siguiente l\u00ednea de c\u00f3digo (pero no utilices el m\u00e9todo <code>start</code> para reproducirlo todav\u00eda): <pre><code>superficie.setVideoUri(Uri.parse(\"android.resource://es.ua.jtech.android.video/\" + R.raw.tetris));\n</code></pre></p> </li> <li> <p>Det\u00e9n la ejecuci\u00f3n del v\u00eddeo cuando se pulse el bot\u00f3n Detener por medio del m\u00e9todo <code>stopPlayback</code>.</p> </li> <li> <p>Para poder reproducir el v\u00eddeo y poder obtener su duraci\u00f3n y mosrtrarla en el <code>TextView</code>, hemos de esperar a que la superficie donde se va a reproducir est\u00e9 preparada. Para ello hemos de implementar el manejador para el evento <code>OnPreparedListener</code> de la vista <code>VideoView</code>. A\u00f1ade el manejador: <pre><code>superficie.setOnPreparedListener(new OnPreparedListener() {\npublic void onPrepared(MediaPlayer mp) {\n// Tu c\u00f3digo aqu\u00ed\n}\n});\n</code></pre></p> </li> <li> <p>A\u00f1ade c\u00f3digo en el manejador anterior para comenzar la reproducci\u00f3n por medio del m\u00e9todo <code>start</code>. Obt\u00e9n la duraci\u00f3n en minutos y segundos por medio del m\u00e9todo <code>getDuration</code> del <code>VideoView</code> y mu\u00e9strala en el <code>TextView</code> con el formato mm:ss.</p> <p>El m\u00e9todo <code>getDuration</code> devuelve la duraci\u00f3n del v\u00eddeo en milisegundos.</p> </li> </ul>"},{"location":"reproduccion-ios.html","title":"Reproducci\u00f3n de medios en iOS","text":"<p>En esta sesi\u00f3n veremos las diferentes APIs de las que disponemos en iOS para introducir y manipular contenidos multimedia en nuestras aplicaciones. En primer lugar repasaremos las APIs disponibles y sus principales caracter\u00edsticas. Tras esto, pasaremos a ver c\u00f3mo reproducir audio y video en las aplicaciones, y por \u00faltimo estudiaremos c\u00f3mo capturar audio, video y fotograf\u00edas, y c\u00f3mo procesar estos medios.</p>"},{"location":"reproduccion-ios.html#apis-multimedia-en-ios","title":"APIs multimedia en iOS","text":"<p>En el SDK de iOS encontramos un gran n\u00famero de frameworks que nos permiten reproducir y manipular contenido multimedia. Seg\u00fan las necesidades de nuestra aplicaci\u00f3n, deberemos seleccionar uno u otro. A continuaci\u00f3n mostramos los m\u00e1s destacados y sus caracter\u00edsticas:</p> <ul> <li>Media Player MP: Nos da acceso a la librer\u00eda multimedia del iPod. Con esta librer\u00eda podemos   reproducir medios de forma sencilla incrustando el reproductor de medios del dispositivo en nuestra   aplicaci\u00f3n, y personaliz\u00e1ndolo para que se adapte a nuestra interfaz y quede integrado correctamente.</li> <li>AV Foundation AVAV: Esta librer\u00eda nos permite controlar la reproducci\u00f3n y captura de audio y v\u00eddeo   a bajo nivel. Con ella por ejemplo podremos tener acceso a los fotogramas capturados por la c\u00e1mara en tiempo   real, permitiendo implementar aplicaciones basadas en visi\u00f3n artificial.</li> <li>Audio Toolbox AUAU: Se trata de una librer\u00eda de manipulaci\u00f3n de audio, que nos permite capturar,   reproducir, y convertir el formato del audio.</li> <li>OpenAL framework ALAL: Nos da un gran control sobre la reproducci\u00f3n de audio. Por ejemplo,   nos permite reproducir audio posicional, es decir, nos dar\u00e1 control   sobre la posici\u00f3n en la que se encuentra la fuente de audio, para que as\u00ed cada sonido se oiga con m\u00e1s fuerza   por el altavoz que corresponda. Esto es especialmente interesante para videojuegos, para implementar de esta forma   sonido en est\u00e9reo.</li> <li>Assets Library ALAL: Nos da acceso a nuestra librer\u00eda multimedia de fotos y v\u00eddeos.</li> <li>Core Image CICI: Es una API incorporada a partir de iOS 5. Permite procesar im\u00e1genes de forma   eficiente, aprovechando al m\u00e1ximo la arquitectura hardware del dispositivo, y evitando que tengamos   que ir a programar a bajo nivel para implementar estas funcionalidades de forma \u00f3ptima. Con estas funciones podremos   crear filtros para fotograf\u00eda, o implementar procedimientos de visi\u00f3n artificial como por ejemplo el reconocimiento   de caras.</li> </ul> <p>Vamos a centrarnos en esta sesi\u00f3n en el uso del reproductor de medios, para integrar v\u00eddeo en nuestras aplicaciones de forma personalizada, y en la API para procesamiento de im\u00e1genes.</p>"},{"location":"reproduccion-ios.html#reproduccion-de-audio","title":"Reproducci\u00f3n de audio","text":"<p>En primer lugar vamos a ver algunas formas de reproducir audio en dispositivos iOS. Debemos distinguir dos tipos de sonidos:</p> <ul> <li>Sonidos del sistema. Se reproducen mediante el servicio de sonidos del sistema _System Sound Services__System Sound Services_. Se debe utilizar \u00fanicamente para sonidos como clicks, alertas o notificaciones.</li> <li>Sonidos de la aplicaci\u00f3n. Se reproduce mediante APIs como <code>AVAudioPlayer</code> perteneciente a _AV Foundation_perteneciente a _AV Foundation_. Se utilizar\u00e1 para m\u00fasica de fondo o sonidos.</li> </ul>"},{"location":"reproduccion-ios.html#reproduccion-de-sonidos-del-sistema","title":"Reproducci\u00f3n de sonidos del sistema","text":"<p>El servicio de sonidos del sistema _System Sound Services__System Sound Services_ nos permite reproducir sonidos sencillos. Este servicio est\u00e1 destinado a utilizarse para sonidos de la interfaz, como por ejemplo la pulsaci\u00f3n de un bot\u00f3n o una alarma. Los sonidos que permite reproducir este servicio no pueden pasar de los 30 segundos de duraci\u00f3n, y el formato s\u00f3lo puede ser Linear PCM o IMA4, dentro de ficheros <code>.caf</code>, <code>.aif</code>, o <code>.wav</code>. Tambi\u00e9n nos permite activar la vibraci\u00f3n del dispositivo. No tenemos apenas ning\u00fan control sobre los sonidos reproducidos por este servicio, ni siquiera podemos alterar su volumen, sonar\u00e1n con el volumen que haya seleccionado el usuario en el dispositivo.</p> <p>Los sonidos del sistema se representan con el tipo <code>SystemSoundID</code>. Se trata de una API C, por lo que encontraremos una serie de funciones con las que crear y reproducir sonidos. Podemos crear un objeto de este tipo a partir de la URL del fichero de audio, mediante la funci\u00f3n <code>AudioServicesCreateSystemSoundID</code>.</p> <p>Swift <pre><code>var sonido = SystemSoundID()\nvar urlSonido = Bundle.main.url(forResource: \"alarma\", withExtension: \"caf\")\nAudioServicesCreateSystemSoundID((urlSonido as! CFURL), sonido)\n</code></pre> Objective-C <pre><code>SystemSoundID sonido;\nNSURL *urlSonido = [[NSBundle mainBundle] URLForResource:@\"alarma\"  withExtension:@\"caf\"];\nAudioServicesCreateSystemSoundID((CFURLRef)urlSonido, &amp;sonido);\n</code></pre></p> <p>En este caso la URL se debe indicar mediante el tipo <code>CFURLRef</code>. Este es un tipo de datos de Core Foundation. Se trata de una estructura de datos no un objeto Objective-Cno un objeto Objective-C, que est\u00e1 vinculada a la clase <code>NSURL</code>. Podemos encontrar diferentes tipos de Core Foundation con prefijo <code>CF</code>con prefijo &lt;code&gt;CF&lt;/code&gt; vinculados a objetos de Cocoa Touch. Estos objetos pueden convertirse directamente a su tipo Core Foundation correspondiente simplemente mediante un cast.</p> <p>Tras hacer esto, el sonido queda registrado como sonido del sistema y se le asigna un identificador, que podemos almacenar en una variable de tipo <code>SystemSoundID</code>.</p> <p>Podemos reproducir el sonido que hemos creado con la funci\u00f3n <code>AudioServicesPlaySystemSound</code>. Esto reproduce el sonido inmediatamente, sin ning\u00fan retardo, simplemente proporcionando el identificador del sonido a reproducir, ya que dicho sonido se encuentra cargado ya como sonido del sistema.</p> <pre><code>AudioServicesPlaySystemSound(sonido);\n</code></pre> <p>En caso de que queramos que junto a la reproducci\u00f3n del audio tambi\u00e9n se active la vibraci\u00f3n del dispositivo, llamaremos a la funci\u00f3n <code>AudioServicesPlayAlertSound</code>:</p> <pre><code>AudioServicesPlayAlertSound(sonido);\n</code></pre> <p>En este caso tambi\u00e9n debemos proporcionar el sonido a reproducir, pero adem\u00e1s de reproducirlo tambi\u00e9n se activar\u00e1 la vibraci\u00f3n. Si \u00fanicamente queremos activar la vibraci\u00f3n, entonces podemos proporcionar como par\u00e1metro la constante <code>kSystemSoundID_Vibrate</code>.</p>"},{"location":"reproduccion-ios.html#reproduccion-de-musica","title":"Reproducci\u00f3n de m\u00fasica","text":"<p>Si necesitamos que nuestra aplicaci\u00f3n reproduzca m\u00fasica de cualquier duraci\u00f3n, y no necesitamos tener un gran control sobre la forma en la que se reproduce el sonido por ejemplo posicionamiento _stereo_por ejemplo posicionamiento _stereo_, entonces podemos utilizar el reproductor de audio <code>AVAudioPlayer</code>. Con esto podremos reproducir ficheros de cualquier duraci\u00f3n, lo cual nos ser\u00e1 de utilidad para reproducir m\u00fasica de fondo en nuestra aplicaci\u00f3n. Soporta todos los formatos vistos anteriormente, y su uso resulta muy sencillo:</p> <p>Swift <pre><code> var error: Error? = nil\n var urlMusica = Bundle.main.url(forResource: \"musica\", withExtension: \"mp3\")\n let player = try! AVAudioPlayer(contentsOf: urlMusica!)\n player.prepareToPlay()\n player.play()\n</code></pre></p> <p>Objective-C <pre><code>NSError *error = nil;\nNSURL *urlMusica = [[NSBundle mainBundle] URLForResource:@\"musica\"\nwithExtension:@\"mp3\"];\n\nAVAudioPlayer *player = [[AVAudioPlayer alloc]\ninitWithContentsOfURL:urlMusica error:&amp;error];\n\n[player prepareToPlay];\n[player play];\n</code></pre></p> <p>Una desventaja de este reproductor es que la reproducci\u00f3n puede tardar en comenzar, ya que la inicializaci\u00f3n del buffer es una operaci\u00f3n lenta. Por ello tenemos el m\u00e9todo <code>prepareToPlay</code> que nos permite hacer que se inicialicen todos los recursos necesarios para que pueda comenzar la reproducci\u00f3n. Una vez hayamos hecho esto, al llamar a <code>play</code> la reproducci\u00f3n comenzar\u00e1 de forma instant\u00e1nea.</p> <p>Con esta API, en el reproductor objeto <code>AVAudioPlayer</code>objeto &lt;code&gt;AVAudioPlayer&lt;/code&gt; tenemos una serie de propiedades con las que podemos hacer que la m\u00fasica se reproduzca de forma c\u00edclica <code>numberOfLoops</code>&lt;code&gt;numberOfLoops&lt;/code&gt;, o controlar su volumen <code>volume</code>&lt;code&gt;volume&lt;/code&gt;. Tambi\u00e9n podemos definir un delegado sobre el reproductor <code>delegate</code>&lt;code&gt;delegate&lt;/code&gt; de tipo <code>AVAudioPlayerDelegate</code>, para as\u00ed poder controlar los eventos que ocurran en \u00e9l, como por ejemplo la finalizaci\u00f3n de la reproducci\u00f3n del audio. Podemos tambi\u00e9n saber en cualquier momento si se est\u00e1 reproduciendo audio actualmente <code>playing</code>&lt;code&gt;playing&lt;/code&gt;, y podemos pausar, reanudar, o deterner la reproducci\u00f3n con los m\u00e9todos <code>pause</code>, <code>play</code> y <code>stop</code>.</p> <p>Esta librer\u00eda es adecuada para reproductores multimedia, en los que simplemente nos interese reproducir m\u00fasica y poder controlar el estado de la reproducci\u00f3n. Si necesitamos tener un mayor control sobre el audio, como por ejemplo reproducir varios efectos de sonido simult\u00e1neamente, con distintos niveles de volumen y posicionados de diferente forma, deberemos utilizar una API como OpenAL. Esto ser\u00e1 especialmente adecuado para videojuegos, en los que necesitamos disponer de este control sobre el audio. Muchos motores para videojuegos incorporan librer\u00edas para gesti\u00f3n del audio basadas en OpenAL.</p> <p>Si queremos reproducir m\u00fasica de la librer\u00eda del iPod, podemos utilizar el objeto <code>MPMusicPlayerController</code>. La diferencia entre <code>AVAudioPlayer</code> y <code>MPMusicPlayerController</code> radica en que el primero se encarga de reproducir audio propio de nuestra aplicaci\u00f3n, mientras que el segundo se encarga de reproducir medios de la librer\u00eda del iPod, y nos permite hacerlo tanto dentro de nuestra aplicaci\u00f3n, como controlando el estado de reproducci\u00f3n de la aplicaci\u00f3n del iPod.</p>"},{"location":"reproduccion-ios.html#sesiones-de-audio","title":"Sesiones de audio","text":"<p>La sesi\u00f3n de audio nos permite especificar el uso que nuestra aplicaci\u00f3n quiere hacer del audio. Tendremos un objto <code>AVAudioSession</code> por cada aplicaci\u00f3n, que podr\u00e1 ser obtenido como un singleton:</p> <p>Swift <pre><code>var audioSession = AVAudioSession.sharedInstance()\n</code></pre></p> <p>Objective-C <pre><code>AVAudioSession *audioSession = [AVAudioSession sharedInstance];\n</code></pre></p> <p>En la sesi\u00f3n podemos controlar diferentes aspectos:</p> <ul> <li>\u00bfPuede mezclarse el sonido con otras aplicaciones?</li> <li>\u00bfEl sonido se interrumpe cuando se silencia el tel\u00e9fono o se bloquea la pantalla?</li> </ul> <p>Seg\u00fan el uso que vayamos a hacer del audio, podemos especificar diferentes categor\u00edas de sesiones, de las cuales destacamos las siguientes:</p> Categor\u00eda Mezcla Silencio <code>AVAudioSessionCategorySoloAmbient</code> categor\u00eda **por defecto**categor\u00eda **por defecto** No Si <code>AVAudioSessionCategoryAmbient</code> Si Si <code>AVAudioSessionCategoryPlayback</code> No No <p>Seg\u00fan la categor\u00eda que asignemos a la sesi\u00f3n el sonido se podr\u00e1 mezclar con otras aplicaciones o se silenciar\u00e1 cuando se bloquee la pantalla o se silencie el dispositivo.</p> <p>Podemos establecer y activar una determinada categor\u00eda de forma global para nuestra aplicaci\u00f3n con:</p> <p>Swift <pre><code>var audioSession = AVAudioSession.sharedInstance()\nvar error: Error? = nil\n\ndo {\n    try audioSession.setCategory(AVAudioSessionCategoryPlayback)\n    try audioSession.setActive(true)\n}\ncatch let error {\n}\n</code></pre></p> <p>Objective-C <pre><code>AVAudioSession *audioSession = [AVAudioSession sharedInstance];\n\nNSError *error = nil;\nBOOL success;\n\nsuccess = [audioSession setCategory:AVAudioSessionCategoryPlayback error:&amp;error];\nsuccess = [audioSession setActive:YES error:&amp;error];\n</code></pre></p> <p>Normalmente bastar\u00e1 con establecer la categor\u00eda una \u00fanica vez al iniciar la aplicaci\u00f3n.</p> <p>El singleton <code>AVAudioSession</code> tambi\u00e9n nos proporciona informaci\u00f3n sobre si otra aplicaci\u00f3n est\u00e1 reproduciendo audio actualmente mediante la propiedad <code>otherAudioPlaying</code>.</p> <p>A partir de iOS 8 se recomienda utilizar la propiedad <code>secondaryAudioShouldBeSilencedHint</code> que nos indica si otra actividad est\u00e1 reproduciendo audio no mezclable.</p> <p>Mediante la propiedad anterior podemos tomar la decisi\u00f3n de reproducir m\u00fasica de fondo s\u00f3lo en el caso de que otra aplicaci\u00f3n no lo est\u00e9 haciendo:</p> <p>Swift <pre><code>if self.audioSession.isOtherAudioPlaying {\n // El sonido de esta aplicaci\u00f3n se mezcla con el de fondo\n do {\n   try self.audioSession.setCategory(AVAudioSessionCategoryAmbient)\n }\n catch let error {\n }\n}else{\n do {\n   try self.audioSession.setCategory(AVAudioSessionCategorySoloAmbient)\n } catch let error {\n }\n self.reproduceMusica()\n}\n</code></pre></p> <p>Objective-C <pre><code>if ([self.audioSession isOtherAudioPlaying]) {\n// El sonido de esta aplicaci\u00f3n se mezcla con el de fondo\n[self.audioSession setCategory:AVAudioSessionCategoryAmbient error:&amp;error];\n} else {\n[self.audioSession setCategory:AVAudioSessionCategorySoloAmbient error:&amp;error];\n[self reproduceMusica];\n}\n</code></pre></p> <p>Algunos escenarios t\u00edpicos son los siguientes:</p> <ul> <li>Aplicaci\u00f3n donde el audio resulta imprescindible, como por ejemplo un reproductor multimedia. En este caso la categor\u00eda adecuada ser\u00e1 <code>AVAudioSessionCategoryPlayback</code>. No se mezclar\u00e1 con otras aplicaciones y podr\u00e1 seguir reproduci\u00e9ndose con la pantalla apagada.</li> <li>Aplicaci\u00f3n con audio secundario opcional. En algunas aplicaciones puede interesarnos dar la opci\u00f3n de que el usuario pueda ponerse su m\u00fasica preferida de fondo. Por ejemplo en un videojuego podemos tener una banda sonora de fondo y efectos de sonido. Si el usuario ya est\u00e1 reproduciendo m\u00fasica en el m\u00f3vil podemos tomar la decisi\u00f3n de mezclar dicha m\u00fasica con los efectos del juego, mientras que si no hay ninguna m\u00fasica reproduci\u00e9ndose se puede utilizar la banda sonora por defecto del juego. Podemos utilizar <code>AVAudioSessionCategorySoloAmbient</code> en caso de que no haya m\u00fasica reproduci\u00e9ndose lo comprobaremos con la propiedad <code>isOtherAudioPlaying</code>lo comprobaremos con la propiedad &lt;code&gt;isOtherAudioPlaying&lt;/code&gt;, o <code>AVAudioSessionCategoryAmbient</code> en caso contrario.</li> <li>Aplicaci\u00f3n en la que resulte adecuado combinar el audio. Aqu\u00ed podemos encontrar como ejemplo alguna aplicaci\u00f3n para crear m\u00fasica, en la que pueda ser interesante poner una m\u00fasica de fondo mientras tocamos un instrumento para acompa\u00f1arla. En este caso siempre utilizaremos <code>AVAudioSessionCategoryAmbient</code>.</li> </ul>"},{"location":"reproduccion-ios.html#reproduccion-de-audio-en-segundo-plano","title":"Reproducci\u00f3n de audio en segundo plano","text":"<p>Aunque en iOS la multitarea est\u00e1 restringida a un limitado conjunto de funcionalidades, una de las funcionalidades que se permiten ejecutar en segundo plano es la reproducci\u00f3n de m\u00fasica. Para poder reproducir m\u00fasica aunque tengamos silenciado el m\u00f3vil o la pantalla est\u00e9 bloqueada deberemos reproducirla dentro de una sesi\u00f3n con categor\u00eda <code>AVAudioSessionCategoryPlayback</code>.</p> <p>Adem\u00e1s de esto, para poder continuar la reproducci\u00f3n con la aplicaci\u00f3n cerrada deberemos declarar en <code>Info.plist</code> que uno de los modos el los que nuestra aplicaci\u00f3n puede funcionar en segundo plano _background__background_ es la reproducci\u00f3n de m\u00fasica, a\u00f1adiendo el item <code>audio</code> a la propiedad <code>UIBackgroundModes</code>.</p> <p></p> <p>Esto lo podemos hacer tambi\u00e9n desde la pesta\u00f1a Capabilities del nuestro target, activando los Background Modes, y dentro de estos el de Audio, AirPlay and Picture in Picture:</p> <p></p> <p>Esta reproducci\u00f3n en segundo plano tambi\u00e9n nos servir\u00e1 para emitir la reproducci\u00f3n v\u00eda AirPlay.</p>"},{"location":"reproduccion-ios.html#metadatos-del-audio","title":"Metadatos del audio","text":"<p>Podemos establecer metadatos del medio que actualmente se est\u00e9 reproduciendo con:</p> <p>Swift <pre><code>var info = [MPMediaItemPropertyTitle: \"By the Throat\",\n MPMediaItemPropertyArtist: \"CHVRCHES\",\n MPMediaItemPropertyAlbumTitle: \"The Bones of What You Believe\"]\nMPNowPlayingInfoCenter.default().nowPlayingInfo = info\n</code></pre></p> <p>Objective-C <pre><code>NSDictionary *info = @{\nMPMediaItemPropertyTitle : @\"By the Throat\",\nMPMediaItemPropertyArtist : @\"CHVRCHES\",\nMPMediaItemPropertyAlbumTitle : @\"The Bones of What You Believe\"\n};\n[[MPNowPlayingInfoCenter defaultCenter] setNowPlayingInfo: info];\n</code></pre></p> <p>Estos metadatos aparecer\u00e1n tambi\u00e9n en la pantalla de bloqueo del tel\u00e9fono mientras se reproduce el audio en segundo plano.</p>"},{"location":"reproduccion-ios.html#control-remoto","title":"Control remoto","text":"<p>Podemos permitir que la reproducci\u00f3n de nuestra aplicaci\u00f3n se controle de forma remota. Posibles controles remotos son:</p> <ul> <li>Controles del audio en la pantalla de bloqueo del tel\u00e9fono. Aparecer\u00e1n siempre que estemos reproduciendo audio en segundo plano y hayamos introducido metadatos con <code>MPNowPlayingInfoCenter</code>.</li> <li>Botones presentes en el cable de los auriculares. Nos permiten pausar\\/reanudar la reproducci\u00f3n, y cambiar de pista.</li> </ul> <p>Para utilizar estos controles remotos la aplicaci\u00f3n debe ser capaz de recibir eventos de control:</p> <p>Swift <pre><code>UIApplication.shared.beginReceivingRemoteControlEvents()\n</code></pre></p> <p>Objective-C <pre><code>[[UIApplication sharedApplication] beginReceivingRemoteControlEvents];\n</code></pre></p> <p>Estos eventos externos vienen de los auriculares, o de la pantalla de bloqueo, y permiten controlar la reproducci\u00f3n de audio de la aplicaci\u00f3n.</p> <p>Adem\u00e1s, para controlar estos eventos, el controlador que reproduzca el audio debe pasar a ser el first responder:</p> <p>Swift <pre><code>self.becomeFirstResponder()\n</code></pre> Objetive-C <pre><code>[self becomeFirstResponder];\n</code></pre></p> <p>Ser\u00e1 el first responder el que recibir\u00e1 el evento externo:</p> <p>Swift <pre><code> override func remoteControlReceived(with event: UIEvent?) {\n   if event?.type == .remoteControl {\n     if event?.subtype == .remoteControlTogglePlayPause {\n       if self.player.isPlaying() {\n         self.player.pause()\n       }\n       else {\n         self.player.play()\n       }\n     }\n     else if event?.subtype == .remoteControlPause {\n       self.player.pause()\n     }\n     else if event?.subtype == .remoteControlPlay {\n       self.player.play()\n     }\n   }\n }\n</code></pre></p> <p>Objective-C <pre><code>- (void)remoteControlReceivedWithEvent:(UIEvent *)event {\nif(event.type == UIEventTypeRemoteControl) {\nif(event.subtype == UIEventSubtypeRemoteControlTogglePlayPause) {\nif([self.player isPlaying]) {\n[self.player pause];\n} else{\n[self.player play];\n}\n} else if(event.subtype == UIEventSubtypeRemoteControlPause) {\n[self.player pause];\n} else if(event.subtype == UIEventSubtypeRemoteControlPlay) {\n[self.player play];\n}\n}\n}\n</code></pre></p>"},{"location":"reproduccion-ios.html#desconexion-de-los-auriculares","title":"Desconexi\u00f3n de los auriculares","text":"<p>Es recomendable que los reproductores multimedia respondan ante un cambio de dispositivo de reproducci\u00f3n auriculares, altavoces externosauriculares, altavoces externos. Por ejemplo, si desconectamos los auriculares durante la reproducci\u00f3n de m\u00fasica ser\u00eda recomendable pausarla autom\u00e1ticamente. En otras aplicaciones, como los videojuegos, esto no es importante.</p> <p>Podemos estar al tanto es estos cambios de hardware de reproducci\u00f3n _route change__route change_ mediante la notificaci\u00f3n <code>AVAudioSessionRouteChangeNotification</code>.</p> <p>Swift <pre><code>var nc = NotificationCenter.default\nnc.addObserver(self, selector: #selector(self.routeChanged), name: NSNotification.Name.AVAudioSessionRouteChange, object: nil)\n</code></pre> Objective-C <pre><code>NSNotificationCenter *nc = [NSNotificationCenter defaultCenter];\n[nc addObserver:self selector:@selector(routeChanged:) name:AVAudioSessionRouteChangeNotification object:nil];\n</code></pre></p> <p>Podemos responder a esta notificaci\u00f3n deteniendo la reproducci\u00f3n:</p> <p>Swift <pre><code>func routeChanged(_ sender: Any) {\n if self.player.isPlaying() {\n   self.player.pause()\n }\n}\n</code></pre> Objective-C <pre><code>- (void)routeChanged:(id)sender {\nif([self.player isPlaying]) {\n[self.player pause];\n}\n}\n</code></pre></p>"},{"location":"reproduccion-ios.html#reproduccion-de-video","title":"Reproducci\u00f3n de video","text":"<p>Vamos a ver ahora c\u00f3mo reproducir video en dispositivos iOS. Para reproducir video podemos utilizar una interfaz sencilla proporcionada por el framework Media Player, o bien reproducirlo a bajo nivel utilizando las clases <code>AVPlayer</code> y <code>AVPlayerLayer</code> del framework AV Foundation. Vamos a centrarnos en principio en la reproducci\u00f3n de video mediante la interfaz sencilla, y m\u00e1s adelante veremos c\u00f3mo realizar la captura mediante la API a bajo nivel.</p>"},{"location":"reproduccion-ios.html#reproductor-multimedia","title":"Reproductor multimedia","text":"<p>La reproducci\u00f3n de video puede realizarse de forma sencilla con la clase <code>MPMoviePlayerViewController</code>. Debemos inicializar el reproductor a partir de una URL <code>NSURL</code>&lt;code&gt;NSURL&lt;/code&gt;. Recordemos que la URL puede referenciar tanto un recurso local como remoto, por ejemplo podemos acceder a un video incluido entre los recursos de la aplicaci\u00f3n de la siguiente forma:</p> <p>Swift <pre><code>var movieUrl = Bundle.main.url(forResource: \"video\", withExtension: \"m4v\")\n</code></pre> Objective-C <pre><code>NSURL *movieUrl = [[NSBundle mainBundle] URLForResource:@\"video\" withExtension:@\"m4v\"];\n</code></pre></p> <p>Para reproducir el v\u00eddeo utilizando el reproductor nativo del dispositivo simplemente deberemos inicializar su controlador y mostrarlo de forma modal. Podemos fijarnos en que tenemos un m\u00e9todo espec\u00edfico para mostrar el controlador de reproducci\u00f3n de video de forma modal:</p> <p>Swift <pre><code> let controller = MPMoviePlayerViewController(contentURL: movieUrl)\n self.presentMoviePlayerViewControllerAnimated(controller)\n controller?.release()\n</code></pre></p> <p>Objective-C <pre><code>MPMoviePlayerViewController *controller =\n[[MPMoviePlayerViewController alloc] initWithContentURL:movieUrl];\n[self presentMoviePlayerViewControllerAnimated: controller];\n[controller release];\n</code></pre></p> <p>Con esto iniciaremos la reproducci\u00f3n de video en su propio controlador, que incorpora un bot\u00f3n para cerrarlo y controles de retroceso, avance y pausa. Cuando el v\u00eddeo finalice el controlador se cerrar\u00e1 autom\u00e1ticamente. Tambi\u00e9n podr\u00edamos cerrarlo desde el c\u00f3digo con <code>dismissMoviePlayerViewController</code>. Estos m\u00e9todos espec\u00edficos para mostrar y cerrar el controlador de reproducci\u00f3n se a\u00f1aden cuando importamos los ficheros de cabecera del framework Media Player, ya que se incorporan a <code>UIViewController</code> mediante categor\u00edas de dicha librer\u00eda.</p> <p></p> <p>Esta forma de reproducir v\u00eddeo es muy sencilla y puede ser suficiente para determinadas aplicaciones, pero en muchos casos necesitamos tener un mayor control sobre el reproductor. Vamos a ver a continuaci\u00f3n c\u00f3mo podemos ajustar la forma en la que se reproduce el v\u00eddeo.</p>"},{"location":"reproduccion-ios.html#personalizacion-del-reproductor","title":"Personalizaci\u00f3n del reproductor","text":"<p>Para poder tener control sobre el reproductor de v\u00eddeo, en lugar de utilizar simplemente <code>MPMoviePlayerViewController</code>, utilizaremos la clase <code>MPMoviePlayerController</code>. Debemos remarcar que esta clase ya no es un <code>UIViewController</code>, sino que simplemente es una clase que nos ayudar\u00e1 a controlar la reproducci\u00f3n del v\u00eddeo, pero deberemos utilizar nuestro propio controlador de la vista <code>UIViewController</code>&lt;code&gt;UIViewController&lt;/code&gt;.</p> <p>En primer lugar, creamos un objeto <code>MPMoviePlayerController</code> a partir del a URL con el v\u00eddeo a reproducir:</p> <p>Swift <pre><code>self.moviePlayer = MPMoviePlayerController(contentURL: movieUrl)\n</code></pre></p> <p>Objective-C <pre><code>self.moviePlayer = [[MPMoviePlayerController alloc] initWithContentURL:movieUrl];\n</code></pre></p> <p></p> <p>Ahora deberemos mostrar el controlador en alg\u00fan sitio. Para ello deberemos a\u00f1adir la vista de reproducci\u00f3n de video propiedad <code>view</code> del controlador de v\u00eddeo del controlador de v\u00eddeopropiedad &lt;code&gt;view&lt;/code&gt; del controlador de v\u00eddeo del controlador de v\u00eddeo a la jerarqu\u00eda de vistas en pantalla. Tambi\u00e9n deberemos darle un tama\u00f1o a dicha vista. Por ejemplo, si queremos que ocupe todo el espacio de nuestra vista actual podemos utilizar como tama\u00f1o de la vista de v\u00eddeo el mismo tama\u00f1o propiedad <code>bounds</code>propiedad &lt;code&gt;bounds&lt;/code&gt; de la vista actual, y a\u00f1adir el v\u00eddeo como subvista suya:</p> <p>Swift <pre><code>self.moviePlayer.view.frame = self.view.bounds\nself.view.addSubview(self.moviePlayer.view)\n</code></pre></p> <p>Objective-C <pre><code>self.moviePlayer.view.frame = self.view.bounds;\n[self.view addSubview: self.moviePlayer.view];\n</code></pre></p> <p>Si queremos que la vista del reproductor de v\u00eddeo cambie de tama\u00f1o al cambiar la orientaci\u00f3n de la pantalla, deberemos hacer que esta vista se redimensione de forma flexible en ancho y alto:</p> <p>Swift <pre><code> self.moviePlayer.view.autoresizingMask = [.flexibleHeight, .flexibleWidth]\n</code></pre> Objective-C <pre><code> self.moviePlayer.view.autoresizingMask =\nUIViewAutoresizingFlexibleHeight | UIViewAutoresizingFlexibleWidth;\n</code></pre></p> <p></p> <p>Por \u00faltimo, comenzamos la reproducci\u00f3n del v\u00eddeo con <code>play</code>:</p> <p>Swift <pre><code>self.moviePlayer.play()\n</code></pre></p> <p>Objective-C <pre><code>[self.moviePlayer play];\n</code></pre></p> <p>Con esto tendremos el reproductor de v\u00eddeo ocupando el espacio de nuestra vista y comenzar\u00e1 la reproducci\u00f3n. Lo que ocurre es que cuando el v\u00eddeo finalice, el reproductor seguir\u00e1 estando en pantalla. Es posible que nos interese que desaparezca autom\u00e1ticamente cuando finalice la reproducci\u00f3n. Para hacer esto deberemos utilizar el sistema de notificaciones de Cocoa. Concretamente, para este caso necesitaremos la notificaci\u00f3n <code>MPMoviePlayerPlaybackDidFinishNotification</code>, aunque en la documentaci\u00f3n de la clase <code>MPMoviePlayerController</code> podemos encontrar la lista de todos los eventos del reproductor que podemos tratar mediante notificaciones. En nuestro caso vamos a ver c\u00f3mo programar la notificaci\u00f3n para ser avisados de la finalizaci\u00f3n del v\u00eddeo:</p> <p>Swift <pre><code>NotificationCenter.default.addObserver(self, selector: #selector(self.videoPlaybackDidFinish), name: MPMoviePlayerPlaybackDidFinishNotification, object: self.moviePlayer)\n</code></pre> Objective-C <pre><code>[[NSNotificationCenter defaultCenter] addObserver: self\nselector: @selector(videoPlaybackDidFinish:)\nname: MPMoviePlayerPlaybackDidFinishNotification\nobject: self.moviePlayer];\n</code></pre></p> <p>En este caso, cuando recibamos la notificaci\u00f3n se avisar\u00e1 al m\u00e9todo que hayamos especificado. Por ejemplo, si queremos que el reproductor desaparezca de pantalla, podemos hacer que en este m\u00e9todo se elimine como subvista, se nos retire como observadores de la notificaci\u00f3n, y se libere de memoria el reproductor:</p> <p>Swift <pre><code>func videoPlaybackDidFinish(_ notification: Notification)\n  self.moviePlayer.view.removeFromSuperview()\n  NotificationCenter.default.removeObserver(self, name:   MPMoviePlayerPlaybackDidFinishNotification, object: self.moviePlayer)\n  self.moviePlayer = nil\n}\n</code></pre></p> <p>Objective-C <pre><code>-(void) videoPlaybackDidFinish: (NSNotification*) notification {\n[self.moviePlayer.view removeFromSuperview];\n\n[[NSNotificationCenter defaultCenter] removeObserver: self\nname: MPMoviePlayerPlaybackDidFinishNotification\nobject: self.moviePlayer];\n\nself.moviePlayer = nil;\n}\n</code></pre></p> <p>El reproductor mostrado anteriormente muestra sobre el v\u00eddeo una serie de controles predefinidos para retroceder, avanzar, pausar, o pasar a pantalla completa, lo cual muestra el v\u00eddeo en la pantalla predefinida del sistema que hemos visto en el punto anterior. Vamos a ver ahora c\u00f3mo personalizar este aspecto. Para cambiar los controles mostrados sobre el v\u00eddeo podemos utilizar la propiedad <code>controlStyle</code> del controlador de v\u00eddeo, y establecer cualquier de los tipos definidos en la enumeraci\u00f3n <code>MPMovieControlStyle</code>. Si queremos que el reproductor de v\u00eddeo quede totalmente integrado en nuestra aplicaci\u00f3n, podemos especificar que no se muestre ning\u00fan control del sistema:</p> <p>Swift <pre><code>self.moviePlayer.controlStyle = .none\n</code></pre></p> <p>Objective-C <pre><code>self.moviePlayer.controlStyle = MPMovieControlStyleNone;\n</code></pre></p> <p>Cuando el tama\u00f1o del v\u00eddeo reproducido no se ajuste totalmente a la relaci\u00f3n de aspecto de la pantalla, veremos que algunas zonas quedan en negro. Podemos observar esto por ejemplo en la imagen en la que reproducimos v\u00eddeo desde la orientaci\u00f3n vertical del dispositivo. Para evitar que la pantalla quede vac\u00eda podemos incluir una imagen de fondo, que se ver\u00e1 en todas aquellas zonas que no abarque el v\u00eddeo. Para ello podemos utilizar la vista de fondo del video <code>backgroundView</code>&lt;code&gt;backgroundView&lt;/code&gt;. Cualquier subvista que a\u00f1adamos a dicha vista, se mostrar\u00e1 como fondo del v\u00eddeo. Por ejemplo podemos motrar una imagen con:</p> <p>Swift <pre><code>self.moviePlayer.backgroundView.addSubview( UIImageView(image: UIImage(named: \"fondo.png\")!))\n</code></pre> Objective-C <pre><code>[self.moviePlayer.backgroundView addSubview: [[[UIImageView alloc]\ninitWithImage:[UIImage imageNamed:@\"fondo.png\"]];\n</code></pre></p> <p></p>"},{"location":"reproduccion-ios.html#reproduccion-con-avplayer","title":"Reproducci\u00f3n con <code>AVPlayer</code>","text":"<p>A partir de iOS 4.0 tenemos la posibilidad de reproducir video con <code>AVPlayer</code> y <code>AVPlayerLayer</code>. El primer objeto es el reproductor de v\u00eddeo, mientras que el segundo es una capa es un subtipo de <code>CALayer</code>es un subtipo de &lt;code&gt;CALayer&lt;/code&gt; que podemos utilizar para mostrar la reproducci\u00f3n. Este tipo de reproductor nos da un mayor control sobre la forma en la que se muestra el v\u00eddeo, desacoplando la gesti\u00f3n del origen del v\u00eddeo y la visualizaci\u00f3n de dicho v\u00eddeo en pantalla.</p> <p>Swift <pre><code>let videoURL = NSURL(string: \"https://clips.vorwaerts-gmbh.de/big_buck_bunny.mp4\")\nlet player = AVPlayer(url: videoURL! as URL)\nlet playerLayer = AVPlayerLayer(player: player)\nself.view.layer.addSublayer(playerLayer)\n</code></pre> Objective-C <pre><code>AVPlayer *player = [AVPlayer playerWithUrl: videoUrl];\n\nAVPlayerLayer *playerLayer = [AVPlayerLayer playerLayerWithPlayer:player];\n[self.view.layer addSublayer:playerLayer];\n</code></pre></p> <p>A partir de iOS 8 aparece adem\u00e1s la clase <code>AVPlayerViewController</code>, que nos proporciona un controllador que permite reproducir v\u00eddeo de forma sencilla mediante un objeto <code>AVPlayer</code> a pantalla completa o en PiP _Picture in Picture__Picture in Picture_:</p> <p>Swift <pre><code>var controller = AVPlayerViewController()\nlet player = AVPlayer(url: videoURL! as URL)\ncontroller.player = player\nself.present(controller, animated: true, completion: { _ in })\n</code></pre></p> <p>Objective-C <pre><code>AVPlayerViewController *controller = [[AVPlayerViewController alloc] init];\nAVPlayer *player = [AVPlayer playerWithURL:videoUrl];\ncontroller.player = player;\n\n[self presentViewController:controller animated:YES completion:nil];\n</code></pre></p> <p>A partir de iOS 9 esta es la forma recomendada de reproducir v\u00eddeo, quedando desaprobado el uso de <code>MPMoviePlayerViewController</code>. Utilizaremos \u00e9ste \u00faltimo \u00fanicamente si necesitamos tener compatibilidad con versiones de iOS previas a la 8.</p> <p>A diferencia de las clases anteriores <code>AVPlayer</code>, <code>AVPlayerLayer</code> y <code>AVAudioPlayer</code>, todas ellas pertenecientes a AVFoundation, la nueva clase <code>AVPlayerViewController</code> pertenece al _framework <code>AVKit</code>, por lo que deberemos incluir dicho framework en nuestras aplicaciones para poder utilizar dicho controlador.</p> <p>La nueva clase <code>AVPlayerViewController</code> nos permitir\u00e1 tambi\u00e9n poner una vista superpuesta sobre el v\u00eddeo, o controlar la zona de la pantalla en la que se mostrar\u00e1 el v\u00eddeo, al igual que en el caso del componente ahora desaprobado.</p> <p>Para tener control sobre el estado del v\u00eddeo en reproducci\u00f3n podemos utilizar KVO sobre propiedades del objeto <code>AVPlayer</code>, o podemos suscribirlos a la notificaci\u00f3n <code>AVPlayerItemDidPlayToEndTimeNotification</code> del item que se est\u00e9 reproduciendo actualmente para estar al tanto de su finalizaci\u00f3n:</p> <p>Swift <pre><code> NotificationCenter.default.addObserver(self, selector: #selector(self.reproduccionFinalizada), name: NSNotification.Name.AVPlayerItemDidPlayToEndTime, object: player.currentItem)\n</code></pre></p> <p>Objective-C <pre><code>[[NSNotificationCenter defaultCenter]\naddObserver:self\nselector:@selector(reproduccionFinalizada:)\nname:AVPlayerItemDidPlayToEndTimeNotification\nobject:[player currentItem]];\n</code></pre></p> <p>Para utilizar PiP disponible \u00fanicamente en iPad a partir de iOS 9disponible \u00fanicamente en iPad a partir de iOS 9 deberemos activar el modo Audio, AirPlay and Picture in Picture como Background Modes, en la pesta\u00f1a Capabilities de nuestro target, y adem\u00e1s configurar la sesi\u00f3n de audio de forma apropiada, como hemos visto anteriormente.</p>"},{"location":"reproduccion-ios.html#ejercicios","title":"Ejercicios","text":""},{"location":"reproduccion-ios.html#reproduccion-de-video_1","title":"Reproducci\u00f3n de v\u00eddeo","text":"<p>Vamos en primer lugar a ver c\u00f3mo crear un reproductor de v\u00eddeo en iOS. Tenemos una plantilla en el proyecto <code>VideoPlayer</code>. Vemos que tiene un bot\u00f3n Reproducir v\u00eddeo, y un fichero <code>video.m4v</code>. La reproducci\u00f3n de v\u00eddeo deber\u00e1 comenzar en el m\u00e9todo <code>playVideo:</code>, que es el que se ejecuta al pulsar el bot\u00f3n anterior. Se pide:</p> <p>a) Reproducir el v\u00eddeo con la pantalla de reproducci\u00f3n predefinida <code>MPMoviePlayerViewController</code>&lt;code&gt;MPMoviePlayerViewController&lt;/code&gt;.</p> <p>b) Comentar el c\u00f3digo implementado en el punto anterior. Ahora vamos a crear un reproductor propio mediante <code>MPMoviePlayerController</code>. Crearemos un reproductor de este tipo, haremos que su tama\u00f1o sea el mismo tama\u00f1o de la vista principal, a\u00f1adiremos el reproductor a dicha vista como subvista, y comenzaremos la reproducci\u00f3n. c) Con el reproductor anterior tenemos el problema de que al finalizar el v\u00eddeo el reproductor se queda en pantalla y no hay forma de salir de \u00e9l. Vamos a escuchar la notificaci\u00f3n de reproducci\u00f3n finalizada para que cuando esto ocurra el reproductor sea eliminado de pantalla. Cuando recibamos esta notificaci\u00f3n llamaremos al m\u00e9todo <code>videoPlaybackDidFinish:</code>, que ya se encuentra implementado.</p> <p>d) Si giramos el dispositivo veremos que el v\u00eddeo no se adapta de forma correcta a la pantalla. Ajustar su propiedad <code>autoresizingMask</code> para que sea flexible tanto de ancho como de alto. Comprobar ahora que al girar la pantalla el v\u00eddeo se adapta correctamente.</p> <p>e) Al reproducir el v\u00eddeo en vertical gran parte de la pantalla queda en negro. Vamos a decorar el fondo para darle un mejor aspecto. Crearemos una vista que muestre la imagen <code>fondo.png</code>, y la mostraremos como subvista de la vista de fondo del v\u00eddeo.</p> <p>f) Por \u00faltimo, para que el reproductor quede totalmente integrado en nuestra aplicaci\u00f3n, eliminaremos los controles de reproducci\u00f3n que incorpora por defecto. De esta forma el usuario no podr\u00e1 saltar el v\u00eddeo, ni volver atr\u00e1s en \u00e9l.</p> <p>g) Modifica el c\u00f3digo anterior para utilizar <code>AVPlayerViewController</code> y <code>AVPlayer</code>. Ten en cuenta que deber\u00e1s incluir los frameworks <code>AVFoundation</code> y <code>AVKit</code>, y sus imports correspondientes.</p>"},{"location":"seccion.html","title":"Seccion","text":""},{"location":"streaming.html","title":"Sistemas de difusi\u00f3n de medios","text":"<p>En los dispositivos m\u00f3viles podemos reproducir tanto medios almacenados localmente en el dispositivos, como medios a los que accedemos de forma remota. Para tener v\u00eddeo local podemos copiarlo directamente al espacio de almacenamiento del dispositivo o bien distribuirlo junto con nuestras aplicaciones. Esto puede ser conveniente para v\u00eddeos de escaso tama\u00f1o que no van a cambiar a lo largo del tiempo, pero en el resto de casos ser\u00e1 m\u00e1s apropiado contar con sistemas que suministren de forma remota estos contenidos multimedia a nuestros dispositivos. Vamos a estudiar los siguientes mecanismos:</p> <ul> <li>Podcasts</li> <li>Descarga progresiva</li> <li>Streaming</li> </ul>"},{"location":"streaming.html#podcast","title":"Podcast","text":"<p>Los podcasts surgieron como el mecanismo de difusi\u00f3n de medios para el iPod. De hecho, su nombre viene de iPod + broadcast.</p> <p>Un podcast se define como un canal que contiene contenido epis\u00f3dico multimedia. Estos episodios pueden ser audios, v\u00eddeos, o incluso libros. El creados del podcast podr\u00e1 a\u00f1adir nuevos episodios al canal en cualquier momento.</p> <p>Como usuarios podemos suscribirnos a un podcast en nuestros dispositivos. Esto har\u00e1 que cuando se publique un nuevo episodio \u00e9ste se descargue de forma autom\u00e1tica en nuestro dispositivo en el momento que el que contemos con una conexi\u00f3n Wi-Fi o por cable. De esta forma podremos reproducirlo offline posteriormente, sin necesidad de consumir datos m\u00f3viles.</p>"},{"location":"streaming.html#formatos-de-un-podcast","title":"Formatos de un podcast","text":"<p>En un podcast podemos publicar medios en diferentes formatos. En el caso del audio los formatos soportados son:</p> <ul> <li><code>.m4a</code>  (audio/x-m4a)</li> <li><code>.mp3</code>  (audio/mpeg)</li> </ul> <p>Para el v\u00eddeo tenemos:</p> <ul> <li><code>.mov</code>  (video/quicktime)</li> <li><code>.mp4</code>  (video/mp4)</li> <li><code>.m4v</code>  (video/x-m4v)</li> </ul> <p>Y por \u00faltimo, en el caso de libros y documentos encontramos:</p> <ul> <li><code>.pdf</code>  (application/pdf)</li> <li><code>.epub</code> (document/x-epub)</li> </ul>"},{"location":"streaming.html#creacion-de-un-podcast","title":"Creaci\u00f3n de un podcast","text":"<p>Un podcast se define como un fichero XML con formato de feed RSS. En este fichero la etiqueta principal es <code>&lt;channel&gt;</code>, que define el propio canal del podcast. Dentro de esta etiqueta tendremos informaci\u00f3n general sobre el canal, y contendr\u00e1 un conjunto de episodios, cada uno de ellos definido mediante la etiqueta <code>&lt;item&gt;</code>:</p> <pre><code>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n&lt;rss xmlns:itunes=\"http://www.itunes.com/dtds/podcast-1.0.dtd\" version=\"2.0\"&gt;\n&lt;channel&gt;\n&lt;!-- Informacion del canal --&gt;\n&lt;item&gt;\n&lt;!-- Informacion del item --&gt;\n&lt;/item&gt;\n&lt;item&gt;\n&lt;!-- Informacion del item --&gt;\n&lt;/item&gt;\n...\n&lt;/channel&gt;\n&lt;/rss&gt;\n</code></pre> <p>La informaci\u00f3n general que debemos proporcional del canal es la siguiente:</p> <pre><code>    &lt;title&gt;Servicios Multimedia para Dispositivos M\u00f3viles&lt;/title&gt;\n&lt;link&gt;http://jtech.ua.es/podcast/index.html&lt;/link&gt;\n&lt;language&gt;en-us&lt;/language&gt;\n&lt;copyright&gt;&amp;#x2117; &amp;amp; &amp;#xA9; 2012 Miguel \u00c1ngel Lozano&lt;/copyright&gt;\n&lt;itunes:subtitle&gt;Podcast sobre dispositivos m\u00f3viles&lt;/itunes:subtitle&gt;\n&lt;itunes:author&gt;Miguel \u00c1ngel Lozano&lt;/itunes:author&gt;\n&lt;itunes:summary&gt;Resumen&lt;/itunes:summary&gt;\n&lt;description&gt;Descripci\u00f3n&lt;/description&gt;\n&lt;itunes:owner&gt;\n&lt;itunes:name&gt;Miguel \u00c1ngel Lozano&lt;/itunes:name&gt;\n&lt;itunes:email&gt;malozano@ua.es&lt;/itunes:email&gt;\n&lt;/itunes:owner&gt;\n&lt;itunes:image href=\"http://jtech.ua.es/podcast/ios.jpg\" /&gt;\n&lt;itunes:category text=\"Technology\"&gt;\n&lt;itunes:category text=\"Gadgets\"/&gt;\n&lt;/itunes:category&gt;\n&lt;itunes:category text=\"TV &amp;amp; Film\"/&gt;\n</code></pre> <p>De cada item deberemos proporcionar la siguiente informaci\u00f3n:</p> <pre><code>&lt;item&gt;\n\n&lt;title&gt;Episodio 1&lt;/title&gt;\n\n&lt;itunes:author&gt;Miguel \u00c1ngel Lozano&lt;/itunes:author&gt;\n&lt;itunes:subtitle&gt;Caracter\u00edsticas de los dispositivos&lt;/itunes:subtitle&gt;\n&lt;itunes:summary&gt;\nRedes de telefon\u00eda m\u00f3vil. Tipos de dispositivos. Plataformas de desarrollo\n  &lt;/itunes:summary&gt;\n&lt;itunes:image href=\"http://www.jtech.ua.es/images/logo-especialista-moviles_mini.png\" /&gt;\n\n&lt;enclosure url=\"http://localhost/~malozano/episodio1.mp3\" length=\"169608456\" type=\"audio/mpeg\" /&gt;\n&lt;guid&gt;http://localhost/~malozano/episodio1.mp3&lt;/guid&gt;\n\n&lt;pubDate&gt;Mon, 5 Mar 2012 19:00:00 GMT&lt;/pubDate&gt;\n&lt;itunes:duration&gt;1:28:20&lt;/itunes:duration&gt;\n&lt;itunes:keywords&gt;dispositivos, moviles, redes, desarrollo&lt;/itunes:keywords&gt;\n\n&lt;/item&gt;\n</code></pre> <p>Debemos prestar especial atenci\u00f3n a <code>&lt;enclosure&gt;</code> y <code>&lt;guid&gt;</code>. En el primero especificaremos la URL donde est\u00e1 publicado el contenido multimedia y sus datos (tipo y longitud del contenido). En el segundo deberemos indicar un identificador \u00fanico para cada episodio. Si introducimos dos episodios con el mismo guid las aplicaciones pensar\u00e1n que se trata del mismo episodio, y no lo mostrar\u00e1n correctamente. Una buena pr\u00e1ctica es utilizar la URL del medio como guid.</p>"},{"location":"streaming.html#subscripcion-a-un-podcast","title":"Subscripci\u00f3n a un podcast","text":"<p>Existen diferentes aplicaciones en las que podemos suscribirnos a podcasts, aunque la principal de ellas es iTunes. Con la opci\u00f3n Archivo &gt; Suscribirse a un podcast ... podremos introducir la URL del feed del podcast al que nos queremos suscribir.</p> <p>Encontramos otras aplicaciones como iVoox en las que tambi\u00e9n podemos suscribirnos a podcasts desde cualquier dispositivo.</p> <p>Una vez hemos creado un podcast podemos enviar la URL del feed a Apple para solicitar que lo incluya en el cat\u00e1logo de iTunes Store.</p>"},{"location":"streaming.html#descarga-progresiva","title":"Descarga progresiva","text":"<p>Mediante podcasts podemos suministrar v\u00eddeo a dispositivos m\u00f3viles para ser reproducido offline en cualquier momento. Sin embargo muchas veces necesitaremos reproducir de forma online v\u00eddeos que no hayan sido descargados previamente. Una forma sencilla de hacer esto es simplemente publicar el v\u00eddeo en un servidor web y descargarlo mediante HTTP.</p> <p>Al ser los v\u00eddeos ficheros de gran tama\u00f1o, la descarga puede durar un tiempo considerable. Esta espera para empezar a reproducir el v\u00eddeo puede resultar algo prohibitivo para el uso de nuestra aplicaci\u00f3n. Para evitar este problema lo que podemos hacer es reproducir el v\u00eddeo durante la descarga. Esto es lo que se conoce como descarga progresiva o pseudostreaming.</p> <p>Para utilizar descarga progresiva normalmente basta con publicar los v\u00eddeos en un servidor web. Sin embargo, para garantizar m\u00e1xima compatibilidad deberemos tener en cuenta algunos factores sobre la estructura del fichero.</p>"},{"location":"streaming.html#atomos-del-fichero-de-video","title":"\u00c1tomos del fichero de v\u00eddeo","text":"<p>Para poder reproducir el v\u00eddeo de forma progresiva el reproductor debe tener la informaci\u00f3n sobre el v\u00eddeo (\u00edndice del fichero) antes de empezar a recibir el contenido.</p> <p>Los formatos de fichero <code>mov</code> y <code>mp4</code> constan de una serie de partes o \u00e1tomos:</p> <ul> <li><code>ftyp</code>: Formato de la pel\u00edcula</li> <li><code>moov</code>: Contiene metadatos</li> <li><code>wide</code>: Permite ampliar <code>moov</code></li> <li><code>mdat</code>: Contiene datos de la pel\u00edcula</li> </ul> <p>Es importante que el bloque de metadatos <code>moov</code> se obtanga antes de empezar a leer el bloque de datos <code>mdat</code> para as\u00ed poder reproducir el v\u00eddeo durante la descarga, pero no siempre el \u00e1tomo <code>moov</code> est\u00e1 antes que <code>mdat</code> en el fichero. Muchas aplicaciones al guardar este formato esperan al final para escribir el \u00edndice del fichero, cuando ya est\u00e1 todo el contenido almacenado, y por lo tanto dejan <code>moov</code> al final.</p> <p>Para la mayor\u00eda de reproductores actuales tener el bloque <code>moov</code> al final del fichero no supone un problema, ya que son capaces de solicitar al servidor que les devuelva en primer lugar los \u00faltimos bloques del fichero, para as\u00ed poder reproducir el v\u00eddeo en descarga progresiva.</p> <p>Sin embargo, para algunos reproductores no es posible hacer esto. Para que la descarga progresiva funcione en estos casos deberemos mover el bloque <code>moov</code> al principio del fichero. Esto es lo que se conoce como Faststart.</p> <p>Si no realizamos la reordenaci\u00f3n, en estos reproductores (normalmente de versiones antiguas de Android) nos aparecer\u00e1 un mensaje como el siguiente:</p> <p></p> <p>Al no poder reproducir el v\u00eddeo durante la descarga nos pregunta si queremos descargarlo para reproducirlo en local.</p>"},{"location":"streaming.html#faststart","title":"Faststart","text":"<p>Faststart consiste en mover el \u00e1tomo <code>moov</code> al comienzo de un fichero de v\u00eddeo para poderlo reproducir en descarga progresiva en cualquier navegador.</p> <p></p> <p>Es posible aplicar faststart sin tener que recodificar el v\u00eddeo, simplemente tenemos que reordenar los datos. Podemos aplicarlo con <code>ffmpeg</code>:</p> <pre><code>ffmpeg -movflags faststart\n</code></pre> <p>Tambi\u00e9n existen herramientas como QT Index Swapper (http://renaun.com/blog/code/qtindexswapper/) que nos permiten realizar este cambio.</p>"},{"location":"streaming.html#desventajas-de-la-descarga-progresiva","title":"Desventajas de la descarga progresiva","text":"<p>Con la descarga progresiva podemos reproducir un v\u00eddeo conforme se descarga del servidor, sin tener que esperar a que finalice la descarga, y pudiendo utilizar cualquier servidor web para su publicaci\u00f3n.</p> <p>Sin embargo, mientras estemos viendo el v\u00eddeo el fichero se descargar\u00e1 normalmente a una velocidad mayor a la velocidad de reproducci\u00f3n. Esto puede suponer que viendo unos pocos segundos de v\u00eddeo hayamos descargado una gran cantidad de datos. Esto puede ser un problema importante cuando accedamos a v\u00eddeo desde la red de datos m\u00f3viles, que tiene un l\u00edmite de descarga acotado. Por lo tanto, para v\u00eddeos de gran tama\u00f1o deberemos evitar este pr\u00e1ctica.</p> <p>Por ejemplo, aquellas aplicaciones iOS que accedan mediante descarga progresiva a v\u00eddeos de m\u00e1s de 10 minutos o m\u00e1s de 5Mb a trav\u00e9s de la red de datos m\u00f3viles ser\u00e1n rechazadas por Apple y no podr\u00e1n ser publicadas en la App Store. En estos casos deberemos acceder a los v\u00eddeos mediante streaming</p>"},{"location":"streaming.html#streaming","title":"Streaming","text":"<p>Los sistemas de streaming consisten en enviar al cliente peque\u00f1os fragmentos de v\u00eddeo conforme estos se necesiten para la reproducci\u00f3n. En el cliente se recibir\u00e1n y se reproducir\u00e1n estos fragmentos pero, al contrario que en el caso de la descarga progresiva, el v\u00eddeo completo no se descargar\u00e1 en el dispositivo. Los fragmentos se ir\u00e1n desechando conforme no se necesiten. De este modo el dispositivo descargar\u00e1 s\u00f3lo los datos de v\u00eddeo que se est\u00e9n reproduciendo, y no el fichero entero.</p> <p>Deberemos utilizar un sistema de streaming en los siguientes casos:</p> <ul> <li>Cuando tengamos V\u00eddeo Bajo Demanda (VOD) con ficheros de gran tama\u00f1o. Servirlos mediante streaming har\u00e1 que s\u00f3lo se descargue lo que se vaya a reproducir. Si no se reproduce el v\u00eddeo entero, no tendr\u00e1 que bajar todo el fichero.</li> <li>Cuando tengamos emisiones de v\u00eddeo en directo. En este caso es imposible enviar el v\u00eddeo mediante un fichero, ya que el fichero se est\u00e1 grabando en este momento. Lo \u00fanico que se puede enviar son los fragmentos del v\u00eddeo conforme estos son capturados.</li> </ul>"},{"location":"streaming.html#protocolos-de-streaming","title":"Protocolos de streaming","text":"<p>Existen diferentes protocolos para ofrecer v\u00eddeo mediante streaming:</p> <ul> <li>RTSP (Real Time Streaming Protocol): Funciona como un reproductor de v\u00eddeo, con comandos para reproducir, pausar y parar. En lugar de tener comandos como <code>GET</code> o <code>POST</code> de HTTP, tenemos otros comandos como <code>PLAY</code>, <code>PAUSE</code> o `STOP``. Fue desarrollado por RealNetworks.</li> <li>RTMP: Protocolo propietario desarrollado por Macromedia/Adobe, e implementado en Flash Media Server. La especificaci\u00f3n hizo p\u00fablica en 2009, permitiendo as\u00ed integrarlo en otras redes de distribuci\u00f3n de contenidos (CDNs). Este protocolo fue dise\u00f1ado para emitir v\u00eddeo Flash, y se utiliza por ejemplo en aplicaciones como YouTube. Encapsula v\u00eddeo FLV y audio MP3 o AAC.</li> <li>HLS (HTTP Live Streaming): Procolo desarrollado por Apple. Es el sistema que Apple recomienda y el que debemos utilizar en las aplicaciones iOS. Se basa en particionar los v\u00eddeos en una serie de peque\u00f1os fragmentos, y publicarlos en un servidor web HTTP convecional.</li> </ul> <p>La mayor diferencia entre los protocolos anteriores es que tanto RTSP como RTMP requieren servidores especiales, sin embargo, HLS puede funcionar sobre cualquier servidor web al funcionar sobre HTTP. Esto tiene tambi\u00e9n la ventaja de que HLS tendr\u00e1 una mayor compatibilidad con los distintos nodos de la red (gateways, routers, etc).</p> <p>RTMP tiene la ventaja de que soporta multicast (un mismo stream sirve para varios clientes), cosa que HTTP lo permite. Adem\u00e1s, RTMP es m\u00e1s granular. Env\u00eda peque\u00f1os paquetes, y permite reanudar el v\u00eddeo en el lugar que se qued\u00f3 con mayor precisi\u00f3n. Para utilizar RTMP ser\u00e1 recomendable utilizar las tecnlog\u00edas Adobe Air.</p> <p>A continuaci\u00f3n mostramos la compatibilidad de cada protocolo con las diferentes plataformas m\u00f3viles:</p> <ul> <li>HLS: iOS y Android 3.0 o superior.</li> <li>RTSP: Android.</li> <li>RTMP: Adobe Air (multiplataforma)</li> </ul>"},{"location":"streaming.html#http-live-streaming","title":"HTTP Live Streaming","text":"<p>HTTP Live Streaming es un protocolo de streaming desarrollado por Apple que se basa en segmentar el v\u00eddeo en peque\u00f1os fragmentos, y publicarlos en un servidor web ordinario para su descarga mediante HTTP.</p> <p>Podr\u00e1 utilizarse tanto para VOD como para emisiones en directo. Simplemente necesitaremos una aplicaci\u00f3n que codifique y segmente el v\u00eddeo. En caso del VOD, podremos segmentar el v\u00eddeo offline y copiar los fragmentos al servidor web. El caso de tener v\u00eddeo en directo, deberemos codificar, generar segmentos, y publicarlos en el servidor conforme se realiza la captura del v\u00eddeo.</p> <p></p> <p>Apple nos proporciona las herramientas <code>mediafilesegmenter</code> y <code>mediastreamsegmenter</code> que nos permiten segmentar VOD y v\u00eddeo en directo respectivamente. Tambi\u00e9n podremos hacerlo mediante aplicaciones como VLC o <code>ffmpeg</code>.</p>"},{"location":"streaming.html#formato-de-video-en-hls","title":"Formato de v\u00eddeo en HLS","text":"<p>En HLS el v\u00eddeo se particiona en una serie de segmentos con las siguientes caracter\u00edsticas:</p> <ul> <li>Formato de fichero <code>.ts</code></li> <li>V\u00eddeo H.264</li> <li>Audio AAC</li> </ul> <p>Todos estos segmentos se integran mediante un \u00edndice (playlist) con formato <code>M3U8</code>. Esta playlist tiene la siguiente estructura:</p> <pre><code>#EXTM3U\n#EXT-X-TARGETDURATION:10\n#EXT-X-MEDIA-SEQUENCE:1\n#EXTINF:10,\nhttp://media.example.com/segment0.ts\n#EXTINF:10,\nhttp://media.example.com/segment1.ts\n#EXTINF:10,\nhttp://media.example.com/segment2.ts\n#EXT-X-ENDLIST\n</code></pre> <p>En este caso tenemos un v\u00eddeo con 3 segmentos, cada uno de ellos de 10 segundos de duraci\u00f3n. Para publicar el v\u00eddeo, tendremos que publicar en el servidor web los segmentos y la playlist:</p> <ul> <li>segment0.ts</li> <li>segment1.ts</li> <li>segment2.ts</li> <li>video.m3u8</li> </ul> <p>El v\u00eddeo completo tendr\u00e1 por lo tanto una duraci\u00f3n se 30 segundos. Para reproducir el v\u00eddeo deberemos especificar la direcci\u00f3n donde est\u00e1 el playlist. Por ejemplo, si tenemos la playlist publicada en <code>http://media.example.com/video.m3u8</code>, bastar\u00e1 con introducir esta URL en Safari para reproducirlo.</p> <p>En caso de tener VOD bastar\u00e1 con segmentar el v\u00eddeo con alguna herramienta y publicar los ficheros generados en un servidor web. Para emisiones en directo deberemos tener ejecut\u00e1ndose la herramienta de segmentaci\u00f3n, que ir\u00e1 generando segmentos del v\u00eddeo capturado y los ir\u00e1 publicando en el servidor, modificando la playlist en cada momento.</p>"},{"location":"streaming.html#generacion-de-hls-con-ffmpeg","title":"Generaci\u00f3n de HLS con <code>ffmpeg</code>","text":"<p>Podemos utilizar la herramienta <code>ffmpeg</code> para generar v\u00eddeo HLS. A continuaci\u00f3n mostramos ejemplos para generar v\u00eddeo con diferentes perfiles de calidad.</p> <p>Perfil bajo:</p> <pre><code>ffmpeg -i entrada.mp4 -c:a aac -ac 1 -b:a 32k -ar 22050\n-c:v h264 -profile:v baseline -level 3.0 -b:v 200K -r 12 -g 36\n-f hls -hls_time 10 -hls_list_size 999 -s 320x180 low.m3u8\n</code></pre> <p>Perfil medio:</p> <pre><code>ffmpeg -i entrada.mp4 -c:a aac -ac 2 -b:a 64k -ar 44100\n-c:v h264 -profile:v baseline -level 3.1 -b:v 600K -r 24 -g 72\n-f hls -hls_time 10 -hls_list_size 999 -s 640x360 medium.m3u8\n</code></pre> <p>Perfil alto:</p> <pre><code>ffmpeg -i entrada.mp4 -c:a aac -ac 2 -b:a 96k -ar 44100\n-c:v h264 -profile:v main -level 3.1 -b:v 1500K -r 24 -g 72\n-f hls -hls_time 10 -hls_list_size 999 -s 1280x720 high.m3u8\n</code></pre>"},{"location":"streaming.html#bitrate-adaptativo-abr","title":"Bitrate adaptativo (ABR)","text":"<p>En el caso anterior hemos creado diferentes versiones del v\u00eddeo HLS dirigidas a distintos anchos de banda. Este protocolo nos permite integrar todas estas versiones en una \u00fanica playlist, para que as\u00ed el sistema seleccione la m\u00e1s adecuada en funci\u00f3n del ancho de banda disponible. De esta forma, si empezamos con un perfil de alta calidad y a mitad se produce una ca\u00edda del ancho de banda en la conexi\u00f3n, autom\u00e1ticamente saltar\u00e1 a una versi\u00f3n con una calidad adecuada al nuevo ancho de banda disponible.</p> <p>Tendremos lo que se conoce como una variant playlist. En ella indicamos las distintas playlists disponibles para las diferentes versiones, y el ancho de banda m\u00ednimo necesario para cada una de ellas:</p> <pre><code>#EXTM3U\n\n#EXT-X-STREAM-INF:PROGRAM-ID=1,BANDWIDTH=200000\nlow.m3u8\n\n#EXT-X-STREAM-INF:PROGRAM-ID=1,BANDWIDTH=600000\nmedium.m3u8\n\n#EXT-X-STREAM-INF:PROGRAM-ID=1,BANDWIDTH=1500000\nhigh.m3u8\n</code></pre> <p>Podemos generar con <code>ffmpeg</code> directamente una variant playlist a partir de un v\u00eddeo de la siguiente forma:</p> <pre><code>VIDSOURCE=\"video.mp4\"\nRESOLUTION=\"854x480\"\nBITRATE1=\"800000\"\nBITRATE2=\"600000\"\nBITRATE3=\"400000\"\n\nAUDIO_OPTS=\"-c:a libfaac -b:a 160000 -ac 2\"\nVIDEO_OPTS1=\"-s $RESOLUTION -c:v libx264 -b:v $BITRATE1 -vprofile baseline -preset medium -x264opts level=41\"\nVIDEO_OPTS2=\"-s $RESOLUTION -c:v libx264 -b:v $BITRATE2 -vprofile baseline -preset medium -x264opts level=41\"\nVIDEO_OPTS3=\"-s $RESOLUTION -c:v libx264 -b:v $BITRATE3 -vprofile baseline -preset medium -x264opts level=41\"\nOUTPUT_HLS=\"-hls_time 3 -hls_list_size 10 -hls_wrap 30 -start_number 1\"\n\nffmpeg -i \"$VIDSOURCE\" -y -threads 4 \\\n$AUDIO_OPTS $VIDEO_OPTS1 $OUTPUT_HLS alto.m3u8 \\\n$AUDIO_OPTS $VIDEO_OPTS2 $OUTPUT_HLS medio.m3u8 \\\n$AUDIO_OPTS $VIDEO_OPTS3 $OUTPUT_HLS bajo.m3u8\n</code></pre>"},{"location":"streaming.html#servidores-de-streaming","title":"Servidores de streaming","text":"<p>Existen diferentes servidores y servicios en la nube que nos permiten emitir v\u00eddeo bajo demanda y en directo en diferentes formatos, dando soporte a la pr\u00e1ctica totalidad de dispositivos actuales.</p> <p>Encontramos por ejemplo servicios en la nube como Zencoder (http://zencoder.com) y VideoCloud (http://videocloud.brightcove.com/).</p> <p>Tambi\u00e9n encontramos servidores que podemos instalar en nuestro propio equipo como Wowza Media Server (http://wowza.com/). Vamos a centrarnos en estudiar el uso de este servidor.</p>"},{"location":"streaming.html#configuracion-de-wowza-media-server","title":"Configuraci\u00f3n de Wowza Media Server","text":"<p>Para poder utilizar Wowza Media Server deberemos seguir los siguientes pasos:</p> <ul> <li> <p>El primer lugar debemos configurar un usuario y password para acceder al servidor. Para ello editamos el fichero <code>/Library/WowzaStreamingEngine/conf/admin.password</code> y a\u00f1adimos la siguiente linea: <pre><code>admin mastermoviles\n</code></pre> Con esto habremos creado un usuario <code>admin</code> con password <code>mastermoviles</code>.</p> </li> <li> <p>Iniciamos el servidor, entrando en Aplicaciones &gt; Wowza Streaming Engine y seleccionando Start Standalone Mode</p> </li> </ul> <p></p> <ul> <li>Podremos acceder al servidor desde un navegador utilizando la direcci\u00f3n <code>http://localhost:8088</code>. La primera vez que accedamos veremos la siguiente pantalla de bienvenida:</li> </ul> <p></p> <ul> <li>Pulsamos en Next, y nos mostrar\u00e1 el workflow del servidor:</li> </ul> <p></p> <ul> <li>Pulsamos de nuevo en Next, y nos pedir\u00e1 que introduzcamos los datos del usuario administrador del servidor. Introduciremos los datos que configuramos en el primer paso:</li> </ul> <p></p> <ul> <li>A continuaci\u00f3n nos pedir\u00e1 que demos de alta un usuario con perfil de publisher. Este usuario ser\u00e1 necesario para poder emitir video en directo a trav\u00e9s del servidor:</li> </ul> <p></p> <ul> <li>Una vez creado ese usuario, veremos la pantalla principal de Wowza. Esto es lo que veremos las pr\u00f3ximas veces que arranquemos Wowza, al estar ya configurado:</li> </ul> <p></p>"},{"location":"streaming.html#publicacion-de-contenido-vod","title":"Publicaci\u00f3n de contenido VOD","text":"<p>Una vez configurado el servidor podremos a\u00f1adir tanto emisiones en directo como contenido bajo demanda (VOD). Vamos a centrarnos en primer lugar en la forma de a\u00f1adir contenido VOD.</p> <p>Para a\u00f1adir este tipo de contenido simplemente tenemos que copiar los videos que queramos publicar a la carpeta de contenido (Content) del servidor.</p> <ul> <li>Tenemos un enlace a esta carpeta desde el directorio principal de la aplicaci\u00f3n:</li> </ul> <p></p> <ul> <li>Entramos en la carpeta <code>Content</code>, y copiaremos en ella todos los v\u00eddeos que queramos publicar:</li> </ul> <p></p> <ul> <li>Una vez hecho esto, podremos acceder al contenido desde la web de Wowza. Para ello desde la p\u00e1gina principal del servidor pulsaremos sobre Applications &gt; vod:</li> </ul> <p></p> <ul> <li>Con esto entraremos en la aplicaci\u00f3n de VOD. En esta aplicaci\u00f3n veremos un bot\u00f3n con el texto Test Players que nos permitir\u00e1 acceder a una serie de reproductores de prueba para los v\u00eddeos publicados en la aplicaci\u00f3n de VOD. Pulsando sobre este bot\u00f3n accederemos a ellos, y por ejemplo entraremos en el reproductor Apple HLS. Aqu\u00ed introducir el nombre del fichero que queramos reproducir y verlo en el reproductor de prueba:</li> </ul> <p></p> <ul> <li>En la pesta\u00f1a Mobile el servidor nos mostrar\u00e1 los enlaces mediante los cuales podr\u00edamos reproducir los v\u00eddeos v\u00eda streaming en dispositivos m\u00f3viles iOS y Android, mediante HLS y RTSP respectivamente:</li> </ul> <p></p> <p>Si abrimos estos enlaces desde el navegador de dispositivos que est\u00e9n en una red desde la que tengan visibilidad de la IP del servidor podremos ver los v\u00eddeos publicados.</p>"},{"location":"streaming.html#acceso-a-wowza-mediante-ssl","title":"Acceso a Wowza mediante SSL","text":"<p>A partir de iOS 9 s\u00f3lo se permite establecer conexiones mediante SSL (<code>https</code>) por motivos de seguridad. Aunque podemos a\u00f1adir excepciones en nuestra aplicaciones para poder seguir accediendo mediante <code>http</code>, no es recomendable hacerlo. Vamos a ver a continuaci\u00f3n la forma de configurar Wowza para poder acceder a los contenidos multimedia mediante SSL tal como se recomienda a partir de iOS 9.</p> <p>Para poder configurar <code>https</code> en Wowza deberemos crearnos un certificado cuyo nombre (<code>CN</code>) coincida con el dominio a proteger. Por ejemplo, si nuestro contenido va a estar en <code>eps.ua.es</code>, deberemos crear un certificado con <code>CN=eps.ua.es</code>. Esto nos serviar\u00e1 para aseguramos de que no se est\u00e1 suplantando al servidor. Adem\u00e1s, nuestro certificado deber\u00e1 estar firmado por una autoridad de certificaci\u00f3n (CA), como por ejemplo Verisign o Thawte.  Como alternativa, veremos tambi\u00e9n la posibilidad de convertirnos nosotros mismos en \"autoridad de certificaci\u00f3n\", aunque en ese caso deberemos proporcionar al usuario nuestro certificado ra\u00edz para que lo instale en su dispositivo como certificado de confianza.</p> <p>Una vez creado el certificado, se incluir\u00e1 en Wowza y se habilitar\u00e1 un puerto de acceso mediante SSL utilizando dicho certificado. </p> <p>Vamos a ver a continuaci\u00f3n c\u00f3mo realizar esta configuraci\u00f3n paso a paso:</p>"},{"location":"streaming.html#creacion-del-certificado","title":"Creaci\u00f3n del certificado","text":"<p>En primer lugar debemos crear el certificado para nuestro sitio web. Imaginemos que queremos proteger el sitio <code>mastermoviles.eps.ua.es</code>, donde tendremos instalado un servidor Wowza, y acceder a \u00e9l mediante SSL garantizando que no ha sido suplantado por otro. Crearemos un par de claves p\u00fablica-privada para Wowza de la siguiente forma:</p> <pre><code>keytool -genkey -keysize 2048 -alias wowza -keyalg RSA -keystore mastermoviles.eps.ua.es.jks\n</code></pre> <p>En el asistente para la creaci\u00f3n de las claves, cuando nos pregunte por el Common Name (CN) es importante indicar <code>mastermoviles.eps.ua.es</code>, ya que debe coincidir con el dominio del sitio web a proteger. </p> <p>Al crear el almac\u00e9n de claves y el alias <code>wowza</code>, que ser\u00e1 la clave que utilice Wowza para establecer el canal SSL, deberemos especificar el mismo password para el almac\u00e9n y para el alias.</p> <p>Una vez hecho esto, crearemos una solicitud de certificado (CSR) para nuestra clave:</p> <pre><code>keytool -certreq -file mastermoviles.eps.ua.es.csr -alias wowza -keyalg RSA -keystore mastermoviles.eps.ua.es.jks\n</code></pre> <p>Esta solicitud podr\u00e1 ser enviada a una autoridad de certificaci\u00f3n para que nos proporcionen un certificado firmado por ellos correspondiente a nuestro par de claves. Como alternativa, vamos a ver c\u00f3mo convertirnos en \"autoridad de certificaci\u00f3n\" creando un certificado ra\u00edz autofirmado.</p>"},{"location":"streaming.html#autofirmar-nuestro-certificado","title":"Autofirmar nuestro certificado","text":"<p>Si optamos por solicitar nuestro certificado firmado a una autoridad de certificaci\u00f3n (CA) existente no ser\u00e1 necesario realizar este paso. </p> <p>En caso de trabajar en un \u00e1mbito de pruebas, o en un proyecto de \u00e1mbito cerrado, puede ser conveniente firmar nosotros mismos nuestro certificado, en lugar de solicitarlo a una CA, aunque en este caso a quienes conecten a nuestro sitio web les aparecer\u00e1 como \"No seguro\", a no ser que se instalen y conf\u00eden en nuestro certificado ra\u00edz autofirmado.</p> <p>Para crear este certificado autofirmado en primer lugar creamos el par de claves:</p> <pre><code>openssl genrsa -out eps.ua.es.key 2048\n</code></pre> <p>Tras esto, generamos el certificado autofirmado:</p> <pre><code>openssl req -x509 -sha256 -new -key eps.ua.es.key -out eps.ua.es.cer -days 730 -subj /CN=\u201ceps.ua.es\u201d\n</code></pre> <p>Este certificado deber\u00e1 difundirse entre todos los que vayan a utilizar nuestra aplicaci\u00f3n, para que lo instalen en sus dispositivos como certificado ra\u00edz de confianza. De no ser as\u00ed, los certificados que firmemos con \u00e9l tampoco ser\u00e1n validos.</p> <p>Una vez tenemos el certificado ra\u00edz autofirmado, podemos firmar con \u00e9l el certificado para nuestro sitio web:</p> <pre><code>openssl x509 -req -in mastermoviles.eps.ua.es.csr -out mastermoviles.eps.ua.es.cer -CAkey eps.ua.es.key -CA eps.ua.es.cer -days 365 -CAcreateserial -CAserial serial\n</code></pre> <p>Con esto obtenemos un certificado firmado por nosotros mismos. Si hubi\u00e9semos recurrido a una CA, nos habr\u00eda proporcionado tambi\u00e9n este mismo fichero <code>cer</code>, pero en ese caso firmado por un certificado ra\u00edz en el que ya conf\u00edan los diferentes dispositivos. </p>"},{"location":"streaming.html#incluir-el-certificado-firmado-en-el-almacen-de-claves","title":"Incluir el certificado firmado en el almacen de claves","text":"<p>Una vez tenemos nuestro certificado firmado por una CA (o por un certificado ra\u00edz nuestro autofirmado), deberemos importar el certificado de la CA y nuestro certificado firmado por ella en nuestro almac\u00e9n de claves. </p> <p>En caso de haber creado un certificado ra\u00edz autofirmado, lo importamos en nuestro almac\u00e9n de claves. En caso de haber obtenido el certificado de una CA, en su lugar importaremos el certificado puente de dicha CA:</p> <pre><code>keytool -import -alias root -trustcacerts -file eps.ua.es.cer -keystore mastermoviles.eps.ua.es.jks\n</code></pre> <p>Adem\u00e1s tambi\u00e9n debemos importar en el almac\u00e9n el certificado propio de nuestro sitio web:</p> <pre><code>keytool -import -alias wowza -trustcacerts -file mastermoviles.eps.ua.es.cer -keystore mastermoviles.eps.ua.es.jks\n</code></pre> <p>Con esto ya tendremos listo el almacen de claves para ser utilizado en Wowza. </p>"},{"location":"streaming.html#configuracion-del-puerto-seguro-en-wowza","title":"Configuraci\u00f3n del puerto seguro en Wowza","text":"<p>Una vez contamos con nuestro almacen de claves configurado para nuestro sitio web, podemos configurar un puerto seguro en Wowza para acceder a trav\u00e9s de \u00e9l al contenido mediante SSL. Para ello, en la consola de Wowza entramos en la pesta\u00f1a Server y dentro de ella en Virtual Host Setup. Veremos la lista de puertos (Host Ports) activos actualmente. Si pulsamos el bot\u00f3n Edit podremos editar este lista o a\u00f1adir nuevos puertos. </p> <p>Para configurar un nuevo puerto que utilice SSL pulsamos sobre Add Host Port... e introducimos la siguiente informaci\u00f3n:</p> <ul> <li>Name: Indicamos un nombre para identificar el puerto en el entorno, por ejemplo \"Default SSL Streaming.</li> <li>Type: Como tipo indicamos que ser\u00e1 un puerto para la aplicaci\u00f3n de Streaming.</li> <li>IP Adresses: Indicamos que se puede acceder desde cualquier IP, con \"*\".</li> <li>Port(s): El puerto al que conectaremos mediante SSL. Si el puerto no seguro por defecto es 1935, podr\u00edamos utilizar para SSL por ejemplo el puerto 1936.</li> <li>Enable SSL/StreamLock: Ser\u00e1 importante marcar esta casilla para que este puerto utilice conexi\u00f3n segura con SSL.</li> <li>Keystore Path: Indicamos aqu\u00ed la ruta del almac\u00e9n de claves que hemos creado anteriormente. Dado que lo hemos copiado en el directorio de configuraci\u00f3n de Wowza, podemos indicar una ruta como la siguiente: <pre><code>${com.wowza.wms.context.VHostConfigHome}/conf/mastermoviles.eps.ua.es.jks\n</code></pre></li> <li>Keystore Password: Indicamos aqu\u00ed el password que hemos utilizado en nuestro almac\u00e9n de claves (debe ser el mismo password para el almac\u00e9n y para el alias <code>wowza</code> que almacena la clave a utilizar por este servidor.</li> </ul> <p> </p> <p>Una vez introducidos los datos pulsamos Add y veremos el nuevo puerto en la lista:</p> <p> </p> <p>Ya podemos guardar pulsando en Save y reiniciar el servidor (nos aparecer\u00e1 un bot\u00f3n Restart Now invit\u00e1ndonos a hacerlo para que se apliquen los cambios). </p> <p>Tras esto ya podremos acceder a los contenidos desde un navegador o desde nuestras aplicaciones.</p>"},{"location":"streaming.html#acceder-a-aplicaciones-con-certificado-autofirmado","title":"Acceder a aplicaciones con certificado autofirmado","text":"<p>Si nuestro certificado no est\u00e1 firmado por una CA v\u00e1lida, sino que est\u00e1 autofirmado por nosotros, los navegadores y aplicaciones en principio no lo considerar\u00e1n de confianza y por lo tanto no podremos acceder a los contenidos hasta que no conf\u00ede en \u00e9l.</p> <p>En caso de que intentemos acceder a uno de los v\u00eddeos por SSL en Wowza, el navegador nos indicar\u00e1 lo siguiente:</p> <p> </p> <p>Aqu\u00ed es muy importante no pulsar sobre Continuar, sino pulsar sobre Detalles para que nos abra la siguiente vista:</p> <p> </p> <p>Aqu\u00ed vemos los detalles del certificado y podemos indicarle que conf\u00ede en \u00e9l. Una vez sea de confianza ya tendremos acceso a los contenidos. </p> <p>En caso de querer que nuestras aplicaciones puedan acceder, tendremos que instalar el certificado ra\u00edz en el dispositivo como certificado de confianza. Para ello enviaremos el certificado creado (fichero <code>.cer</code>) al dispositivo, por ejemplo como adjunto en un email. Abriremos el fichero y veremos lo siguiente:</p> <p> </p> <p>Aqu\u00ed vemos que el certificado aparece como no v\u00e1lido, indicando que est\u00e1 Sin verificar. Si pulsamos sobre Instalar podremos hacer que pase a ser de confianza, tal como nos indica en la siguiente pantalla:</p> <p> </p> <p>Volvemos a pulsar sobre Instalar para confirmar la instalaci\u00f3n y que lo guarde como certificado de confianza. A continuaci\u00f3n veremos que el certificado ya est\u00e1 Verificado:</p> <p> </p> <p>Una vez instalado el certificado ra\u00edz, si abrimos el certificado de nuestro sitio web veremos que ya nos aparece directamente como Verificado, ya que est\u00e1 firmado por un certificado ra\u00edz que actualmente es de confianza:</p> <p> </p> <p>Con esto ya tendremos el dispositivo configurado para que pueda acceder a los v\u00eddeos en nuestro dominio mediante protocolo seguro. Con este sistema hay que remarcar que deberemos distribuir el certificado ra\u00edz entre todos aquellos que vayan a utilizar la aplicaci\u00f3n, para que as\u00ed lo puedan instalar como certificado de confianza. Para no tener que hacer esto deber\u00edamos enviar nuestra aplicaci\u00f3n para que la firme una CA v\u00e1lida, cuyo certificado ra\u00edz ya venga como certificado de confianza en el dispositivo.</p>"},{"location":"streaming.html#ejercicios","title":"Ejercicios","text":""},{"location":"streaming.html#creacion-e-un-podcast","title":"Creaci\u00f3n e un podcast","text":"<p>Para publicar el podcast puedes utilizar el servidor web Apache incluido en MacOS. Para arrancar el servidor introduciremos el siguiente comando en un Terminal:</p> <pre><code>sudo -s launchctl load -w /System/Library/LaunchDaemons/org.apache.httpd.plist\n</code></pre> <p>Podemos comprobar que el servidor est\u00e1 en marcha accediendo en un navegador a la direcci\u00f3n http://localhost. Veremos el texto It works!.</p> <p>Los contenidos deber\u00e1n publicarse en el directorio <code>/Library/WebServer/Documents</code>. Podemos abrirlo desde Finder con Ir &gt; Ir a la carpeta \u2026.</p> <p>Una vez hayamos terminado de trabajar con el servidor podemos pararlo con:</p> <pre><code>sudo -s launchctl unload -w /System/Library/LaunchDaemons/org.apache.httpd.plist\n</code></pre> <p>Para la realizaci\u00f3n puedes partir de la plantilla que encontrar\u00e1s en la documentaci\u00f3n oficial:   http://www.apple.com/es/itunes/podcasts/specs.html</p> <p>Se pide:</p> <ul> <li> <p>Crea un podcast con s\u00f3lo el primer episodio que encontrar\u00e1s en los recursos de la sesi\u00f3n (o cualquier otro fichero). Pon tu nombre como autor/a, y modifica el t\u00edtulo del podcast y del episodio. Publica el podcast en Jetty y accede a \u00e9l con iTunes.</p> </li> <li> <p>A\u00f1ade al podcast dos episodios m\u00e1s (puedes utilizar los ficheros proporcionados o recursos propios). Comprueba ahora c\u00f3mo se actualiza la informaci\u00f3n del podcast en iTunes.</p> </li> </ul>"},{"location":"streaming.html#prueba-de-faststart","title":"Prueba de faststart","text":"<p>Vamos a probar el efecto de utilizar o no faststart en un v\u00eddeo MP4. Se pide:</p> <ul> <li> <p>Descarga de los recursos de la sesi\u00f3n el fichero Video sin faststart y publ\u00edcalo en el servidor web que utilizaste en el ejercicio anterior.</p> </li> <li> <p>Abre el fichero con un editor de texto o hexadecimal. \u00bfD\u00f3nde est\u00e1 el bloque <code>moov</code>? Si cuentas con un dispositivo m\u00f3vil Android o emulador con una versi\u00f3n previa a la 2.2, accede a la URL del v\u00eddeo desde su navegador. \u00bfQu\u00e9 ocurre?</p> </li> <li> <p>Modificar el v\u00eddeo con <code>ffmpeg</code> para a\u00f1adir faststart. <pre><code>ffmpeg -i video.mp4 -c:a copy -c:v copy -movflags faststart video_fs.mp4\n</code></pre></p> </li> <li> <p>Abre el fichero con un editor y busca el bloque <code>moov</code> de nuevo. \u00bfD\u00f3nde est\u00e1 ahora? Publica el v\u00eddeo modificado y vuelve a probarlo desde el navegador del m\u00f3vil. \u00bfQu\u00e9 ocurre ahora?</p> </li> </ul>"},{"location":"unity.html","title":"El motor Unity","text":"<p>Unity es un motor gen\u00e9rico para la creaci\u00f3n de videojuegos 2D y 3D enfocado hacia el desarrollo casual. La curva de aprendizaje del motor es bastante suave, especialmente si lo comparamos con motores m\u00e1s complejos como Unreal Engine 4, y nos permitir\u00e1 realizar un desarrollo r\u00e1pido de videojuegos. Esta caracter\u00edstica hace este motor muy apropiado tambi\u00e9n para crear r\u00e1pidamente prototipos de nuestros juegos.</p> <p>A partir de la versi\u00f3n Unity 5, existen dos ediciones: Personal y Profesional. La primera es gratuita e incluye todas las funcionalidades del motor. La segunda incluye funcionalidades adicionales de soporte construcci\u00f3n en la nube, herramientas de trabajo en equipo, etc, y es de pago suscripci\u00f3n de $75 o pago \u00fanico de $1.500suscripci\u00f3n de $75 o pago \u00fanico de $1.500. La versi\u00f3n Personal podr\u00e1 ser utilizada por cualquier individuo o empresa cuyas ganancias anuales no superen los $100.000.</p> <p>Uno de los puntos fuertes de Unity es la posibilidad de exportar a gran cantidad de plataformas. Soporta las plataformas m\u00f3viles iOS, Android, Windows Phone y Blackberry, y adem\u00e1s tambi\u00e9n permite exportar a web WebGLWebGL, a videoconsolas PS4, PS3, PS Vita, Xbox One, Xbox 360, Wii U, etcPS4, PS3, PS Vita, Xbox One, Xbox 360, Wii U, etc y a ordenadores Mac, Windows y LinuxMac, Windows y Linux.</p>"},{"location":"unity.html#introduccion-a-unity","title":"Introducci\u00f3n a Unity","text":""},{"location":"unity.html#el-editor-de-unity","title":"El editor de Unity","text":"<p>Unity incorpora su propia herramienta integrada para la creaci\u00f3n de videojuegos, que nos permite incluso crear algunos videojuegos de forma visual sin la necesidad de programar.</p> <p>Dentro del entorno del editor de Unity encontramos diferentes paneles, de los cuales destacamos los siguientes:</p> <ul> <li>Project: Encontramos aqu\u00ed todos los recursos _assets__assets_ que componen nuestro proyecto. Estos recursos pueden ser por ejemplo texturas, clips de audio, scripts, o escenas. Destacamos aqu\u00ed el asset de tipo escena, que es el componente que nos permite definir cada estado pantallapantalla del juego. Al hacer doble click sobre una escena se abrir\u00e1 para trabajar con ella desde el editor.</li> <li>Hierarchy: La escena est\u00e1 formada por una serie de nodos _game objects__game objects_ organizados de forma jer\u00e1rquica. En este panel vemos el \u00e1rbol de objetos que contiene la escena abierta actualmente. Podemos seleccionar en ella cualquier objeto pulsando sobre su nombre.</li> <li>Scene: En este panel vemos de forma visual los elementos de la escena actual. Podremos movernos libremente por el espacio 3D de la escena para ubicar de forma correcta cada game object y tener una previsualizaci\u00f3n del escenario del juego.</li> <li>Inspector: Muestra las propiedades del game object o el asset seleccionado actualmente en el entorno. </li> </ul> <p></p>"},{"location":"unity.html#arquitectura-orientada-a-componentes","title":"Arquitectura Orientada a Componentes","text":"<p>Como hemos comentado, todos los elementos de la escena son objetos de tipo <code>GameObject</code> organizados de forma jer\u00e1rquica. Todos los objetos son del mismo tipo, independientemente de la funci\u00f3n que desempe\u00f1en en el juego. Lo que diferencia a unos de otros son los componentes que incorporen. Cada objeto podr\u00e1 contener varios componentes, y estos componentes determinar\u00e1n las funciones del objeto.</p> <p>Por ejemplo, un objeto que incorpore un componente <code>Camera</code> ser\u00e1 capaz de renderizar en pantalla lo que se vea en la escena desde su punto de vista. Si adem\u00e1s incorpora un componente <code>Light</code>, emitir\u00e1 luz que se proyectar\u00e1 sobre otros elementos de la escena, y si tiene un componente <code>Renderer</code>, tendr\u00e1 un contenido gr\u00e1fico que se renderizar\u00e1 dentro de la escena.</p> <p>Esto es lo que se conoce como Arquitectura Basada en Componentes, que nos proporciona la ventaja de que las funcionalidades de los componentes se podr\u00e1n reutilizar en diferentes tipos de entidades del juego. Es especialmente \u00fatil cuando tener un gran n\u00famero de diferentes entidades en el juego, pero que comparten m\u00f3dulos de funcionalidad.</p> <p>En Unity esta arquitectura se implementa mediante agregaci\u00f3n. Si bien en todos los objetos de la escena son objetos que heredan de <code>GameObject</code>, \u00e9stos podr\u00e1n contener un conjunto de componentes de distintos tipos <code>Light</code>, , <code>Camera</code>, , <code>Renderer</code>, etc, etc&lt;code&gt;Light&lt;/code&gt;, , &lt;code&gt;Camera&lt;/code&gt;, , &lt;code&gt;Renderer&lt;/code&gt;, etc, etc que determinar\u00e1n el comportamiento del objeto.</p> <p>En el inspector podremos ver la lista de componentes que incorpora el objeto seleccionado actualmente, y modificar sus propiedades:</p> <p></p> <p>En esta figura anterior podemos observar que el objeto <code>GameObject</code>&lt;code&gt;GameObject&lt;/code&gt; que contiene la c\u00e1mara del juego contiene los siguientes componentes:</p> <ul> <li><code>Transform</code>: Le da a la c\u00e1mara una posici\u00f3n y orientaci\u00f3n en la escena.</li> <li><code>Camara</code>: Hace que se comporte como c\u00e1mara. Capta lo que se ve en la escena desde su posici\u00f3n y lo renderiza en pantalla o en una textura.</li> <li><code>GUILayer</code>: Permite introducir sobre la imagen renderizada elementos de la GUI etiquetas de texto, botones, etcetiquetas de texto, botones, etc.</li> <li><code>FlareLayer</code>: Permite crear sobre la imagen renderizada un efecto de destello.</li> <li><code>AudioListener</code>: Escucha lo que se oye en la escena desde la posici\u00f3n de la c\u00e1mara y lo reproduce a trav\u00e9s de los altavoces.</li> </ul> <p>Podemos modificar los componentes, a\u00f1adiendo o eliminando seg\u00fan nos convenga. Podemos eliminar un componente pulsando sobre su cabecera en el inspector con el bot\u00f3n derecho y seleccionando Remove Component. Tambi\u00e9n podemos a\u00f1adir componentes pulsando sobre el bot\u00f3n Add Component que encontramos en la parte inferior del inspector. Por ejemplo, podr\u00edamos a\u00f1adir a la c\u00e1mara un componente que haga que podamos moverla por el escenario pulsando determinadas teclas, o podr\u00edamos eliminar el componente Audio Listener para no escuchar por los altavoces lo que se oiga en el lugar de la c\u00e1mara en su lugar podr\u00edamos optar por ponerle este componente a nuestro personaje, para reproducir lo que se oiga desde su posici\u00f3n en la escenaen su lugar podr\u00edamos optar por ponerle este componente a nuestro personaje, para reproducir lo que se oiga desde su posici\u00f3n en la escena.</p>"},{"location":"unity.html#la-escena-3d","title":"La escena 3D","text":"<p>El el editor de Unity veremos la escena con la que estemos trabajando actualmente, tanto de forma visual _Scene__Scene_ como de forma jer\u00e1rquica _Hierarchy__Hierarchy_. Nos podremos mover por ella y podremos a\u00f1adir diferentes tipos de objetos.</p>"},{"location":"unity.html#anadir-game-objects-a-la-escena","title":"A\u00f1adir game objects a la escena","text":"<p>Podemos a\u00f1adir a la escena nuevos game objects seleccionando en el men\u00fa la opci\u00f3n GameObject &gt; Create Empty, lo cual crear\u00e1 un nuevo objeto vac\u00edo con un \u00fanico componente <code>Transform</code>, al que le deber\u00edamos a\u00f1adir los componentes que necesit\u00e1semos, o bien podemos crear objetos ya predefinidos mediante GameObject &gt; Create Other.</p> <p>Entre los tipos de objetos predefinidos que nos permite crear, encontramos diferentes formas geom\u00e9tricas como Cube, Sphere, Capsule o Plane entre otras. Estas figuras pueden resultarnos de utilidad como objetos impostores en primeras versiones del juego en las que todav\u00eda no contamos con nuestros propios modelos gr\u00e1ficos. Por ejemplo, podr\u00edamos utilizar un cubo que de momento haga el papel de nuestro personaje hasta que contemos con su modelo 3D.</p> <p></p> <p>Si a\u00f1adimos por ejemplo un GameObject de tipo Cube al seleccionarlo veremos sus propiedades en el inspector:</p> <p></p> <p>Como podemos ver, este tipo de objetos geom\u00e9tricos tienen los siguientes componentes:</p> <ul> <li><code>Transform</code>: Posici\u00f3n, rotaci\u00f3n y escalado del objeto en la escena 3D.</li> <li><code>Renderer</code>: Hace que el objeto se renderice en pantalla como una maya 3D. Con este componente conseguimos que el objeto tenga una representaci\u00f3n gr\u00e1fica dentro de la escena. En este caso se representa con la forma de un cubo, tal como podemos ver indicado en el componente Mesh Filter, pero podr\u00eda ser otra forma geom\u00e9trica, o cualquier maya que hayamos creado con alguna herramienta de modelado como Autodesk Maya, 3DS Max o Blender. Tambi\u00e9n vemos que el Renderer lleva asociado un material, que se le aplicar\u00e1 a la maya al renderizarse. Podremos crear materiales e incluirlos como assets del proyecto, para as\u00ed poderlos aplicar a las mayas.</li> <li><code>Collider</code>: Hace que el objeto tenga una geometr\u00eda de colisi\u00f3n, que nos permita detectar cuando colisiona con otros objetos de la escena. En este caso la geometr\u00eda de colisi\u00f3n es de tipo caja _Box Collider__Box Collider_, para as\u00ed ajustarse a la forma de la geometr\u00eda de su representaci\u00f3n gr\u00e1fica. </li> </ul>"},{"location":"unity.html#posicionamiento-de-los-objetos-en-la-escena","title":"Posicionamiento de los objetos en la escena","text":"<p>Todos los game objects incorporan al menos un componente <code>Transform</code> que nos permite situarlo en la escena, indicando su traslaci\u00f3n, orientaci\u00f3n y escala. Podremos introducir esta informaci\u00f3n en el editor, para as\u00ed poder ajustar la posici\u00f3n del objeto de forma precisa.</p> <p>Tambi\u00e9n podemos mover un objeto de forma visual desde la vista Scene. Al seleccionar un objeto, bien en Scene o en Hierarchy, veremos sobre \u00e9l en Scene una serie de ejes que nos indicar\u00e1n que podemos moverlo. El tipo de ejes que se mostrar\u00e1n depender\u00e1 del tipo de transformaci\u00f3n que tengamos activa en la barra superior:</p> <p></p> <p>Las posibles transformaciones son:</p> <ul> <li>Traslaci\u00f3n: Los ejes aparecer\u00e1n como flechas y nos permitir\u00e1n cambiar la posici\u00f3n del objeto.</li> <li>Rotaci\u00f3n: Veremos tres c\u00edrculos alrededor del objeto que nos pemtir\u00e1n rotarlo alrededor de sus ejes x, y, z. </li> <li>Escalado: Veremos los ejes acabando en cajas, indicando que podemos escalar el objeto en x, y, z.</li> </ul> <p>Si pinchamos sobre uno de los ejes y arrastramos, trasladaremos, rotaremos, o escalaremos el objeto s\u00f3lo en dicha eje. Si pinchamos sobre el objeto, pero no sobre ninguno de los ejes, podremos trasladarlo, rotarlo y escalarlo en todos los ejes al mismo tiempo.</p>"},{"location":"unity.html#jerarquia-de-objetos","title":"Jerarqu\u00eda de objetos","text":"<p>Podremos organizar de forma jer\u00e1rquica los objetos de la escena mediante la vista Hierarchy. Si arrastramos un game object sobre otro en esta vista, haremos que pase a ser su hijo en el \u00e1rbol de la escena. Los objetos vac\u00edos con un \u00fanico componente <code>Transform</code> pueden sernos de gran utilidad para agrupar dentro de \u00e9l varios objetos. De esta forma, moviendo el objeto padre podremos mover de forma conjunta todos los objetos que contiene. De esta forma estaremos creando objetos compuestos.</p> <p>Tambi\u00e9n resulta de utilidad dar nombre a los objetos de la escena, para poder identificarlos f\u00e1cilmente. Si hacemos click sobre el nombre de un objeto en la vista Hierarchy podremos editarlo y darle un nombre significativo por ejemplo _Suelo_, _Pared_, _Enemigo_, etcpor ejemplo _Suelo_, _Pared_, _Enemigo_, etc.</p> <p></p>"},{"location":"unity.html#navegacion-en-la-escena","title":"Navegaci\u00f3n en la escena","text":"<p>Adem\u00e1s de podemos a\u00f1adir objetos a la escena y moverlos a diferentes posiciones, deberemos poder movernos por la escena para posicionarnos en los puntos de vista que nos interesen para crear el contenido. Ser\u00e1 importante conocer una serie de atajos de teclado para poder movernos con fluidez a trav\u00e9s de la escena.</p> <p>Encontramos tres tipos de movimientos b\u00e1sicos para movernos por la escena en el editor:</p> <ul> <li>Traslaci\u00f3n lateral: Hacemos click sobre la escena y arrastramos el rat\u00f3n.</li> <li>Giro: Pulsamos Alt + click y arrastramos el rat\u00f3n para girar nuestro punto de vista.</li> <li>Avance: Pultamos Ctrl + click y arrastramos el rat\u00f3n, o bien movemos la rueda del rat\u00f3n para avanzar hacia delante o hace atr\u00e1s en la direcci\u00f3n en la que estamos mirando.</li> </ul> <p>Con los comandos anteriores podremos desplazarnos libremente sobre la escena, pero tambi\u00e9n es importante conocer otras forma m\u00e1s directas de movernos a la posici\u00f3n que nos interese:</p> <ul> <li>Ver un objeto: Si nos interesa ir r\u00e1pidamente a un punto donde veamos de cerca un objeto concreto de la escena, podemos hacer doble click sobre dicho objeto en la vista Hierarchy.</li> <li>Alineaci\u00f3n con un objeto: Alinea la vista de la escena con el objeto seleccionado. Es especialmente \u00fatil cuando se utiliza con la c\u00e1mara, ya que veremos la escena tal como se estar\u00eda bien desde la camara. Para hacer esto, seleccionaremos el game object con el que nos queramos alinear y seleccionaremos la opci\u00f3n del men\u00fa GameObject &gt; Align View To Selected.</li> </ul>"},{"location":"unity.html#assets","title":"Assets","text":"<p>Los Assets en Unity hacen referencia a cualquiero item que podemos utilizar en nuestro juego/aplicaci\u00f3n. Un asset puede ser un fichero creado fuera de la plataforma Unity, por ejemplo un modelo 3D, un fichero de audio, o cualquier otro tipo de fichero que este soportado. Tambi\u00e9n existen algunos tipos de assets que pueden ser creados desde Unity, por ejemplo el controlador de una animaci\u00f3n, controlador de un jugador, mezclador de audio o el render de una textura.</p>"},{"location":"unity.html#prefabs","title":"Prefabs","text":"<p>Los prefabs son una colecci\u00f3n de GameObjects y componentes predefinidos que se pueden reutilizar en nuestra aplicaci\u00f3n. De esta forma, podemos crear un GameObject, a\u00f1adir componentes y configurarlo con una serie de valores y posteriormente reutilizarlo en nuestra escena varias veces. Un prefab almcenar\u00e1 toda esta informaci\u00f3n y adem\u00e1s a diferencia de copiar y pegar un GameObject existente en nuestra aplicaci\u00f3n, si modificamos un prefab, el resto de prefabs en la escena tambi\u00e9n se modificar\u00e1n propagando estos cambios. Al copiar y pegar un GameObject, los cambios que hagamos en el original no se propagan a la copia, por lo que cada instancia tiene sus propia configuraci\u00f3n.</p> <p>En el inspector de un prefab tenemos 3 botones que no estan presentes en un objeto normal: Select, Revert y Apply. El bot\u00f3n \"Select\" selecciona el prefab original del cual esta instancia fue creada y por lo tanto nos permite modificar todas las instancias que existan en la escena. Sin embargo, tambi\u00e9n podemos sobreescribir estos cambios en una determinada instancia de un prefab utilizando el bot\u00f3n \"Apply\". Finalmente, podemos usar el bot\u00f3n \"Revert\" para revertir la configuraci\u00f3n del prefab a los valores default valores del prefab original, antes de sobreescribir los cambiosvalores del prefab original, antes de sobreescribir los cambios.</p> <p>Los prefabs son la forma en que normalmente las librer\u00edas y SDKs tienen de ofrecer funcionalidades para Unity, por ejemplo ya veremos en la secci\u00f3n de realidad virtual o aumentada que utilizando Prefabs ya existentes podemos a\u00f1adir estas funcionalidades a nuestra aplicaci\u00f3n.</p>"},{"location":"unity.html#paquetes-de-assets","title":"Paquetes de assets","text":"<p>Los paquetes de Assets nos permiten en unity f\u00e1cilmente reutilizar projectos y colecciones de assets. De hecho como ya veremos, los <code>Standard assets</code> y los items que encontramos en la <code>Asset store</code> se descargan en forma de pquetes. Podemos importar estos paquetes bien haciendo doble click en el fichero que se genera o bien desde el menu Assets &gt; Import package. Encontramos dos tipos de paquetes, los standard, que son colecciones ya disponibles por defecto al instalar Unity, y los paquetes personalizados que pueden ser creados por usuarios y descargarse online. De igual forma podemos exportar un paquete desde nuestro proyecto. Elige Assets &gt; Export package... y nos aparecer\u00e1 un di\u00e1logo para seleccionar los assets que queremos incluir en nuestro paquete. Deja marcado la casilla <code>include dependencies</code> para a\u00f1adir autom\u00e1ticamente los assets utilizados por losque has seleccionado. Al hacer click en <code>Export</code> podremos seleccionar donde almacenar el paquete que contiene los assets previamente seleccionados.</p>"},{"location":"unity.html#asset-store","title":"Asset Store","text":"<p>Unity ha creado una tienda desde la que podemos descargar assets ya existentes. Estos assets pueden ser libres o comerciales, creados por la compa\u00f1ia Unity technologies o bien miembros de la comunidad. Podemos encontrar una gama muy variada de assets, que van desde texturas, modelos, animaciones, hasta proyectos completos, tutoriales y extensiones para el editor Unity. La tienda se puede acceder desde el propio entorno de Unity y los assets son descargados e instalados directamente en el proyecto que tengamos abierto. Los usuarios de Unity pueden publicar assets en la <code>Asset Store</code> y vender contenido que hayan creado.</p>"},{"location":"unity.html#interfaz-de-usuario","title":"Interfaz de usuario","text":"<p>El sistema con el que cuenta Unity para crear la interfaz de usuario se introdujo a partir de la versi\u00f3n 4.6. Se trata de un sistema bastante vers\u00e1til, que utilizado de forma adecuada nos permite crear interfaces como por ejemplo los men\u00fas o el HUD del juego que se adapten a diferentes tama\u00f1os y formas de pantalla.</p> <p>Todo el contenido de la interfaz de usuario estar\u00e1 contenido en nuestra escena dentro de un elemento tipo <code>Canvas</code> es decir, un game object que cuente con un componente <code>Canvas</code>es decir, un &lt;em&gt;game object&lt;/em&gt; que cuente con un componente &lt;code&gt;Canvas&lt;/code&gt;. Dentro de \u00e9l ubicaremos todos los componentes de la interfaz, como por ejemplo im\u00e1genes, etiquetas de texto o botones.</p>"},{"location":"unity.html#canvas","title":"Canvas","text":"<p>El <code>Canvas</code> ser\u00e1 el panel 2D _lienzo__lienzo_ donde podremos crear el contenido de la interfaz de usuario. Los componentes de la interfaz siempre deber\u00e1n estar dentro de un <code>Canvas</code> en la jerarqu\u00eda de la de escena. Si intentamos arrastrar sobre la escena un componente de la UI sin un <code>Canvas</code>, el <code>Canvas</code> se crear\u00e1 de forma autom\u00e1tica.</p> <p></p> <p>Una propiedad importante del componente <code>Canvas</code> es Render Mode, que podr\u00e1 tomar 3 valores:</p> <ul> <li>Screen Space - Overlay: El <code>Canvas</code> se dibuja sobre el contenido que aparece en pantalla, ajust\u00e1ndose siempre al tama\u00f1o de la misma.</li> <li>Screen Space - Camera: Similar a la anterior, pero en este caso debemos vincularlo a una c\u00e1mara, indicando la distancia a la que estar\u00e1 el <code>Canvas</code> de la c\u00e1mara seleccionada, y el <code>Canvas</code> se ajustar\u00e1 al tama\u00f1o que tenga el tronco de la c\u00e1mara a dicha distancia. Se aplicar\u00e1n sobre el <code>Canvas</code> los par\u00e1metros de la c\u00e1mara seleccionada.</li> <li>World Space: En este caso el <code>Canvas</code> se comportar\u00e1 como cualquier otro objeto 3D en la escena. Le daremos un tama\u00f1o fijo al panel y lo situaremos en una posici\u00f3n del mundo. As\u00ed podremos tener interfaces con las que podamos interactuar en nuestro mundo 3D.</li> </ul> <p>En la previsualizaci\u00f3n de la escena, cuando tengamos un <code>Canvas</code> de tipo Screen Space es posible que lo veamos de un tama\u00f1o mucho mayor que el resto de elementos de la escena. Esto se debe a que las unidades con las que trabaja internamente el <code>Canvas</code> son pixels en pantalla, mientras que es habitual que los elementos de la escena tengan dimensiones de alrededor de una unidad. Al ejecutar el juego no habr\u00e1 ning\u00fan problema ya que el <code>Canvas</code> se ajustar\u00e1 al tama\u00f1o de la pantalla o de la c\u00e1mara.</p>"},{"location":"unity.html#elementos-de-la-ui","title":"Elementos de la UI","text":"<p>Una vez contamos con un <code>Canvas</code>, podemos a\u00f1adir dentro de \u00e9l diferentes componentes de la interfaz. Encontramos diferentes tipos de componentes como etiquetas de texto, im\u00e1genes o botones que podremos incluir de forma sencilla.</p>"},{"location":"unity.html#etiquetas-de-texto","title":"Etiquetas de texto","text":"<p>Las etiquetas de texto ser\u00e1n objetos con un componente de tipo <code>Text</code>. Este componente nos permite indicar principalmente el texto a mostrar en la etiqueta, y tambi\u00e9n podemos especificar otras propiedades como la fuente, color, alineaci\u00f3n o espaciado.</p> <p>Es de especial inter\u00e9s la propiedad Best Fit. Con ella podemos especificar un tama\u00f1o m\u00e1ximo y m\u00ednimo de la fuente, de forma que se ajuste de forma autom\u00e1tica entre estos valores al tama\u00f1o m\u00e1ximo que podamos tener sin que el texto se salga de su espacio. Es interesante cuando el texto puede ser variable por ejemplo diferentes idiomaspor ejemplo diferentes idiomas y hay riesgo de que en alg\u00fan caso quede demasiado largo y pudiera mostrarse truncado.</p> <p></p>"},{"location":"unity.html#imagenes","title":"Im\u00e1genes","text":"<p>Las im\u00e1genes son objetos con un componente <code>Image</code>. En este componente deberemos introducir principalmente la textura o sprite a mostrar como imagen, que deberemos haber introducido previamente entre los assets del proyecto.</p> <p>Utilizaremos preferiblemente im\u00e1genes de tipo PNG, y las podremos incluir como assets simplemente arrastr\u00e1ndolas sobre la secci\u00f3n Project del editor podemos organizarlas en carpetaspodemos organizarlas en carpetas.</p> <p></p>"},{"location":"unity.html#botones","title":"Botones","text":"<p>Un bot\u00f3n es un objeto con un componente <code>Button</code>, que adem\u00e1s contendr\u00e1 como hijo un objeto de tipo Text para mostrar la etiqueta del bot\u00f3n se trata por separado el bot\u00f3n de su etiquetase trata por separado el bot\u00f3n de su etiqueta. El bot\u00f3n tendr\u00e1 una imagen que podremos mostrar como marco, y distintos colores con los que tintar el bot\u00f3n seg\u00fan el estado en el que se encuentre:</p> <ul> <li>Normal: Bot\u00f3n activo pero sin pulsar ni seleccionar.</li> <li>Highlighted: Bot\u00f3n activo y seleccionado para que se pueda pulsar. Este estado ser\u00e1 \u00fatil cuando utilicemos un control mediante joystick o teclado: al pulsar los controles direccionales cambiaremos el bot\u00f3n seleccionado, y al pulsar la tecla de acci\u00f3n presionaremos el bot\u00f3n seleccionado actualmente.</li> <li>Pressed: Bot\u00f3n actualmente presionado. </li> <li>Disabled: Indica que el bot\u00f3n est\u00e1 deshabilitado y que no puede ser presionado.</li> </ul> <p>Adem\u00e1s en la parte inferior podremos conectar el evento On Click del bot\u00f3n con alg\u00fan m\u00e9todo de nuestro c\u00f3digo, para que al pulsar sobre el bot\u00f3n se ejecute dicho m\u00e9todo.</p> <p></p>"},{"location":"unity.html#posicionamiento-en-el-espacio-de-la-ui","title":"Posicionamiento en el espacio de la UI","text":"<p>Todos los elementos de la UI de Unity se posicionan mediante un componente de tipo <code>RectTransform</code> a diferencia del resto de objetos que tienen un componente <code>Tranform</code>a diferencia del resto de objetos que tienen un componente &lt;code&gt;Tranform&lt;/code&gt;.</p> <p>La principal diferencia de <code>RectTransform</code> sobre <code>Tranform</code> es que nos permite indicar el \u00e1rea rectangular que ocupar\u00e1 el componente dentro del <code>Canvas</code>, adem\u00e1s de las propiedades de posici\u00f3n, rotaci\u00f3n y escala que tenemos en todos los objetos.</p> <p></p> <p>A la hora de definir el rect\u00e1ngulo que un componente de la UI ocupar\u00e1 en pantalla, lo primero que deberemos hacer es definir a qu\u00e9 posici\u00f3n de pantalla lo vamos a anclar es decir, qu\u00e9 posici\u00f3n de la pantalla tomaremos como referencia para posicionarloes decir, qu\u00e9 posici\u00f3n de la pantalla tomaremos como referencia para posicionarlo. Si es un men\u00fa que queramos que aparezca centrado, lo deberemos anclar al centro de la pantalla, mientras que si es un marcador que queramos que aparezca en una esquina de la pantalla, lo conveniente ser\u00e1 anclarlo a dicha esquina. Unity nos proporciona algunos valores predefinidos t\u00edpicos para el anclaje _Anchor Presets__Anchor Presets_:</p> <p></p> <p>La posici\u00f3n del objeto ser\u00e1 relativa siempre al punto de anclaje. Adem\u00e1s, con la propiedad Pivot indicaremos el punto del objeto rect\u00e1ngulorect\u00e1ngulo que haremos coincidir con la posici\u00f3n especificada. El Pivot se indicar\u00e1 siempre en coordenadas normalizadas, entre 0,00,0 y 1,11,1. Con 0,00,0 indicamos la esquina inferior izquierda, con 1,11,1 la esquina superior derecha, y con 0.5,0.50.5,0.5 el centro del rect\u00e1ngulo.</p> <p>Por ejemplo, si queremos situar un bot\u00f3n centrado en pantalla, utilizaremos como punto de anclaje el centro de la pantalla, como posici\u00f3n 0,0,00,0,0 para situarlo exactamente en el punto de anclaje, y como pivot 0.5,0.50.5,0.5 para que el bot\u00f3n aparezca centrado en dicho punto. A continuaci\u00f3n vemos un ejemplo:</p> <p></p> <p>En caso de querer ubicar un marcador en la esquina superior derecha de la pantalla, en primer lugar deberemos establecer el anclaje en dicha esquina. Para que quede lo mejor ajustado posible es recomendable que el pivot en este caso sea 1,11,1, para que as\u00ed lo que posicionemos sea la esquina superior derecha. De esta forma, si como posici\u00f3n indicamos 0,0,00,0,0 el objeto quedar\u00e1 perfectamente ajustado a la esquina. Podr\u00edamos modificar la posici\u00f3n si queremos darle un margen respecto a la esquina, pero de esta forma siempre quedar\u00e1 bien ajustado a la esquina y no se nos saldr\u00e1 de pantalla. A continuaci\u00f3n vemos un ejemplo:</p> <p></p> <p>Un caso algo m\u00e1s complejo es aquel en el que queremos que un elemento pueda estirarse. En este caso, en lugar de tener un ancla \u00fanica, podemos establecer un ancla para cada esquina del objeto. Podemos ver sobre la imagen una serie de flechas blancas alrededor del t\u00edtulo que definen los puntos de anclaje. Podemos arrastrar dichas flechas pulsando sobre ellas con el rat\u00f3n para as\u00ed modificar el anclaje:</p> <p></p> <p>Vemos que al tener este anclaje abierto en lugar de dar una posici\u00f3n x,y,zx,y,z al objeto lo que debemos introducir son valores Left, Right, Top y Bottom. Aqu\u00ed introduciremos la distancia que debe haber entre los l\u00edmites del objeto y la zona de anclaje. La zona de anclaje ser\u00e1 siempre relativa al tama\u00f1o del <code>Canvas</code> se estirar\u00e1 y se contraer\u00e1 seg\u00fan la pantalla o ventana donde se ejecute el juegose estirar\u00e1 y se contraer\u00e1 seg\u00fan la pantalla o ventana donde se ejecute el juego. De esta forma conseguimos que el rect\u00e1ngulo de nuestro componente se estire o se contraiga tambi\u00e9n seg\u00fan el espacio disponible, pudiendo hacer as\u00ed por ejemplo que el t\u00edtulo ocupe todo el espacio disponible.</p>"},{"location":"unity.html#escalado-del-canvas","title":"Escalado del Canvas","text":"<p>Normalmente en nuestro <code>Canvas</code> encontraremos un componente adicional que es el <code>CanvasScaler</code>. Se trata de un elemento importante a la hora de conseguir interfaces adaptables, ya que nos permite personalizar la forma en la que se escala el contenido del <code>Canvas</code> a distintos tama\u00f1os de pantalla.</p> <p>Podemos optar por tres modos:</p> <ul> <li>Constant Pixel Size: Los tama\u00f1os de los objetos de la interfaz se especifican en p\u00edxels. Si la pantalla tiene m\u00e1s densidad los objetos se ver\u00e1n m\u00e1s peque\u00f1os a no ser que los hagamos _estirables_ como hemos visto en el apartado anteriora no ser que los hagamos _estirables_ como hemos visto en el apartado anterior. </li> <li>Constant Physical Size: En este caso, a diferencia del anterior, los objetos siempre ocupar\u00e1n el mismo espacio f\u00edsico, independientemente de la densidad de pantalla. El posicionamiento y tama\u00f1o de los objetos se especifican en p\u00edxeles, pero para una determinada densidad de referencia indicada en dpi dentro del componente <code>CanvasScaler</code>indicada en &lt;em&gt;dpi&lt;/em&gt; dentro del componente &lt;code&gt;CanvasScaler&lt;/code&gt;. Si la pantalla destino tiene una densidad diferente, todos los valores se actualizar\u00e1n para que todo acabe teniendo las mismas dimensiones f\u00edsicas en _cm_ realesen _cm_ reales.</li> <li>Scale With Screen Size: Este \u00faltimo modo nos permite hacer que todo el contenido de la UI se escale seg\u00fan el tama\u00f1o de la pantalla destino. Dise\u00f1aremos la interfaz para un tama\u00f1o de pantalla en pixelesen pixeles de referencia, indicado mediante una propiedad del componente <code>CanvasScaler</code>. Si la pantalla tiene diferente ancho o alto, todos los valores se escalar\u00e1n de forma proporcional. Podemos indicar si queremos que se escale s\u00f3lo seg\u00fan el ancho, s\u00f3lo seg\u00fan el alto, o seg\u00fan una mezcla de ambos. </li> </ul> <p>El modo Constant Pixel Size ser\u00e1 poco adecuado cuando estemos destinando el juego a pantallas que puedan tener diferente densidad, pero en caso de que no sea as\u00ed nos permitir\u00e1 ajustar mejor los contenidos de la interfaz por ejemplo en caso de PC o videoconsolaspor ejemplo en caso de PC o videoconsolas.</p> <p>En caso de destinar nuestro juego a dispositivos m\u00f3viles, lo recomendable ser\u00e1 utilizar Constant Physical Size o Scale With Screen Size. La primera nos puede venir bien por ejemplo para men\u00fas, donde nos interese que siempre los botones tengan siempre el mismo tama\u00f1o suficiente para poder pulsar con el dedo sobre \u00e9l, pero que no ocupe toda la pantalla en un dispositivos grandesuficiente para poder pulsar con el dedo sobre \u00e9l, pero que no ocupe toda la pantalla en un dispositivos grande. Por otro lado, para elementos del HUD con los que no tenemos que interactuar nos puede venir bien la segunda opci\u00f3n, para as\u00ed hacer que se escale seg\u00fan la pantalla y no ocupen demasiado espacio en dispositivos peque\u00f1os.</p>"},{"location":"unity.html#realidad-virtual","title":"Realidad Virtual","text":"<p>Existen diferentes dispositivos de realidad virtual, que nos proporcionan una inmersi\u00f3n casi total en la escena, reflejando en la c\u00e1mara los movimientos de nuestra cabeza, y proporcionando una visi\u00f3n estereosc\u00f3pica de la escena. Entre los dispositivos m\u00e1s famosos se encuentran Oculus Rift, Samsung Gear VR y Google Cardboard. Aunque todos estos dispositivos proporcionan su propio SDK que podemos integrar en las plataformas nativas de desarrollo, es de especial inter\u00e9s su integraci\u00f3n en el entorno Unity, que nos permitir\u00e1 realizar aplicaciones que los soporten de forma casi inmediata. A continuaci\u00f3n veremos c\u00f3mo utilizarlos con este entorno.</p>"},{"location":"unity.html#oculus-rift-samsung-gear-vr","title":"Oculus Rift / Samsung Gear VR","text":"<p>A partir de Unity 5.1 encontramos en este entorno soporte nativo para los dispositivos Oculus Rift y Samsung Gear VR. Ambos utilizan el mismo SDK y herramientas, con la diferencia de que Oculus Rift funciona sobre plataformas de sobremesa, mientras que Samsung Gear VR funciona sobre m\u00f3viles Samsung.</p> <p>Para activar el soporte para estos dispositivos en Unity simplemente tendremos que entrar en Player Settings _Edit &gt; Project Settings &gt; Player__Edit &amp;gt; Project Settings &amp;gt; Player_ y bien dentro de la plataforma Standalone para Oculus Riftpara Oculus Rift o Android para Samsung Gear VRpara Samsung Gear VR marcar la casilla Virtual Reality Supported, dentro de la secci\u00f3n Other Settings &gt; Rendering.</p> <p></p> <p>Una vez hecho esto, autom\u00e1ticamente la c\u00e1mara de la escena se comportar\u00e1 como una c\u00e1mara VR, girando cuando giremos la cabeza y renderizando una imagen para cada ojo, para as\u00ed proporcionar visi\u00f3n est\u00e9reo.</p>"},{"location":"unity.html#despliegue-de-la-aplicacion-en-un-dispositivo-de-prueba","title":"Despliegue de la aplicaci\u00f3n en un dispositivo de prueba","text":"<p>Antes de desplegar la aplicaci\u00f3n en un dispositivo de prueba, deberemos a\u00f1adir una firma que nos deber\u00e1 proporcionar Oculus para nuestro dispositivo concreto. Dicha firma s\u00f3lo se necesitar\u00e1 durante el desarrollo, cuando la aplicaci\u00f3n se publique ya no har\u00e1 falta.</p> <p>Para conseguir la firma en primer lugar necesitamos obtener el ID de nuestro dispositivo. Para ello lo conectaremos al sistema y ejecutaremos el comando:</p> <pre><code>adb devices\n</code></pre> <p>En la lista de dispositivos en la primera columna veremos los IDs que buscamos, con el siguiente formato:</p> <pre><code>* daemon started successfully *\n1235ba5e7a311272    device\n</code></pre> <p>En este caso, el ID que buscamos ser\u00eda <code>1235ba5e7a311272</code>. Una vez localizado dicho ID, iremos a la siguiente p\u00e1gina para solicitar la firma e introduciremos el ID necesitaremos registrarnos previamente como usuarios de Oculus Developer, si no tenemos cuenta todav\u00edanecesitaremos registrarnos previamente como usuarios de Oculus Developer, si no tenemos cuenta todav\u00eda:</p> <p>https://developer.oculus.com/osig/</p> <p>Una vez introducido el ID nos descargar\u00e1 un fichero <code>.osig</code> que deber\u00e1 ser introducido en nuestro proyecto de Unity en el siguiente directorio:</p> <pre><code>Assets/Plugins/Android/assets\n</code></pre> <p>Esto lo que har\u00e1 ser\u00e1 colocar dicho fichero en el directorio <code>assets</code> del proyecto Unity resultante. Una vez hecho esto ya podremos probar la aplicaci\u00f3n en un dispositivo Samsung con Gear VR seleccionando la plataforma Android y pulsando sobre Build &amp; Run.</p> <p>Al desplegar la aplicaci\u00f3n en el m\u00f3vil Samsung, veremos que al ejecutarla nos pide conectar el dispositivo Gear VR al m\u00f3vil. Una vez conectado, se ejecutar\u00e1 la aplicaci\u00f3n y podremos ver nuestra escena de Unity de forma inmersiva.</p> <p></p> <p>Sin embargo, veremos que la imagen aparece algo distorsionada al verla a trav\u00e9s de las lentes del Gear VR. Esto se debe a que aunque la c\u00e1mara renderiza en est\u00e9reo y responde al movimiento de la cabeza, no se realiza la correcci\u00f3n adecuada a la imagen para verla a trav\u00e9s de las lentes del Gear VR.</p>"},{"location":"unity.html#utilidades-de-oculus","title":"Utilidades de Oculus","text":"<p>Aunque la c\u00e1mara de Unity puede ser utilizada para aplicaciones de VR, hemos visto que tiene algunas carencias como por ejemplo el no realizar una correcci\u00f3n de la distorsi\u00f3n que realiza la lente.</p> <p></p> <p>Para poder resolver estas carencias y tener la posibilidad de configurar e implementar diferentes componentes de la aplicaci\u00f3n VR, Oculus proporciona una serie de utilidades en forma de paquete de assets para Unity que podemos descargar desde su web:</p> <p>https://developer.oculus.com/downloads/</p> <p>Desde esta p\u00e1gina podremos bajar tanto versiones actualizadas del plugin de Oculus/Gear para Unity _OVR Plugin for Unity 5__OVR Plugin for Unity 5_ como las utilidades adicionales _Oculus Utilities for Unity 5__Oculus Utilities for Unity 5_.</p> <p>Para instalar el plugin simplemente tendremos que buscar dentro del directorio de instalaci\u00f3n de Unity el directorio <code>VR</code> en caso de MacOS tendremos que mirar dentro del contenido del paquete <code>Unity</code> para localizar dicho directorio para localizar dicho directorioen caso de MacOS tendremos que mirar dentro del contenido del paquete &lt;code&gt;Unity&lt;/code&gt; para localizar dicho directorio para localizar dicho directorio, y dentro de \u00e9l sustituir el directorio <code>oculus</code> y todo su contenido por el proporcionado por el plugin.</p> <p>Una vez actualizado el plugin podremos a\u00f1adir las utilities carg\u00e1ndolo en el proyecto como paquete de assets.</p> <p>Uno de los assets m\u00e1s \u00fatiles es el prefab <code>OVRCameraRig</code>. Podemos utilizarlo en la escena en lugar de la c\u00e1mara de Unity, y nos permitir\u00e1 configurar diferentes propiedades de la c\u00e1mara en la escena 3D, como por ejemplo la distancia entre los ojos. Adem\u00e1s, aplicar\u00e1 de forma correcta la correcci\u00f3n a la distorsi\u00f3n introducida por las lentes.</p> <p></p>"},{"location":"unity.html#modo-de-desarrollador","title":"Modo de desarrollador","text":"<p>En los casos anteriores hemos probado la aplicaci\u00f3n con el dispositivo Gear VR. Sin embargo, durante el desarrollo nos podr\u00eda interesar poder probar la aplicaci\u00f3n en el m\u00f3vil sin la necesidad de tener que conectarlo al Gear VR.</p> <p>Podemos conseguir esto activando el modo desarrollador de Gear VR en el dispositivo m\u00f3vil Samsung. Para poder hacer esto antes deberemos haber instalado alguna aplicaci\u00f3n nuestra con la firma <code>osig</code>, en caso contrario no nos permitir\u00e1 activarlo.</p> <p>Para activar el modo de desarrollador de Gear VR deberemos entra en Ajustes &gt; Aplicaciones &gt; Administrador de aplicaciones &gt; Gear VR Service &gt; Almacenamiento &gt; Administrar almacenamiento y pulsar repetidas veces sobre VR Service Version. Tras hacer esto nos aparecer\u00e1n opciones para activar el modo de desarrollador.</p> <p></p> <p>Con este modo activo podremos lanzar la aplicaci\u00f3n en el m\u00f3vil sin tener que conectar el dispositivo Gear VR, lo cual agilizar\u00e1 el desarrollo. Esta forma de probar la aplicaci\u00f3n tendr\u00e1 la limitaci\u00f3n de que no reconocer\u00e1 el giro de la c\u00e1mara, ya que los sensores que utilizan estas aplicaciones para obtener la inclinaci\u00f3n de la cabeza van integrados en el dispositivo Gear VR.</p>"},{"location":"unity.html#google-vr","title":"Google VR","text":"<p>Google VR, originalmente conocido como Google Cardboard es el SDK de Google para desarrollar aplicaciones de realidad virtual usando sus visores de VR. Hasta ahora principalmente el Google Cardboard, pero recientemente han lanzado el primer producto similar al anteriormente visto, Samsung Gear VR, conocido como Google Daydream. El SDK de Google VR es compatible con cualquiera de sus visores por lo que las aplicaciones que desarrollemos para el Google Cardboard se puede utilizar tambi\u00e9n en Google Daydream.</p> <p>https://developers.google.com/vr/unity/</p> <p>El plugin consiste en un paquete de assets que podemos incluir en nuestro proyecto deberemos a\u00f1adir todo su contenidodeberemos a\u00f1adir todo su contenido.</p> <p>Recientemente ha aparecido una compilaci\u00f3n personalizada de Unity que incorpora el SDK de Google VR de forma nativa. Esta compilaci\u00f3n personalizada, que se basa en Unity 2017, es compatible el objetivo de compilaci\u00f3n de Android con soporte para VR para Daydream y el Samsung Gear VR. Entre las caracter\u00edsticas de GoogleVR para desarrollar aplicaciones encontramos el soporte para controladores, escenas de ejemplos, scripts con ciertas funciones ya implementadas, sonido espacial, etc\u00e9tera.</p> <p>A continuaci\u00f3n vamos a ver la forma de a\u00f1adir soporte a nuestro proyecto Unity para utilizar el visor de tipo Cardboard o similares. Al igual que con otros plugins o addons, se han creado una serie de prefabs que nos van a facilitar la tarea de hacer nuestro proyecto compatible. Lo primero que tenemos que hacer es configurar el proyecto para que soporte Realidad Virtual, o en su defecto generar un par de im\u00e1genes est\u00e9reo para cada ojo. Desde player setting, habilitaremos en la secci\u00f3n XR settings el soporte para Realidad virtual y el conjunto de SDKs a utilizar, en nuestro caso el de Cardboard.</p> <p></p> <p>Una vez configurado el proyecto, el primer prefab que tenemos que a\u00f1adir es <code>GvrEditorEmulator</code>, este nos va permitir que al ejecutar nuestra escena se active el rendering est\u00e9reo, as\u00ed como la emulaci\u00f3n en el editor Unity del movimiento de la cabeza utilizando el teclado. En concreto, utilizaremos Alt + rat\u00f3n para girar la cabeza y Ctrl + rat\u00f3n para inclinarla. De igual manera, al hacer click con el rat\u00f3n simularemos la pulsaci\u00f3n del bot\u00f3n disponible en los visores tipo Cardboard. Gracias al uso de este prefab ya podremos utilizar la orientaci\u00f3n de nuestra cabeza para ver la escena completo 360\u00ba360\u00ba. Adem\u00e1s, Google VR SDK incluye un sistema de eventos para permitir que los desarrolladores creen aplicaciones con cierta capacidad de interacci\u00f3n con la escena. Este sistema es compatible tanto con Daydream como Cardboard. Para utilizarlo tendremos que a\u00f1adir a nuestra escena el prefab <code>GvrEventSystem</code>. Una vez a\u00f1adido este prefab podremos hacer uso en nuestra escena de un puntero para seleccionar objetos o interactuar con la escena. Dentro del SDK encontramos el prefab <code>GvrReticlePointer</code> que una vez a\u00f1adido a nuestra escena, en concreto deberemos a\u00f1adirlo a nuestra c\u00e1mara principal, el sistema de puntero aparecera en nuestra escena una vez lo ejecutemos Cuando enfoquemos un objeto con el que se pueda interactuar el puntero generar\u00e1 una animaci\u00f3n que podemos configurar y personalizarCuando enfoquemos un objeto con el que se pueda interactuar el puntero generar\u00e1 una animaci\u00f3n que podemos configurar y personalizar. Finalmente, el SDK cuenta con dos scripts para habilitar el sistema de interacci\u00f3n: <code>GvrPointerGraphicRaycaster</code> y <code>GvrPointerPhysicsRaycaster</code> . Estos scripts sustituyen a las versiones equivalentes de Unity para calcular la intersecci\u00f3n de un rayo virtual lanzado Ray castingRay casting desde la c\u00e1mara hacia la escena y hacia gr\u00e1ficos dibujados en un canvas 2D. Los scripts se encuentran en la siguiente ruta: <code>Assets/GoogleVR/Scripts/EventSystem</code>. Estos scripts se a\u00f1adir\u00e1n como componentes a la c\u00e1mara principal para que funcionen correctamente. Adem\u00e1s, cada objeto de la escena con el que queramos interactuar debe responder a los eventos generados, para ello bien podemos utilizar el componente <code>EventTrigger</code> o el sistema de eventos de Unity mediante scripts que desarrollemos y a\u00f1adamos a los objetos. En el caso de la interacci\u00f3n con modelos 3D a trav\u00e9s del script <code>GvrPointerGraphicRaycaster</code>  es necesario que los modelos tengan una componente collider, permitiendo as\u00ed que el_ raycaster_ pueda detectar la colisi\u00f3n con el modelo 3D.</p> <p>La jerarqu\u00eda de objetos en la escena quedar\u00eda como se muestra en la siguiente imagen:</p> <p></p> <p>En la siguiente imagen podemos ver la previsualizaci\u00f3n de la c\u00e1mara en la escena, en las \u00faltimas versiones del SDK se ha deshabilitado el rendering previo de las dos vistas est\u00e9reo. Se puede apreciar como al enfocar el objeto flotante, el puntero produce una animaci\u00f3n, en este caso formando un circulo que se expande y se contrae. De esta forma podemos facilmente identificar los objetos de la escena con los que se puede interactuar.</p> <p></p> <p>Con esto tendr\u00edamos la funcionalidad b\u00e1sica para convertir nuestra escena en Unity en una experiencia de realidad virtual. El SDK est\u00e1 en cont\u00ednuo desarrollo por lo que su API sufre cambios de forma frecuente. Para implementar otra de las caracter\u00edsticas claves en una experiencia de realidad virtual, el sonido espacial, podemos tambi\u00e9n utilizar el SDK de GoogleVR. Este incluye toda la funcionalidad necesaria para dotar a nuestra escena de sonido espacial e immersivo. Podemos consultar una gu\u00eda sobre su utilizaci\u00f3n en Unity en el siguiente enlace: https://developers.google.com/vr/unity/spatial-audio.</p> <p>Gu\u00eda uso Google VR SDK v0.6 e inferiores Versi\u00f3n SDK anterior, no recomendado utilizarse, solo para mantenimiento de proyectos existentes, legacy Versi\u00f3n SDK anterior, no recomendado utilizarse, solo para mantenimiento de proyectos existentes, legacy </p> <p>La forma m\u00e1s sencilla de a\u00f1adir soporte a nuestro proyecto Unity para Cardboard o similares es a\u00f1adir a nuestro proyecto el prefab <code>GvrViewerMain</code>. Este prefab tiene los componentes necesarios de forma que al ejecutar nuestra aplicaci\u00f3n se generar\u00e1n las im\u00e1genes est\u00e9reo utilizando nuestra c\u00e1mara principal de la escena.</p> <p></p> <p>Desde este prefab podemos ajustar la distorsi\u00f3n de las lentes, as\u00ed como seleccionar el tama\u00f1o de la pantalla sobre el que la aplicaci\u00f3n se va a ejecutar. Por ahora, adem\u00e1s nos permite seleccionar que versi\u00f3n de Google Cardboard vamos a utilizar, de forma que se generar\u00e1n las vistas correspondientes para el visor seleccionado distorsi\u00f3ndistorsi\u00f3n.</p> <p></p> <p>Adem\u00e1s, el SDK de Google para Unity cuenta con otros prefabs que nos permiten facilmente utilizar el movimiento de la cabeza para interactuar con nuestra escena. Usando el prefab <code>GvrReticle</code> podemos implementar esta funcionalidad en nuestra aplicaci\u00f3n. Tambi\u00e9n incorpora otras utilidades como un prefab para mostrar una etiqueta flotante en nuestra aplicaci\u00f3n con la tasa de im\u00e1genes por segundo a la que la aplicaci\u00f3n esta siendo ejecutada: <code>GvrFPSCanvas</code>. Esto es importante ya que en las aplicaciones de Realidad Virtual es necesario maximizar la tasa de im\u00e1genes por segundo de forma que el usuario no sienta sensaci\u00f3n de mareo o perciba y que el movimiento de la c\u00e1mara alrededor de la escena sea lo m\u00e1s suave posible.</p> <p>Para integrar el <code>GvrReticle</code> a nuestra aplicaci\u00f3n y habilitar la interacci\u00f3n con modelos de la escena tendremos que seguir los siguientes pasos. Primero, a\u00f1adimos el prefab <code>GvrReticle</code> y los ponemos en la jerarqu\u00eda de nuestra aplicaci\u00f3n debajo de la c\u00e1mara principal. Desde el inspector del prefab podremos cambiar algunos ajustes como el color del puntero, la forma, o la animaci\u00f3n que se produce al apuntar a un objecto de nuestra escena con el que se puede interactuar. Adem\u00e1s, tenemos que a\u00f1adir a nuestra escena un objeto <code>EventSystem</code> y a\u00f1adir la componente <code>Gaze Input Module</code>.  Por \u00faltimo, a\u00f1adiremos el objeto con el que vamos a interactuar, por ejempo un cubo, y nos aseguraremos que tiene habilitada la componente <code>Box Collider</code>. De esta forma nuestra aplicaci\u00f3n en Unity podr\u00e1 comprobar cuando el puntero intersecta con el cubo 3D. Tambi\u00e9n tenemos que a\u00f1adir a nuestra c\u00e1mara principal la componente <code>Physics Raycaster (Script)</code>, de forma que nuestra c\u00e1mara podr\u00e1 detectar colisiones con los objetos de la escena mediante  <code>raycasting</code>.</p> <p></p>"},{"location":"unity.html#realidad-aumentada","title":"Realidad Aumentada","text":"<p>Realidad aumentada es la expresi\u00f3n que usamos para referirnos a las tecnolog\u00edas que nos permiten superponer modelos virtuales en el mundo real. La mayor diferencia con el t\u00e9rmino realidad virtual es que la realidad aumentada mezcla el mundo virtual y el real, mientras que en la realidad virtual no percibimos el mundo real y estamos immersos en un mundo virtual no hay percepci\u00f3n del entorno en el que estamosno hay percepci\u00f3n del entorno en el que estamos.</p> <p>Si pensamos en la expresi\u00f3n \"realidad aumentada\" aplicada a dispositivos m\u00f3viles, se traduce a la capacidad de superponer contenido en el mundo real a trav\u00e9s de la c\u00e1mara del dispositivo. Esto nos permite \"mejorar\" el mundo real superponiendo informaci\u00f3n o contenido multimedia. Por ejemplo, apuntar con la c\u00e1mara de nuestro tel\u00e9fono a una foto y que nos muestre una ventana 3D a trav\u00e9s de la pantalla de nuestro tel\u00e9fono donde podemos reproducir un v\u00eddeo. Cientos de aplicaciones relacionadas se estan utilizando en museos de todo el mundo para mostrar informaci\u00f3n tridimensional y aportar informaci\u00f3n adicional de objetos que se exhiben en los museos. Gracias a las pantallas t\u00e1ctiles de nuestros tel\u00e9fonos, la realidad aumentada tambi\u00e9n nos proporciona cierta capaz de interacci\u00f3n entre el contenido virtual y el mundo real.</p> <p>Actualmente la mayor\u00eda de tecnolog\u00edas que permiten \"aumentar la realidad\" utilizan marcadores con una forma conocida a priori, plano, cubo, cilindro, etc\u00e9tera, de forma que el software utilizado es capaz de detectar dicho marcador y mostrar el contenido alrededor del mismo. Pese a que la mayor\u00eda utilizan marcadores, cabe destacar que en los \u00faltimos a\u00f1os con la aparici\u00f3n de sensores 3D como el dispositivo Kinect de Microsoft, la utilizaci\u00f3n de marcadores se ha reducido, ya que tenemos informaci\u00f3n geom\u00e9trica del entorno mundo realmundo real y por lo tanto podemos superponer informaci\u00f3n 3D de forma precisa. Debido a que la mayor\u00eda de dispositivos m\u00f3viles no disponen de sensores 3D, la mayor\u00eda de tecnolog\u00edas para \"aumentar la realidad\" en estos dispositivos se basan en la utilizaci\u00f3n de marcadores. En \u00e9l ultimo a\u00f1o, han aparecido varios dispositivos m\u00f3viles con capacidad de aumentar la realidad, el primero de ellos es un proyecto de la empresa Google, conocido como project Tango, el cual propone un disposito m\u00f3vil con c\u00e1mara 3D capaz de mapear el entorno y superponer contenido. El otro proyecto a destacar ha sido desarrollado por la empresa Microsoft y se llama HoloLens. Este es un casco de realidad aumentada o realidad mixta como ellos lo denominan. Este dispositivo m\u00f3vil es capaz de mapear el entorno y visualizar contenido tridimensional en nuestro entorno como si de verdad estuviera ah\u00ed.</p> <p>Por \u00faltimo mencionar que existen otras tecnolog\u00edas de realidad virtual basadas en posicionamiento GPS, de forma que utilizando las coordenadas proporcionadas por un sistema GPS habilitan en ciertos lugares la visualizaci\u00f3n de cierta informaci\u00f3n adicional utilizando la c\u00e1mara del dispositivo m\u00f3vil.</p> <p></p>"},{"location":"unity.html#librerias-de-realidad-aumentada","title":"Librer\u00edas de realidad aumentada","text":"<p>Actualmente existen varias librer\u00edas de realidad aumentada que nos permiten desarrollar aplicaciones para smartphones, tablets e incluso gafas de realidad aumentada. A continuaci\u00f3n vamos a revisar las caracter\u00edsticas de algunas de estas librer\u00edas.</p>"},{"location":"unity.html#droidar","title":"DroidAR","text":"<p>DroidAR es una librer\u00eda open-source para Android que nos ofrece funcionalidades de realidad aumentada mediante la detecci\u00f3n de puntos de inter\u00e9s basada en localizaci\u00f3n GPSbasada en localizaci\u00f3n GPS as\u00ed como la detecci\u00f3n de marcadores. Mediante el uso de la librer\u00eda libGLX adem\u00e1s permite cargar modelos 3D y animaciones para visualizar mediante marcadores o la localizaci\u00f3n del dispositivo. Destacar que la documentaci\u00f3n de esta librer\u00eda no es demasiado extensa y no aporta mucha informaci\u00f3n \u00fatil. En los \u00faltimos dos a\u00f1os no se ha actualizado mucho, de hecho basado en DroidAR ha aparecido la librer\u00eda DroidAR 2 de c\u00f3digo cerrado que ofrece nuevas y mejoradas funcionalidades respecto al proyecto original.</p>"},{"location":"unity.html#vuforia","title":"Vuforia","text":"<p>Vuforia es otra librer\u00eda de realidad aumentada desarrollada por la empresa Qualcomm. En 2015 Qualcomm vendi\u00f3 la plataforma a otra empresa estadounidense que ha continuado desarrollando la misma. Vuforia proporciona m\u00faltiples formas de ofrecer realidad aumentada a trav\u00e9s de marcadores 2D/3D, m\u00faltiple detecci\u00f3n de marcadores de forma simult\u00e1nea, posicionamiento, etc\u00e9tera. Vuforia ofrece soporte para las plataformas iOS y Android de forma nativa, a su vez ofrece un plugin para Unity de forma que podemos desarrollar nuestra aplicaci\u00f3n de realidad aumentada en Unity y posteriormente desplegarla en aplicaciones de Escritorio, MACOSX, Windows 10, Android e iOS. Adem\u00e1s ofrece soporte para otros dispositivos m\u00f3viles, como las gafas de realidad aumentada de EPSON Epson Moverio BT-200Epson Moverio BT-200, gafas de realidad virtual Samsung GearVR e incluso recientemente han dado soporte para el dispositivo de realidad aumentada Microsoft HoloLens. Vuforia ofrece licencias para desarrollar proyectos personales sin fines comerciales, de forma que podemos crear facilmente apps con funcionalidades de realidad aumentada. Si queremos comercializar nuestra app tendremos que adquirir una licencia.</p>"},{"location":"unity.html#wikitude-ar","title":"Wikitude AR","text":"<p>Wikitude AR es una librer\u00eda comercial ofrece per\u00edodo de pruebaofrece per\u00edodo de prueba de realidad aumentada. Permite el reconocimiento de imagenes y seguimiento de las mismas, as\u00ed como visualizaci\u00f3n de modelos 3D y animaciones. Tambi\u00e9n ofrece detecci\u00f3n basada en la geolocalizaci\u00f3n del dispositivo. Wikitude AR SDK esta disponible para Android, iOS, Unity, Google Glass, Epson Moverio, Vuzix M-100, plugin para la librer\u00eda PhoneGap y tambi\u00e9n un componente de Xamarin.</p>"},{"location":"unity.html#desarrollando-una-aplicacion-de-realidad-aumentada-con-vuforia","title":"Desarrollando una aplicaci\u00f3n de Realidad aumentada con Vuforia","text":"<p>En este curso vamos a centrarnos en la librer\u00eda Vuforia, ya que adem\u00e1s de ofrecer compatibilidad con m\u00faltiples plataformas, su integraci\u00f3n con Unity es muy sencilla y nos permite facilmente crear aplicaciones de realidad aumentada. Adem\u00e1s, dispone de licencias personales que podemos utilizar sin restricciones para el desarrollo de nuestras aplicaciones.</p> <p>Lo primero que tenemos que hacer para empezar el desarrollo de una aplicaci\u00f3n de realidad aumenta con Vuforia es registrarnos en su p\u00e1gina web [https://developer.vuforia.com/](https://developer.vuforia.com/)[https://developer.vuforia.com/](https://developer.vuforia.com/).</p> <p></p> <p>Una vez registrados recibiremos un correo electr\u00f3nico para confirmar la cuenta que acabamos de crear. Al identificarnos en el portal de la plataforma veremos varias secciones. Primero debemos ir a la secci\u00f3n \"Downloads\" para descargar el SDK. En esta secci\u00f3n podemos ver una descripci\u00f3n de las distintas caracter\u00edsticas que el SDK de Vuforia nos ofrece, as\u00ed como las distintas version para m\u00faltiples plataformas. En nuestro caso vamos a descargar la versi\u00f3n para Unity. Al descargar el SDK para Unity nos encontramos un paquete que podemos importar en nuestro proyecto mediante la opci\u00f3n de Unity \"import package\" o bien simplemente hacer doble click sobre el fichero y se importara en el proyecto que tengamos abierto en ese momento. A continuaci\u00f3n mostramos las caracter\u00edsticas que ofrece el SDK de Vuforia.</p> <ul> <li>Image Targets: Detecci\u00f3n y seguimiento de marcadores 2D.</li> <li>VuMark: VuMark es un marcador propio de Vuforia al estilo de c\u00f3digos QR o similares. La ventaja es que el tracking es m\u00e1s robusto y estable en comparaci\u00f3n a utilizar Image Targets.</li> <li>Object Recognition: Utilizaci\u00f3n de marcadores 3D: cubos, esferas, cilindros, como objeto a detectar para superponer contenido.</li> <li>Multi Targets: Detecci\u00f3n de m\u00faltiples marcadores de forma simult\u00e1ena permitiendo detectar y seguir varios modelos de forma independiente.</li> <li>User Defined Targets: Permite crear marcadores 2D personalizados basados en im\u00e1genes que el usuario puede subir a la plataforma.</li> <li>Smart Terrain Unity onlyUnity only: Permite crear f\u00e1cilmente escenarios con m\u00faltiples marcadores para ofrecer experiencias de realidad aumentada muy vistosas. Esta enfocado a la creaci\u00f3n de videojuegos y permite crear f\u00e1cilmente distintos niveles dentro de nuestro juego o aplicaci\u00f3n.</li> <li>Cloud Recognition: Permite almacenar los marcadores en la nube de forma que no tienen que almacenarse en el dispositivo m\u00f3vil.</li> <li>Text Recognition: Utilizaci\u00f3n de texto como marcador.</li> <li>Frame Markers: Utilizaci\u00f3n de marcadores 2D convencionales tambi\u00e9n conocidos como marcadores fiduciales. Cada marcador codifica un patr\u00f3n binario que corresponde a un identificador \u00fanico.</li> </ul> <p>Una vez descargado el SDK e importado en Unity, podemos a\u00f1adir una c\u00e1mara de Realidad Aumentada a la escena, de forma que cuando usemos este objeto nos va a permitir detectar y hacer el seguimiento de los marcadores.</p> <p>Creamos un nuevo proyecto, hacemos doble click sobre el SDK que hemos descargado previamente y a continuaci\u00f3n aceptamos que se importe el SDK en el proyecto que acabamos de crear.</p> <p></p> <p></p> <p>Una vez importamos el SDK dispondremos de una carpeta con el nombre \"Vuforia\" dentro de los Assets de nuestro proyecto. Esta carpeta contiene una serie de objetos que podemos a\u00f1adir a nuestra escena. Entre estos objetos encontramos la c\u00e1mara de realidad virtual que mencionamos anteriormente. Lo primero que tenemos que hacer para habilitar la funcionalidad de realidad aumentada en nuestra aplicaci\u00f3n es eliminar la c\u00e1mara principal que se crea en la escena por defecto al crear un proyecto. Una vez eliminada, hacemos click sobre la carpeta \"Vuforia\" y nos digirimos a la subcarpeta \"Prefabs\". Esta carpeta contiene el objeto \"ARCamera\" que podemos arrastar en nuestra escena. Una vez disponemos de este objeto en nuestra escena  vamos a ver que componentes tiene, para ello hacemos click sobre el objeto en la vista que nos muestra de forma jer\u00e1rquica los objetos en la escena.</p> <p></p> <p>Vamos a ver los distintos componentes que tiene asociados el objeto \"ARCamera\" y sobretodo vamos a ver las distintas opciones que nos ofrece el componente \"Vuforia Behaviour\". Podemos ver que por defecto, la mayor\u00eda de componentes se encuentran activados, permitiendo la inicializaci\u00f3n del objeto, cargar una base de datos con marcadores, mostrar v\u00eddeo de fondo obtenido a trav\u00e9s de la c\u00e1mara del dispositivo o habilitar la caracter\u00edstica \"Smart terrain\" de la librer\u00eda Vuforia.</p> <p>Por ahora vamos a centrarnos en el componente \"Vuforia Behaviour\", podemos ver que en el primer campo o propiedad tiene un campo para introducir una licencia de aplicaci\u00f3n. De hecho, si intentamos ejecutar la aplicaci\u00f3n sin introducir esta licencia, obtendremos un mensaje de aviso alert\u00e1ndonos de la necesidad de introducir una licencia v\u00e1lida para poder utilizar la c\u00e1mara de realidad aumentada en nuestra escena. Por ello, antes de continuar tendremos que ir a la plataforma online de Vuforia y obtener una clave para nuestra aplicaci\u00f3n.</p> <p></p> <p>En la pesta\u00f1a developer de la plataforma online veremos que tenemos disponible una subsecci\u00f3n llamada \"License Manager\", desde esta podemos crear una nueva licencia para nuestra aplicaci\u00f3n, tan solo deberemos seguir los siguientes pasos:</p> <ul> <li> <p>Elige un tipo de proyecto: Dependiendo del tipo de aplicaci\u00f3n: Development, Consumer o Enterprise tendremos que pagar por nuestra licencia o no. En nuestro caso seleccionaremos el tipo \"Development\"</p> </li> <li> <p>Definimos un nombre para nuestra app: como la plataforma nos permite crear m\u00faltiples apps es importante poner un nombre identificativo de forma que m\u00e1s tarde podamos relacionar las licencias con las apps que estamos desarrollando.</p> </li> <li> <p>Seleccionamos el tipo de dispositivo: en este caso tan solo tendremos que diferenciar entre disposito m\u00f3vil o \"eyewear\". Dentro de \"eyewear\" engloban dispositivos como las Microsoft Hololens, EPSON moverio, etc\u00e9tera.</p> </li> <li> <p>Obtenemos nuestra licencia: En el caso de que se trate de una licencia comercial tendremos que proveer con un medio de pago para costear la misma, en caso contrario veremos que nuestra licencia se acaba de crear. La licencia de desarrollar tiene algunas restricciones, por ejemplo el n\u00famero m\u00e1ximo de veces por mes que un marcador puede ser detectado usando nuestra aplicaci\u00f3n m\u00f3vil. Para saltarnos esta restricci\u00f3n tendremos que obtener una licencia comercial.</p> </li> </ul> <p>Una vez creada, nos aparecer\u00e1 en el gestor de licencias de la plataforma online y podemos hacer click sobre la misma para ver la cadena de texto que representa la licencia de nuestra aplicaci\u00f3n. Esta cadena se debe de introducir en el campos que vimos anteriormente del componente \"Vuforia Behaviour\" en objeto \"ARCamera\".</p> <p></p> <p>Tr\u00e1s introducir la licencia veremos que podemos ejecutar nuestra aplicaci\u00f3n en Unity y si nuestro ordenador dispone de una c\u00e1mara compatible podremos ver video en directo en la escena Unity.</p> <p>Lo siguiente que tenemos que hacer es habilitar el uso de marcadores y definir el marcador que vamos a reconocer en nuestra aplicaci\u00f3n para suponer contenido.</p> <p>Si es la primera vez que creamos una aplicaci\u00f3n, tendremos que irnos a la plataforma online y crear un nuevo \"Target\" que va a ser reconocido en nuestra aplicaci\u00f3n. Si por el contrario ya dispusieramos de alg\u00fan marcador creado previamente para otro proyecto usando la librer\u00eda Vuforia, podr\u00edamos copiarlo y reutilizarlo en este nuevo proyecto. En nuestro caso vamos a la pesta\u00f1a \"Develop\" de la plataforma online y hacemos click sobre la secci\u00f3n \"Target Manager\". Una vez dentro el sistema nos ofrecela posibilidad de crear una nueva base de datos con marcadores para nuestra aplicaci\u00f3n. Al pinchar en a\u00f1adir nos preguntar\u00e1 donde va a residir el marcador: Device, Cloud o VuMark. Seleccionaremos Device ya que queremos almacenar los marcadores en el propio dispositivo. Una vez creada la base de datos podemos crear distintos marcadores que posteriormente usaremos en nuestra aplicaci\u00f3n.</p> <p>Al a\u00f1adir un marcador tendremos que seleccionar una serie de opciones, primero de todo el tipo de marcador, una imagen, cubo 3D,  cilindro incluso nos permite utilizar un objeto. En todos los casos  es importante adem\u00e1s que el tipo de marcador tenga una textura o patr\u00f3n que permita que los algoritmos de visi\u00f3n por computador para seguimiento de caracter\u00edsticas visuales puedan detectar y hacer el seguimiento de forma correcta. A continuaci\u00f3n seleccionaremos el fichero de la imagen a utilizar y el ancho en unidades unity que queremos que nuestro marcador tenga en la escena. Este par\u00e1metro deber\u00e1 estar en la misma escala que el contenido que vamos a visualizar sobre el marcador. El alto se computar\u00e1 a partir del ancho introducido. Por \u00faltimo le daremos un nombre a este marcador ya que podremos reutilizarlo para otras aplicaciones.</p> <p></p> <p>Al crearlo, la plataforma online procesar\u00e1 la imagen que hemos subido y nos dar\u00e1 una valoraci\u00f3n sobre la calidad de la imagen para ser utilizada como marcador. Como podemos imaginar, si la imagen utilizada no tiene suficientes caracter\u00edsticas visuales texturatextura el sistema le dar\u00e1 una calificaci\u00f3n baja y como consecuencia la detecci\u00f3n y el seguimiento del marcador no ser\u00e1n muy estables. La tecnolog\u00eda Vuforia se basa en la utilizaci\u00f3n del algoritmo SURF, ampliamente conocido en la comunidad de visi\u00f3n por computador, para detectar una determinada textura en la escena. En nuestro caso, hemos subido una textura sugerida por la librer\u00eda Vuforia, la podemos encontrar en su p\u00e1gina web y ofrece buenas caracter\u00edsticas visuales para un correcto funcionamiento. M\u00e1s adelante, probaremos a subir nuestras propias im\u00e1genes y ver qu\u00e9 tal funcionan. Si vamos al listado de marcadores que hemos subido y hacemos click sobre el que acabamos de crear, podemos ver adem\u00e1s de la valoraci\u00f3n que nos ofrece la plataforma, las caracter\u00edsticas visuales que ha detectado.</p> <p></p> <p>En el siguiente enlace encontramos algunos ejemplos de im\u00e1genes que Vuforia recomienda: https://developer.vuforia.com/sites/default/files/sample-apps/targets/imagetargets_targets.pdf</p> <p>El siguiente paso para utilizar este marcador en el proyecto Unity que creamos previamente es descargarlo desde la plataforma online e importarlo de igual forma que hicimos con el SDK de Vuforia. Deberemos seleccionar la opci\u00f3n de descarga para la plataforma Unity.</p> <p></p> <p>Una vez importados ambos paquetes en Unity vamos a repasar la estructura de Assets que tenemos en nuestro proyecto relacionados con el SDK de Vuforia:</p> <ul> <li>Editor: Contiene los scripts que necesitemos para interactuar de forma din\u00e1mica con los marcadores que tengamos en nuestra aplicaci\u00f3n.</li> <li>Plugins: Contiene los binarios Java y otros binarios nativos que integran el SDK de Vuforia con las aplicaciones de Unity para Android o iOS.</li> <li>Vuforia: Contiene los scripts y prefabs requeridos para implementar funcionalidades de realidad aumentada en tu aplicaci\u00f3n Unity.</li> <li>Streaming Assets/QCAR * Contiene la base de datos y su configuraci\u00f3n, de marcadores para nuestro dispositivo en formato XML y DAT fichero importado de la plataforma online para gestionar marcadoresfichero importado de la plataforma online para gestionar marcadores.</li> </ul> <p>Volviendo al proyecto Unity que hemos creado, como ya disponemos de una c\u00e1mara de realidad aumentada en nuestra escena, tan solo nos quedar\u00eda incluir un objeto \"ImageTarget\" en nuestra escena y configurar algunos valores en los scripts de estos objetos. Por lo tanto, arrastramos un objeto \"ImageTarget\" a la escena. Este objeto lo encontramos dentro de la carpeta Vuforia/prefabs dentro del explorador del proyecto en Unity.</p> <p>Una vez a\u00f1adido deberemos configurar algunos de los componentes de este objeto. El componente \"Mesh Renderer\" nos permite cambiar algunos efectos gr\u00e1ficos sobre los modelos que vamos a visualizar sobre el marcador. Por ejemplo, si los modelos que cuelguen de este objeto proyectan sombras o si podemos proyectar sombras sobre el modelo en s\u00ed.</p> <p></p> <p>Para que el marcador funcione en nuestra aplicaci\u00f3n y sea detectado debemos seleccionarlo en el componente \"Image Target Behaviour\". Este script nos permite seleccionar el marcador que previamente creamos e importamos en nuestro proyecto, as\u00ed como configurar algunas opciones del mismo. Por ahora, simplemente seleccionaremos nuestro marcador de la lista despegable \"Database\". Con esto ya tenemos nuestra aplicaci\u00f3n lista para detectar el marcador definido. Para probarla, a\u00f1adimos en la escena un modelo 3D, por ejemplo una esfera, y la posicionamos en la escena cerca del marcador que hemos puesto anteriormente.</p> <p></p> <p>Unity nos permite crear aplicaciones multiplataforma, por lo que podremos generar aplicaci\u00f3nes para los siguientes sistemas: Windows, OSX, Android, iOS, tvOS, Tizen, webGL, Xbox One, PS3, PS4, etcetera. Si tenemos una c\u00e1mara compatible en el ordenador donde estamos desarrollando la aplicaci\u00f3n, podremos probar nuestra aplicaci\u00f3n de realidad aumentada simplemente pulsando el bot\u00f3n de \"Play\" en la parte superior.</p> <p>A continuaci\u00f3n vamos a ver como generar la aplicaci\u00f3n para Android o iOS.</p>"},{"location":"unity.html#desplegando-aplicacion-unity-en-android","title":"Desplegando aplicaci\u00f3n Unity en Android","text":"<p>Accedemos al di\u00e1logo: \"File -&gt; Build Settings\". Desde aqu\u00ed  podemos especificar y configurar la plataforma para la cual queremos generar nuestra aplicaci\u00f3n. En el caso de Android, adem\u00e1s, primero tendremos que especificar el directorio donde se encuentra el SDK Android y el Java Development Kit JDKJDK. En \"Unity -&gt; Preferences -&gt; External tools\" deberemos configurar estos directorios.</p> <p></p> <p>A la hora de desplegar la aplicaci\u00f3n en Android, tenemos varias opciones, bien generamos directamente la aplicaci\u00f3n en formato .apk o tambi\u00e9n podemos generar un proyecto para Android Studio y modificarlo posteriormente all\u00ed. De esta forma podremos ver c\u00f3digo fuentes para la integraci\u00f3n de Vuforia en la app Android y hacer modificaciones en la l\u00f3gica de la aplicaci\u00f3n. Para generar el proyecto para Android Studio tendr\u00edamos que marcar la casilla \"Google Android Project\" en \"Build Settings\".</p> <p>Adem\u00e1s, si pulsamos sobre \"Player Settings\" podremos definir toda una serie de opciones para la generaci\u00f3n de la aplicaci\u00f3n en Android. Entre estas opciones encontramos, nombre de la aplicaci\u00f3n, icono, opciones de rendering, optimizaci\u00f3n de codigo GPU, m\u00ednima versi\u00f3n de API requerida, lugar de instalaci\u00f3n para la app, acceso a Internet, permisos de escritura, etc\u00e9tera.</p> <p></p> <p>Al pulsar el bot\u00f3n \"Build\" nos generar\u00e1 el fichero .apk compatible para Android con las opciones marcadas y si pulsamos \"Build and Run\" buscar\u00e1 un dispositivo Android conectado e instalar\u00e1 y ejecutar\u00e1 la aplicaci\u00f3n en el dispositivo.</p>"},{"location":"unity.html#desplegando-aplicacion-unity-en-ios","title":"Desplegando aplicaci\u00f3n Unity en iOS","text":"<p>Para desplegar la aplicaci\u00f3n en iOS, seleccionaremos esa opci\u00f3n en el di\u00e1logo \"Build Settings\". En este caso, en lugar de ejecutar directamente, nos crear\u00e1 un proyecto en para Xcode que utilizaremos para desplegar la aplicaci\u00f3n en nuestro dispositivo iOS. Si tratamos de compilar el proyecto que nos acaba de generar Unity, obtendremos un error indic\u00e1ndonos que la aplicaci\u00f3n no ha sido firmada y requiere especificar un \"Equipo\".</p> <p></p> <p>Por \u00faltimo, antes de compilar la aplicaci\u00f3n y desplegarla en nuestro dispositivo iOS, tendremos que a\u00f1adir una entrada en la configuraci\u00f3n del proyecto con la descripci\u00f3n del uso de la c\u00e1mara. Por pol\u00edticas de seguridad con el acceso a la c\u00e1mara,  los desarrolladores tienen que detallas el uso de la c\u00e1mara en la aplicaci\u00f3n. Cuando la aplicaci\u00f3n intente acceder a la c\u00e1mara el usuario, iOS nos lo informar\u00e1 con un di\u00e1logo y nos permira permiso para que la aplicaci\u00f3n acceda a la c\u00e1mara. Por ello en la secci\u00f3n \"Info\" de la configuraci\u00f3n del proyecto, creamos la clave \"Privacy - Camera Usage Description\" y ponemos una descripci\u00f3n del uso de la c\u00e1mara en nuestra app: \"detecci\u00f3n marcadores realidad aumentada\".</p> <p></p> <p>Ahora podemos compilar y desplegar la aplicaci\u00f3n en nuestro dispositivo iOS pulsando el bot\u00f3n \"Play\" de la parte superior.</p>"},{"location":"unity.html#ejercicio-realidad-aumentada","title":"Ejercicio realidad aumentada","text":"<p>Para poner en pr\u00e1ctica lo que hemos visto os proponemos el siguiente ejercicio: Vamos a crear una applicaci\u00f3n de realidad aumentada para promocionar el m\u00e1ster en desarrollo de software para dispositivos m\u00f3viles. Utiliza el tr\u00edptico del m\u00e1ster para visualizar una serie de modelos 3D sobre este. Adem\u00e1s, para poner en pr\u00e1ctica lo que hemos aprendido de Unity sobre interfaces 2D en las secciones anteriores, a\u00f1adiremos una simple interfaz de usuario para poder salir de la aplicaci\u00f3n. A\u00f1adiremos varios modelos 3D utilizando distintos formatos .3ds, .obj, .off, ....3ds, .obj, .off, ... Los modelos 3D se pueden descargar desde la plataforma Moodle <code>(assets_ejercicio_realidad_aumentada.zip)</code>. Utilizando la portada y la parte interior del tr\u00edptico crearemos dos marcadores. La detecci\u00f3n de cada marcador visualizar\u00e1 un contenido distinto en nuestra aplicaci\u00f3n. Por ejemplo, al detectar la portada del tr\u00edptico podemos invitar al usuario a abrirlo mostrando texto 3D sobre el marcador y entonces detectar la parte interior del tr\u00edpico y mostrar un contenido distinto.</p> <p>Por \u00faltimo, habilitaremos la interacci\u00f3n con los modelos 3D cargados en la escena. Al pulsar sobre cada uno de los modelos, Android, iMac, logotipo curso, visualizaremos una im\u00e1gen distinta sobre el monitor iMac, por ejemplo, al pulsar sobre Android mostraremos una imagen de Android, al pulsar sobre el monitor visualizaremos una im\u00e1gen sobre iOS, etc\u00e9tera. Podemos utilizar un di\u00e1logo o simplemente un plano 3D con una textura que se posicionar\u00e1 encima de la pantalla del iMac.</p> <p></p>"},{"location":"unity.html#realidad-mixta","title":"Realidad mixta","text":"<p>Realidad mixta es un t\u00e9rmino que se utiliza para la combinaci\u00f3n de t\u00e9cnicas de realidad virtual y aumentada en tiempo real. Consiste en visualizar el mundo real usando t\u00e9cnicas de realidad virtual. Para ello, normalmente se visualiza el mundo real a trav\u00e9s de la utilizaci\u00f3n de una c\u00e1mara, o atrav\u00e9s de pantallas transparentes, cuyo flujo de v\u00eddeo se renderiza en entornos de realidad virtual a\u00f1adiendo contenido virtual y renderiz\u00e1ndolo todo usando un par de im\u00e1genes estereosc\u00f3picas. Utilizando marcadores como los vistos anteriormente, podemos facilmente, usando la c\u00e1mara, a\u00f1adir contenido virtual sobre el flujo de v\u00eddeo de la c\u00e1mara y visualizarlo todo a la vez como si estuviera en el mundo real.</p> <p>Tambi\u00e9n se ha acu\u00f1ado el t\u00e9rmino de realidad mixta, a tecnolog\u00edas como las desarrolladas por Microsoft, Meta, Google o Epson. Estas compa\u00f1ias han desarrollado unos cascos que incorporan pantallas transparentes muy cercanas a los ojos. De esta forma podemos seguir viendo el mundo real, pero a su vez nos permiten visualizar objetos virtuales que son proyectados sobre el mundo real. Ejemplos de productos actuales que implementan esta tecnolog\u00eda son Microsoft Hololens, Meta2 o las Epson Moverio BT-200.</p> <p>La tecnolog\u00eda Vuforia tambi\u00e9n es compatible con tecnolog\u00edas de realidad virtual permiti\u00e9ndonos crear aplicaciones aplicaciones de realidad mixta. De esta forma, la librer\u00eda Vuforia permite generar un par de im\u00e1genes estereosc\u00f3picas que podemos visualizar sobre distintos visores de realidad virtual como el Google Cardboard. De esta forma nuestra aplicaci\u00f3n puede alternar entre el mundo real y el mundo virtual.</p> <p>Utilizando el proyecto que hemos desarrollado anteriormente, podemos f\u00e1cilmente configurarlo para que genere un par de im\u00e1genes estereosc\u00f3picas para el Google cardboard y visualizarlo en un entorno m\u00e1s ceracano a la realidad virtual. El flujo de v\u00eddeo de la c\u00e1mara nos permitir\u00e1 ver lo que hay a nuestro alrededor y detectar el marcador sobre el cual visualizaremos el contenido 3D que hemos posicionado anteriormente. Podemos observar que existe una peque\u00f1a latencia entre nuestros movimientos y el flujo de v\u00eddeo debido al procesamiento de la imagen antes de ser visualizado en la pantalla de nuestro m\u00f3vil.</p> <p>Para configurar el proyecto anterior y que nos permita renderizar un par de im\u00e1genes estereosc\u00f3picas para el Google Cardboard u otros cascos de realidad virtual, tan solo tenemos que hacer click sobre la c\u00e1mara de nuestra escena <code>ARCamera</code> y modificar las siguientes opciones en el script <code>DigitalEyeWearBehaviour</code>:</p> <ul> <li>Eyewear Type = Video See-Through</li> <li>Stereo Camera Config = Vuforia</li> <li>Viewer Type = Generic Cardboard VuforiaVuforia or Cardboard v1 GoogleGoogle</li> </ul> <p>Si desplegamos de nuevo la aplicaci\u00f3n en el dispositivo m\u00f3vil y lo insertamos en el casco de realidad virtual, podremos visualizar la aplicaci\u00f3n de realida aumentada que anteriormente desarrollamos de forma m\u00e1s immersiva.</p>"},{"location":"unity.html#nuevos-frameworks-de-realidad-aumentada-sin-marcadoressin-marcadores","title":"Nuevos frameworks de realidad aumentada sin marcadoressin marcadores","text":""},{"location":"unity.html#arkit","title":"ARKit","text":"<p>ARKit es un nuevo framework que permite crear facilmente experiencias de realidad aumentada sin el uso de marcadores. El uso de esta tecnolog\u00eda proporciona un sistema de tracking muy estable capaz de mezclar objetos virtuales con el entorno que nos rodea. Este framework requiere el uso de un dispositivo Apple compatible, modelos de iPhone e iPad con procesador A9 o posterior: iPad Pro, iPad 2017, iPhone 6S, 7, 8 y X. Es necesario tener instalado iOS 11 y xcode 9 o superior.</p> <p>Aunque podemos desarrollar nuestra aplicaci\u00f3n con soporte ARKit mediante el uso de SceneKit en Swift 4, en este curso vamos a ver que existen plugins para entornos de desarrollo de videojuegos y aplicaciones como Unity y Unreal Engine. En este curso nos vamos a centrar en el uso del plugin de ARKit para Unity. Para m\u00e1s informaci\u00f3n sobre como desarrollar para otras plataformas pod\u00e9is consultar la siguiente lista de tutoriales: https://github.com/olucurious/Awesome-ARKit</p>"},{"location":"unity.html#plugin-arkit-para-unity","title":"Plugin ARKit para Unity","text":"<p>Este plugin desarrollado para Unity nos permite, mediante la utilizaci\u00f3n de scripts, acceder a la interfaz nativa de ARKit desde Unity. Los scripts son asociados a GameObjects de nuestra escena dot\u00e1ndola as\u00ed de capacidad para aumentar el mundo real con objetos virtuales. El script se puede descargar descargar desde la asset store o desde su repositorio en bitbucket.</p> <p>A continuaci\u00f3n vamos a ver los scripts m\u00ednimos necesarios para visualizar un objeto virtual en la escena real.</p> <ul> <li>UnityARCameraManager: este script debe a\u00f1adirse dentro un game object vacio o bien debajo de la c\u00e1mara de la escena que queremos que sea controlada por ARKit. Este script adem\u00e1s inicializa el objeto sesi\u00f3n de ARKit que provee toda la informaci\u00f3n sobre el tracking de la c\u00e1mara en la escena: matriz de roto-translaci\u00f3n.</li> <li>UnityARVideo: este script debe a\u00f1adirse a la c\u00e1mara. Se encarga de renderizar el v\u00eddeo de la c\u00e1mara como background en nuestra aplicaci\u00f3n.</li> </ul> <p>Con estos dos scripts en nuestra escena ya podemos colocar un objeto virtual en 3D en nuestra escena. Ahora bien, existen otros scripts que nos  proveen con funcionalidad adicional como interacci\u00f3n:</p> <ul> <li>UnityARAmbient: este script se a\u00f1ade sobre la luz de nuestra escena y nos permite cambiar la intensidad de la misma acorde a la luz que hay en el mundo real. De esta forma se consigue que en escenas con poca iluminaci\u00f3n, los objetos virtuales se integren y se minimice la diferencia con los objetos reales. </li> <li>UnityARHitTestExample: este script se ejecuta cada vez que tocamos la pantalla. Se encarga de calcular la intersecci\u00f3n entre la posici\u00f3n de la pantalla pulsada rayo virtualrayo virtual y los planos y/o keypoints m\u00e1s cercanos detectados en la escena. De esta forma nos permite colocar objetos sobre las superficies de la escena real.</li> </ul> <p>Por \u00faltimo cabe destacar que el plugin incorpora scripts para ayudarnos a desarrollar aplicaciones de realidad aumentada, as\u00ed como una serie de escenas de ejemplos que vamos a describir a continuaci\u00f3n:</p> <ul> <li>UnityARKitScene: esta es la escena m\u00e1s b\u00e1sica que encontramos entre las escenas de ejemplo. Se puede utilizar como proyecto base para crear nuestras aplicaciones. Por defecto se puede exportar y probar en un dispositivo iOS compatible.</li> <li>UnityARBallz: muestra el uso de colliders y cuerpos r\u00edgidos en una app con realidad aumentada.</li> <li>UnityARShadows: muestra el uso de un material para renderizar sombras sobre objetos reales, especialmente sobre planos.</li> <li>UnityParticlePainter: esta escena de ejemplo nos permite dibujar particulas virtuales en nuestro entorno, aumentando el contenido del mundo real.</li> </ul>"}]}